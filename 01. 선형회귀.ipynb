{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_boston, load_iris # 샘플 데이터 set\n",
    "from sklearn.linear_model import LinearRegression,Ridge, SGDRegressor # 선형 회귀\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score# 선형 모델(Linear Models)\n",
    "\n",
    "# SGDRegressor: 학습을 통해\n",
    "# LinearRegression: 공식\n",
    "# MLPRegressor: 딥러닝을 통해(학습)\n",
    "\n",
    "from sklearn.model_selection import train_test_split # 훈련, 테스트 데이터 분할\n",
    "\n",
    "import mglearn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['font.family']='Malgun Gothic'\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mglearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [1,2,3,4,5,6,7,8,9,10] # 벡터\n",
    "y_data = [1,2,3,4,5,6,7,8,9,10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 공식을 통한 선형회귀\n",
    "- LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = LinearRegression(    fit_intercept=True, # intercept 구하기\n",
    "                                normalize=False, # 데이터 정규화\n",
    "                                copy_X=True, # training 할 때 x 데이터를 복사\n",
    "                                n_jobs=None # CPU core수\n",
    "                           )  # LinearRegression 객체 생성\n",
    "\n",
    "# LinearRegression : **공식**을 이용해서 intercept와 slope를 구하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1],\n",
       "       [ 2],\n",
       "       [ 3],\n",
       "       [ 4],\n",
       "       [ 5],\n",
       "       [ 6],\n",
       "       [ 7],\n",
       "       [ 8],\n",
       "       [ 9],\n",
       "       [10]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n행 1열 :  행은 데이터의 개수, 열은 특성데이터의 갯수\n",
    "print( len(x_data) )\n",
    "# 따라서 x_data를 10행 1열로 줘야함\n",
    "np.array( x_data ).reshape(-1,1) # 열의 개수를 지정하면 행 값에 -1해도 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Feature 입력\n",
    "- x값\n",
    "    - 반드시 **n행 1열**을 줘야한다\n",
    "        - 행은 데이터의 개수, 열은 특성데이터의 갯수\n",
    "    - numpy면 **행렬**\n",
    "    - pandas면 **dataframe**으로 줘야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lr.fit( np.array( x_data ).reshape(10,1), np.array(y_data) ) \n",
    "# 특성데이터는 반드시 **행렬** or **dataframe**으로 줘야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lr.coef_ # w값 (기울기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6645352591003757e-15"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lr.intercept_ # b값(y절편)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [예제-cars.csv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speed</th>\n",
       "      <th>dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   speed  dist\n",
       "1      4     2\n",
       "2      4    10\n",
       "3      7     4\n",
       "4      7    22\n",
       "5      8    16"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_df = pd.read_csv('../data/cars.csv', index_col=0 )\n",
    "car_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    4\n",
       "2    4\n",
       "3    7\n",
       "4    7\n",
       "5    8\n",
       "Name: speed, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_df['speed'].head() # Series-1차원-벡터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     2\n",
       "2    10\n",
       "3     4\n",
       "4    22\n",
       "5    16\n",
       "Name: dist, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_df['dist'].head() # Series-1차원-벡터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   speed\n",
       "1      4\n",
       "2      4\n",
       "3      7\n",
       "4      7\n",
       "5      8"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_df[['speed']].head() # DataFrame-2차원-행렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dist\n",
       "1     2\n",
       "2    10\n",
       "3     4\n",
       "4    22\n",
       "5    16"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_df[['dist']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_car = LinearRegression()\n",
    "model_car.fit( car_df[['speed']], car_df[['dist']] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.93240876]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_car.coef_ # w값(기울기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-17.57909489])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_car.intercept_ # b값( y절편)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) 예측\n",
    "- model.predict( [[ ]] )\n",
    "    - x값을 줄 때에도 **매트릭스**형태로 줘야한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter( car_df['speed'], car_df['dist'])\n",
    "\n",
    "# (1) 수식 직접 입력\n",
    "# car_lr = model_car.coef_*car_df[['speed']] + model_car.intercept_\n",
    "# plt.plot(car_df[['speed']], car_lr, 'r--' )\n",
    "\n",
    "# (2) LinearRegression의 멤버함수인 predict사용\n",
    "car_lr2 = model_car.predict( car_df[['speed']])\n",
    "plt.plot(car_df['speed'], car_lr2 , 'c-.' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[41.4070365]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_car.predict( [[15]]) # 예측값을 구할때도 행렬로 줘야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[33.54221898],\n",
       "       [41.4070365 ]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 자동차 속도가 13, 15인 경우의 제동거리는?\n",
    "\n",
    "model_car.predict( [[13], [15]] ) # 2행 1열"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13, 16],\n",
       "       [29, 36]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array( [[1,2], [3,4]])\n",
    "y = np.array( [[3,4], [5,6]])\n",
    "\n",
    "# predict가 수행하는 방식 - np.matmul\n",
    "\n",
    "np.matmul( x, y ) # 행렬 곱(matmul)\n",
    "# 2x2행렬 나옴 \n",
    "# x, y가 반드시 행렬이어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.84945985],\n",
       "       [-1.84945985],\n",
       "       [ 9.94776642],\n",
       "       [ 9.94776642],\n",
       "       [13.88017518],\n",
       "       [17.81258394],\n",
       "       [21.7449927 ],\n",
       "       [21.7449927 ],\n",
       "       [21.7449927 ],\n",
       "       [25.67740146],\n",
       "       [25.67740146],\n",
       "       [29.60981022],\n",
       "       [29.60981022],\n",
       "       [29.60981022],\n",
       "       [29.60981022],\n",
       "       [33.54221898],\n",
       "       [33.54221898],\n",
       "       [33.54221898],\n",
       "       [33.54221898],\n",
       "       [37.47462774],\n",
       "       [37.47462774],\n",
       "       [37.47462774],\n",
       "       [37.47462774],\n",
       "       [41.4070365 ],\n",
       "       [41.4070365 ],\n",
       "       [41.4070365 ],\n",
       "       [45.33944526],\n",
       "       [45.33944526],\n",
       "       [49.27185401],\n",
       "       [49.27185401],\n",
       "       [49.27185401],\n",
       "       [53.20426277],\n",
       "       [53.20426277],\n",
       "       [53.20426277],\n",
       "       [53.20426277],\n",
       "       [57.13667153],\n",
       "       [57.13667153],\n",
       "       [57.13667153],\n",
       "       [61.06908029],\n",
       "       [61.06908029],\n",
       "       [61.06908029],\n",
       "       [61.06908029],\n",
       "       [61.06908029],\n",
       "       [68.93389781],\n",
       "       [72.86630657],\n",
       "       [76.79871533],\n",
       "       [76.79871533],\n",
       "       [76.79871533],\n",
       "       [76.79871533],\n",
       "       [80.73112409]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_car.predict( car_df[['speed']]) # nx1 feature과 1x1인 w와 곱셈(행렬곱)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [연습문제]\n",
    "- w,b를 구하고 전기 생산량이 3.2인경우와 4.5인경우의 전기 사용량 예측\n",
    "- 스캐터 차트와 회귀선 그래프를 그리시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>전기생산량</th>\n",
       "      <th>전기사용량</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.52</td>\n",
       "      <td>2.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.58</td>\n",
       "      <td>2.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.31</td>\n",
       "      <td>2.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.07</td>\n",
       "      <td>2.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.62</td>\n",
       "      <td>2.98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   전기생산량  전기사용량\n",
       "0   3.52   2.48\n",
       "1   2.58   2.27\n",
       "2   3.31   2.47\n",
       "3   4.07   2.77\n",
       "4   4.62   2.98"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/data4/electric.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 12 entries, 0 to 11\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   전기생산량   12 non-null     float64\n",
      " 1   전기사용량   12 non-null     float64\n",
      "dtypes: float64(2)\n",
      "memory usage: 288.0 bytes\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elec_model = LinearRegression()\n",
    "elec_model.fit( df[['전기생산량']], df[['전기사용량']] ) # 모델 적용(공식으로 회귀식 도출)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.49560324]]), array([0.91958143]))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elec_model.coef_, elec_model.intercept_ # w(기울기),b(절편)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.50551178],\n",
       "       [3.14979599]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elec_model.predict( [[3.2],[4.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.66410482],\n",
       "       [2.19823778],\n",
       "       [2.56002814],\n",
       "       [2.9366866 ],\n",
       "       [3.20926838],\n",
       "       [2.89208231],\n",
       "       [3.04571931],\n",
       "       [3.31334506],\n",
       "       [2.75826943],\n",
       "       [3.20431235],\n",
       "       [2.85243405],\n",
       "       [2.50551178]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elec_model.predict( df[['전기생산량']] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter( df[['전기생산량']], df[['전기사용량']]  )\n",
    "# plt.xlim(0,df['전기생산량'].max() )\n",
    "# plt.ylim(0,df['전기사용량'].max() )\n",
    "\n",
    "plt.xlabel('전기 생산량')\n",
    "plt.ylabel('전기 사용량')\n",
    "\n",
    "plt.plot( df[['전기생산량']] , elec_model.predict( df[['전기생산량']] )  , 'c--' )\n",
    "\n",
    "plt.plot( [3.2,4.5], elec_model.predict( [[3.2],[4.5]]), 'rD')\n",
    "plt.legend(['회귀선', '전기 생산량이 3.2, 4.5일 때'])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) 회귀모델 검증\n",
    "- 결정계수(Coefficient of Determination, $ R^2 $ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ SSR( 예측값 - 평균값 )^2의 합 $\n",
    "\n",
    "$ ( predictions - dist.mean() )^2 $\n",
    "\n",
    "$ SST( 실제값 - 평균값 )^2의  합 $\n",
    "\n",
    "\n",
    "$ SSE( 실제값 - 예측값 )^2의  합 $\n",
    "\n",
    "$ R^2 = SSR/SST $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure( figsize=(8,6))\n",
    "plt.xlabel('speed')\n",
    "plt.ylabel('dist')\n",
    "# 예측값\n",
    "predictions =model_car.predict(  car_df[['speed']] )\n",
    "plt.scatter(car_df.speed, car_df.dist)\n",
    "plt.plot( car_df.speed, predictions, 'c--') \n",
    "\n",
    "plt.axhline( car_df.dist.mean(), color='blue', ls=':') # dist의 평균값\n",
    "plt.legend(['회귀선','dist의 평균값'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6510793807582509"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_car.score(car_df[['speed']] , car_df[['dist']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6510793807582509"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "y_predict = model_car.predict( car_df[['speed']])\n",
    "r2_score( car_df.dist, y_predict ) # y_true, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 머신러닝-학습을 통한 선형회귀\n",
    "- x값\n",
    "    - 반드시 x값을 데이터프레임, 행렬로 주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 3.21, NNZs: 1, Bias: 1.394093, T: 45, Avg. loss: 16100.018420\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.15, NNZs: 1, Bias: 1.022756, T: 90, Avg. loss: 332.036098\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.82, NNZs: 1, Bias: 0.681264, T: 135, Avg. loss: 199.246792\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.24, NNZs: 1, Bias: 0.377368, T: 180, Avg. loss: 255.565045\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.01, NNZs: 1, Bias: 0.011733, T: 225, Avg. loss: 202.673247\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 6.04, NNZs: 1, Bias: 0.032715, T: 270, Avg. loss: 185.681713\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.53, NNZs: 1, Bias: -0.373986, T: 315, Avg. loss: 252.629231\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.77, NNZs: 1, Bias: -0.495908, T: 360, Avg. loss: 182.064567\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 8 epochs took 0.01 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=0.0001, average=False, early_stopping=True, epsilon=0.1,\n",
       "             eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "             learning_rate='invscaling', loss='squared_loss', max_iter=100,\n",
       "             n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "             shuffle=True, tol=0.001, validation_fraction=0.1, verbose=1,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelSGD = SGDRegressor(\n",
    "                loss='squared_loss',  # mean squared error\n",
    "                alpha=0.0001,  # learn rate\n",
    "                max_iter=100, # 학습 횟수\n",
    "                early_stopping=True,  # cost값에 변화가 없다(학습 충분)면 최저점에 도달했다고 인식\n",
    "                verbose=1, # 학습횟수와 learn rate등을 출력하는 인자\n",
    "                    )\n",
    "# model fit - Expected 2D array => x값은 무조건 행렬, 데이터프레임으로 주기\n",
    "modelSGD.fit( car_df[['speed']], car_df['dist'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습을 통한 coef, intercept 값\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([2.76910512]), array([-0.49590795]))"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print( '학습을 통한 coef, intercept 값' )\n",
    "modelSGD.coef_, modelSGD.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 딥러닝-학습을 이용한 선형회귀\n",
    "- Perceptron\n",
    "- 딥러닝은 hidden layer가 구성되어 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNN = MLPRegressor( \n",
    "                            hidden_layer_sizes=(100,10), # vector, matrix, 3차원도 올 수 있음. 1개의 히든레이어에 100개로 구성\n",
    "                            solver='adam', # 향상된 optimizor\n",
    "                            alpha=0.1, # learn rate\n",
    "                            max_iter=5000, # 학습 횟수\n",
    "                            verbose=1, # 값 보이게\n",
    "#                             early_stopping = True\n",
    "                        )\n",
    "# hidden_layer_sizes( 히든레이어 크기, 출력갯수,  히든레이어 갯수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1450.43277743\n",
      "Iteration 2, loss = 1437.10798377\n",
      "Iteration 3, loss = 1422.52148212\n",
      "Iteration 4, loss = 1407.65000321\n",
      "Iteration 5, loss = 1392.51406414\n",
      "Iteration 6, loss = 1377.29417655\n",
      "Iteration 7, loss = 1362.06642078\n",
      "Iteration 8, loss = 1346.85632224\n",
      "Iteration 9, loss = 1331.66713994\n",
      "Iteration 10, loss = 1316.51579198\n",
      "Iteration 11, loss = 1301.38025856\n",
      "Iteration 12, loss = 1286.23583667\n",
      "Iteration 13, loss = 1271.12800437\n",
      "Iteration 14, loss = 1256.09924253\n",
      "Iteration 15, loss = 1241.15312007\n",
      "Iteration 16, loss = 1226.29233217\n",
      "Iteration 17, loss = 1211.51671014\n",
      "Iteration 18, loss = 1196.88427573\n",
      "Iteration 19, loss = 1183.30579133\n",
      "Iteration 20, loss = 1170.85681968\n",
      "Iteration 21, loss = 1158.45274631\n",
      "Iteration 22, loss = 1147.72485431\n",
      "Iteration 23, loss = 1139.69143114\n",
      "Iteration 24, loss = 1132.43774608\n",
      "Iteration 25, loss = 1125.62723500\n",
      "Iteration 26, loss = 1118.72391953\n",
      "Iteration 27, loss = 1111.72183725\n",
      "Iteration 28, loss = 1104.62596784\n",
      "Iteration 29, loss = 1097.43406413\n",
      "Iteration 30, loss = 1090.15944902\n",
      "Iteration 31, loss = 1082.80829987\n",
      "Iteration 32, loss = 1075.38768758\n",
      "Iteration 33, loss = 1067.90285898\n",
      "Iteration 34, loss = 1060.34773474\n",
      "Iteration 35, loss = 1052.72559984\n",
      "Iteration 36, loss = 1045.03492573\n",
      "Iteration 37, loss = 1037.27188386\n",
      "Iteration 38, loss = 1029.43549253\n",
      "Iteration 39, loss = 1021.52532384\n",
      "Iteration 40, loss = 1013.54208701\n",
      "Iteration 41, loss = 1005.47903515\n",
      "Iteration 42, loss = 997.33494630\n",
      "Iteration 43, loss = 989.10925024\n",
      "Iteration 44, loss = 980.80349537\n",
      "Iteration 45, loss = 972.41744119\n",
      "Iteration 46, loss = 963.95083321\n",
      "Iteration 47, loss = 955.40353436\n",
      "Iteration 48, loss = 946.77557135\n",
      "Iteration 49, loss = 938.06663894\n",
      "Iteration 50, loss = 929.27668016\n",
      "Iteration 51, loss = 920.40583660\n",
      "Iteration 52, loss = 911.45448792\n",
      "Iteration 53, loss = 902.42338413\n",
      "Iteration 54, loss = 893.31386091\n",
      "Iteration 55, loss = 884.12430178\n",
      "Iteration 56, loss = 874.85640764\n",
      "Iteration 57, loss = 865.51207802\n",
      "Iteration 58, loss = 856.09130481\n",
      "Iteration 59, loss = 846.58293217\n",
      "Iteration 60, loss = 836.98865759\n",
      "Iteration 61, loss = 827.30837646\n",
      "Iteration 62, loss = 817.54116805\n",
      "Iteration 63, loss = 807.69782704\n",
      "Iteration 64, loss = 797.78000057\n",
      "Iteration 65, loss = 787.78975719\n",
      "Iteration 66, loss = 777.72917832\n",
      "Iteration 67, loss = 767.60110858\n",
      "Iteration 68, loss = 757.40802816\n",
      "Iteration 69, loss = 747.15280629\n",
      "Iteration 70, loss = 736.83848810\n",
      "Iteration 71, loss = 726.46815376\n",
      "Iteration 72, loss = 716.04328584\n",
      "Iteration 73, loss = 705.56840205\n",
      "Iteration 74, loss = 695.04613692\n",
      "Iteration 75, loss = 684.47381500\n",
      "Iteration 76, loss = 673.85217740\n",
      "Iteration 77, loss = 663.18613641\n",
      "Iteration 78, loss = 652.47935877\n",
      "Iteration 79, loss = 641.72017512\n",
      "Iteration 80, loss = 630.91193768\n",
      "Iteration 81, loss = 620.05060385\n",
      "Iteration 82, loss = 609.15203734\n",
      "Iteration 83, loss = 598.23599415\n",
      "Iteration 84, loss = 587.30792542\n",
      "Iteration 85, loss = 576.37418176\n",
      "Iteration 86, loss = 565.44135535\n",
      "Iteration 87, loss = 554.51615149\n",
      "Iteration 88, loss = 543.60536370\n",
      "Iteration 89, loss = 532.71587160\n",
      "Iteration 90, loss = 521.85464231\n",
      "Iteration 91, loss = 511.02873030\n",
      "Iteration 92, loss = 500.24527417\n",
      "Iteration 93, loss = 489.51149182\n",
      "Iteration 94, loss = 478.83467113\n",
      "Iteration 95, loss = 468.22217120\n",
      "Iteration 96, loss = 457.68139854\n",
      "Iteration 97, loss = 447.21979817\n",
      "Iteration 98, loss = 436.84483993\n",
      "Iteration 99, loss = 426.56400331\n",
      "Iteration 100, loss = 416.38476107\n",
      "Iteration 101, loss = 406.31456225\n",
      "Iteration 102, loss = 396.36081434\n",
      "Iteration 103, loss = 386.53086541\n",
      "Iteration 104, loss = 376.83198496\n",
      "Iteration 105, loss = 367.27134385\n",
      "Iteration 106, loss = 357.85599336\n",
      "Iteration 107, loss = 348.59284339\n",
      "Iteration 108, loss = 339.48864154\n",
      "Iteration 109, loss = 330.54994911\n",
      "Iteration 110, loss = 321.78311476\n",
      "Iteration 111, loss = 313.19427490\n",
      "Iteration 112, loss = 304.78930148\n",
      "Iteration 113, loss = 296.57378132\n",
      "Iteration 114, loss = 288.55301203\n",
      "Iteration 115, loss = 280.73197045\n",
      "Iteration 116, loss = 273.11528720\n",
      "Iteration 117, loss = 265.70723844\n",
      "Iteration 118, loss = 258.51165910\n",
      "Iteration 119, loss = 251.53217982\n",
      "Iteration 120, loss = 244.77181985\n",
      "Iteration 121, loss = 238.23314280\n",
      "Iteration 122, loss = 231.91837887\n",
      "Iteration 123, loss = 225.82922921\n",
      "Iteration 124, loss = 219.96693579\n",
      "Iteration 125, loss = 214.33223975\n",
      "Iteration 126, loss = 208.92534844\n",
      "Iteration 127, loss = 203.74589208\n",
      "Iteration 128, loss = 198.79277464\n",
      "Iteration 129, loss = 194.06643455\n",
      "Iteration 130, loss = 189.56339886\n",
      "Iteration 131, loss = 185.28038822\n",
      "Iteration 132, loss = 181.21716288\n",
      "Iteration 133, loss = 177.36931068\n",
      "Iteration 134, loss = 173.73571278\n",
      "Iteration 135, loss = 170.30842074\n",
      "Iteration 136, loss = 167.08234535\n",
      "Iteration 137, loss = 164.05600657\n",
      "Iteration 138, loss = 161.22210414\n",
      "Iteration 139, loss = 158.57680583\n",
      "Iteration 140, loss = 156.11226670\n",
      "Iteration 141, loss = 153.82089420\n",
      "Iteration 142, loss = 151.69764949\n",
      "Iteration 143, loss = 149.73592703\n",
      "Iteration 144, loss = 147.92927277\n",
      "Iteration 145, loss = 146.26747741\n",
      "Iteration 146, loss = 144.74470315\n",
      "Iteration 147, loss = 143.35736345\n",
      "Iteration 148, loss = 142.09476860\n",
      "Iteration 149, loss = 140.94903430\n",
      "Iteration 150, loss = 139.91497237\n",
      "Iteration 151, loss = 138.98500954\n",
      "Iteration 152, loss = 138.14558871\n",
      "Iteration 153, loss = 137.40401414\n",
      "Iteration 154, loss = 136.73629293\n",
      "Iteration 155, loss = 136.14262220\n",
      "Iteration 156, loss = 135.62411731\n",
      "Iteration 157, loss = 135.17323329\n",
      "Iteration 158, loss = 134.77566152\n",
      "Iteration 159, loss = 134.42334353\n",
      "Iteration 160, loss = 134.12973458\n",
      "Iteration 161, loss = 133.87708366\n",
      "Iteration 162, loss = 133.65521926\n",
      "Iteration 163, loss = 133.46494898\n",
      "Iteration 164, loss = 133.30891246\n",
      "Iteration 165, loss = 133.18152553\n",
      "Iteration 166, loss = 133.07093640\n",
      "Iteration 167, loss = 132.97755871\n",
      "Iteration 168, loss = 132.90497304\n",
      "Iteration 169, loss = 132.84810857\n",
      "Iteration 170, loss = 132.79805883\n",
      "Iteration 171, loss = 132.75871036\n",
      "Iteration 172, loss = 132.72674859\n",
      "Iteration 173, loss = 132.70048293\n",
      "Iteration 174, loss = 132.67855200\n",
      "Iteration 175, loss = 132.65963026\n",
      "Iteration 176, loss = 132.64064943\n",
      "Iteration 177, loss = 132.62265861\n",
      "Iteration 178, loss = 132.60232939\n",
      "Iteration 179, loss = 132.58061620\n",
      "Iteration 180, loss = 132.56321610\n",
      "Iteration 181, loss = 132.55014789\n",
      "Iteration 182, loss = 132.53637029\n",
      "Iteration 183, loss = 132.52174938\n",
      "Iteration 184, loss = 132.50621509\n",
      "Iteration 185, loss = 132.48975183\n",
      "Iteration 186, loss = 132.47237904\n",
      "Iteration 187, loss = 132.45416264\n",
      "Iteration 188, loss = 132.43518676\n",
      "Iteration 189, loss = 132.41555612\n",
      "Iteration 190, loss = 132.39538678\n",
      "Iteration 191, loss = 132.37479956\n",
      "Iteration 192, loss = 132.35391423\n",
      "Iteration 193, loss = 132.33284450\n",
      "Iteration 194, loss = 132.31169385\n",
      "Iteration 195, loss = 132.29055232\n",
      "Iteration 196, loss = 132.26949419\n",
      "Iteration 197, loss = 132.24857652\n",
      "Iteration 198, loss = 132.22783866\n",
      "Iteration 199, loss = 132.20730248\n",
      "Iteration 200, loss = 132.18697338\n",
      "Iteration 201, loss = 132.16684193\n",
      "Iteration 202, loss = 132.14688973\n",
      "Iteration 203, loss = 132.12707349\n",
      "Iteration 204, loss = 132.10736456\n",
      "Iteration 205, loss = 132.08771500\n",
      "Iteration 206, loss = 132.06807884\n",
      "Iteration 207, loss = 132.04841089\n",
      "Iteration 208, loss = 132.02866909\n",
      "Iteration 209, loss = 132.00881713\n",
      "Iteration 210, loss = 131.98882208\n",
      "Iteration 211, loss = 131.96866277\n",
      "Iteration 212, loss = 131.94832272\n",
      "Iteration 213, loss = 131.92779388\n",
      "Iteration 214, loss = 131.90707534\n",
      "Iteration 215, loss = 131.88617251\n",
      "Iteration 216, loss = 131.86509602\n",
      "Iteration 217, loss = 131.84386047\n",
      "Iteration 218, loss = 131.82248307\n",
      "Iteration 219, loss = 131.80098234\n",
      "Iteration 220, loss = 131.77936918\n",
      "Iteration 221, loss = 131.75761414\n",
      "Iteration 222, loss = 131.73576214\n",
      "Iteration 223, loss = 131.71381974\n",
      "Iteration 224, loss = 131.69177245\n",
      "Iteration 225, loss = 131.66963102\n",
      "Iteration 226, loss = 131.64739200\n",
      "Iteration 227, loss = 131.62504905\n",
      "Iteration 228, loss = 131.60259488\n",
      "Iteration 229, loss = 131.57959691\n",
      "Iteration 230, loss = 131.55625410\n",
      "Iteration 231, loss = 131.53209660\n",
      "Iteration 232, loss = 131.50721325\n",
      "Iteration 233, loss = 131.47884513\n",
      "Iteration 234, loss = 131.44815802\n",
      "Iteration 235, loss = 131.41308129\n",
      "Iteration 236, loss = 131.37420551\n",
      "Iteration 237, loss = 131.33454407\n",
      "Iteration 238, loss = 131.30471597\n",
      "Iteration 239, loss = 131.27733929\n",
      "Iteration 240, loss = 131.24953273\n",
      "Iteration 241, loss = 131.22155485\n",
      "Iteration 242, loss = 131.19202218\n",
      "Iteration 243, loss = 131.16064218\n",
      "Iteration 244, loss = 131.13167661\n",
      "Iteration 245, loss = 131.10246403\n",
      "Iteration 246, loss = 131.07313377\n",
      "Iteration 247, loss = 131.04355008\n",
      "Iteration 248, loss = 131.01344598\n",
      "Iteration 249, loss = 130.98351333\n",
      "Iteration 250, loss = 130.95298088\n",
      "Iteration 251, loss = 130.92160873\n",
      "Iteration 252, loss = 130.89045863\n",
      "Iteration 253, loss = 130.85893211\n",
      "Iteration 254, loss = 130.82681788\n",
      "Iteration 255, loss = 130.79439939\n",
      "Iteration 256, loss = 130.76203495\n",
      "Iteration 257, loss = 130.72977735\n",
      "Iteration 258, loss = 130.70531927\n",
      "Iteration 259, loss = 130.66395151\n",
      "Iteration 260, loss = 130.63130984\n",
      "Iteration 261, loss = 130.59806303\n",
      "Iteration 262, loss = 130.56317658\n",
      "Iteration 263, loss = 130.52673109\n",
      "Iteration 264, loss = 130.48766190\n",
      "Iteration 265, loss = 130.44797355\n",
      "Iteration 266, loss = 130.41214941\n",
      "Iteration 267, loss = 130.37619259\n",
      "Iteration 268, loss = 130.33961329\n",
      "Iteration 269, loss = 130.31258906\n",
      "Iteration 270, loss = 130.26696826\n",
      "Iteration 271, loss = 130.22804658\n",
      "Iteration 272, loss = 130.19083595\n",
      "Iteration 273, loss = 130.15326979\n",
      "Iteration 274, loss = 130.11742459\n",
      "Iteration 275, loss = 130.07863649\n",
      "Iteration 276, loss = 130.04124292\n",
      "Iteration 277, loss = 130.00321173\n",
      "Iteration 278, loss = 129.96507872\n",
      "Iteration 279, loss = 129.93209117\n",
      "Iteration 280, loss = 129.88969458\n",
      "Iteration 281, loss = 129.85144473\n",
      "Iteration 282, loss = 129.81329499\n",
      "Iteration 283, loss = 129.77624011\n",
      "Iteration 284, loss = 129.73792363\n",
      "Iteration 285, loss = 129.69945083\n",
      "Iteration 286, loss = 129.65982113\n",
      "Iteration 287, loss = 129.61933200\n",
      "Iteration 288, loss = 129.58026387\n",
      "Iteration 289, loss = 129.54072338\n",
      "Iteration 290, loss = 129.50025343\n",
      "Iteration 291, loss = 129.46721961\n",
      "Iteration 292, loss = 129.41957627\n",
      "Iteration 293, loss = 129.37972969\n",
      "Iteration 294, loss = 129.33916689\n",
      "Iteration 295, loss = 129.29799914\n",
      "Iteration 296, loss = 129.25728861\n",
      "Iteration 297, loss = 129.21704266\n",
      "Iteration 298, loss = 129.17866370\n",
      "Iteration 299, loss = 129.13114955\n",
      "Iteration 300, loss = 129.08690148\n",
      "Iteration 301, loss = 129.04509917\n",
      "Iteration 302, loss = 129.00391189\n",
      "Iteration 303, loss = 128.96237664\n",
      "Iteration 304, loss = 128.91990748\n",
      "Iteration 305, loss = 128.87666461\n",
      "Iteration 306, loss = 128.83226354\n",
      "Iteration 307, loss = 128.78969374\n",
      "Iteration 308, loss = 128.75113275\n",
      "Iteration 309, loss = 128.70391771\n",
      "Iteration 310, loss = 128.66157588\n",
      "Iteration 311, loss = 128.61840642\n",
      "Iteration 312, loss = 128.57405275\n",
      "Iteration 313, loss = 128.53039483\n",
      "Iteration 314, loss = 128.49084178\n",
      "Iteration 315, loss = 128.44150932\n",
      "Iteration 316, loss = 128.39722541\n",
      "Iteration 317, loss = 128.35351485\n",
      "Iteration 318, loss = 128.30847549\n",
      "Iteration 319, loss = 128.26313785\n",
      "Iteration 320, loss = 128.21742420\n",
      "Iteration 321, loss = 128.17188383\n",
      "Iteration 322, loss = 128.12564669\n",
      "Iteration 323, loss = 128.07960030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 324, loss = 128.03619472\n",
      "Iteration 325, loss = 127.99581248\n",
      "Iteration 326, loss = 127.94201951\n",
      "Iteration 327, loss = 127.89762810\n",
      "Iteration 328, loss = 127.85268731\n",
      "Iteration 329, loss = 127.80767652\n",
      "Iteration 330, loss = 127.76128664\n",
      "Iteration 331, loss = 127.71581699\n",
      "Iteration 332, loss = 127.67626438\n",
      "Iteration 333, loss = 127.62458807\n",
      "Iteration 334, loss = 127.57936298\n",
      "Iteration 335, loss = 127.53260748\n",
      "Iteration 336, loss = 127.48593164\n",
      "Iteration 337, loss = 127.44065718\n",
      "Iteration 338, loss = 127.39610910\n",
      "Iteration 339, loss = 127.34773217\n",
      "Iteration 340, loss = 127.30202544\n",
      "Iteration 341, loss = 127.25572654\n",
      "Iteration 342, loss = 127.20884750\n",
      "Iteration 343, loss = 127.16149803\n",
      "Iteration 344, loss = 127.11422538\n",
      "Iteration 345, loss = 127.07230924\n",
      "Iteration 346, loss = 127.01961159\n",
      "Iteration 347, loss = 126.97389345\n",
      "Iteration 348, loss = 126.92787066\n",
      "Iteration 349, loss = 126.88183487\n",
      "Iteration 350, loss = 126.83342968\n",
      "Iteration 351, loss = 126.78743446\n",
      "Iteration 352, loss = 126.74030974\n",
      "Iteration 353, loss = 126.69275710\n",
      "Iteration 354, loss = 126.64534953\n",
      "Iteration 355, loss = 126.59957094\n",
      "Iteration 356, loss = 126.55693371\n",
      "Iteration 357, loss = 126.50526438\n",
      "Iteration 358, loss = 126.45758784\n",
      "Iteration 359, loss = 126.40952084\n",
      "Iteration 360, loss = 126.36165070\n",
      "Iteration 361, loss = 126.31591616\n",
      "Iteration 362, loss = 126.27009911\n",
      "Iteration 363, loss = 126.21970933\n",
      "Iteration 364, loss = 126.17259392\n",
      "Iteration 365, loss = 126.12565186\n",
      "Iteration 366, loss = 126.07832352\n",
      "Iteration 367, loss = 126.03674584\n",
      "Iteration 368, loss = 125.98171458\n",
      "Iteration 369, loss = 125.93624879\n",
      "Iteration 370, loss = 125.89012821\n",
      "Iteration 371, loss = 125.84230623\n",
      "Iteration 372, loss = 125.79426930\n",
      "Iteration 373, loss = 125.74632429\n",
      "Iteration 374, loss = 125.70093520\n",
      "Iteration 375, loss = 125.65447322\n",
      "Iteration 376, loss = 125.60777110\n",
      "Iteration 377, loss = 125.55968183\n",
      "Iteration 378, loss = 125.51047512\n",
      "Iteration 379, loss = 125.46380047\n",
      "Iteration 380, loss = 125.41672822\n",
      "Iteration 381, loss = 125.36988159\n",
      "Iteration 382, loss = 125.32213498\n",
      "Iteration 383, loss = 125.27358923\n",
      "Iteration 384, loss = 125.22679654\n",
      "Iteration 385, loss = 125.18117600\n",
      "Iteration 386, loss = 125.13418075\n",
      "Iteration 387, loss = 125.08679629\n",
      "Iteration 388, loss = 125.03945386\n",
      "Iteration 389, loss = 124.99313215\n",
      "Iteration 390, loss = 124.94498942\n",
      "Iteration 391, loss = 124.89694087\n",
      "Iteration 392, loss = 124.84990747\n",
      "Iteration 393, loss = 124.80364577\n",
      "Iteration 394, loss = 124.75747072\n",
      "Iteration 395, loss = 124.70834507\n",
      "Iteration 396, loss = 124.65741342\n",
      "Iteration 397, loss = 124.60816237\n",
      "Iteration 398, loss = 124.56151654\n",
      "Iteration 399, loss = 124.51473552\n",
      "Iteration 400, loss = 124.46720599\n",
      "Iteration 401, loss = 124.41887882\n",
      "Iteration 402, loss = 124.37039255\n",
      "Iteration 403, loss = 124.32315530\n",
      "Iteration 404, loss = 124.27536192\n",
      "Iteration 405, loss = 124.22713858\n",
      "Iteration 406, loss = 124.17841236\n",
      "Iteration 407, loss = 124.13049731\n",
      "Iteration 408, loss = 124.08265613\n",
      "Iteration 409, loss = 124.03457236\n",
      "Iteration 410, loss = 123.98664767\n",
      "Iteration 411, loss = 123.93766215\n",
      "Iteration 412, loss = 123.88758874\n",
      "Iteration 413, loss = 123.83843735\n",
      "Iteration 414, loss = 123.78938348\n",
      "Iteration 415, loss = 123.74059710\n",
      "Iteration 416, loss = 123.69244118\n",
      "Iteration 417, loss = 123.64348926\n",
      "Iteration 418, loss = 123.60141213\n",
      "Iteration 419, loss = 123.54630906\n",
      "Iteration 420, loss = 123.49882790\n",
      "Iteration 421, loss = 123.45117751\n",
      "Iteration 422, loss = 123.40331977\n",
      "Iteration 423, loss = 123.35515772\n",
      "Iteration 424, loss = 123.30781925\n",
      "Iteration 425, loss = 123.25991629\n",
      "Iteration 426, loss = 123.21104983\n",
      "Iteration 427, loss = 123.16320977\n",
      "Iteration 428, loss = 123.11546268\n",
      "Iteration 429, loss = 123.06690049\n",
      "Iteration 430, loss = 123.01901010\n",
      "Iteration 431, loss = 122.97049698\n",
      "Iteration 432, loss = 122.92201278\n",
      "Iteration 433, loss = 122.87374863\n",
      "Iteration 434, loss = 122.82571508\n",
      "Iteration 435, loss = 122.77668000\n",
      "Iteration 436, loss = 122.73066445\n",
      "Iteration 437, loss = 122.68271685\n",
      "Iteration 438, loss = 122.63410514\n",
      "Iteration 439, loss = 122.59080545\n",
      "Iteration 440, loss = 122.53911370\n",
      "Iteration 441, loss = 122.49264075\n",
      "Iteration 442, loss = 122.44500353\n",
      "Iteration 443, loss = 122.39847254\n",
      "Iteration 444, loss = 122.35248174\n",
      "Iteration 445, loss = 122.30526789\n",
      "Iteration 446, loss = 122.25838399\n",
      "Iteration 447, loss = 122.21072751\n",
      "Iteration 448, loss = 122.16424326\n",
      "Iteration 449, loss = 122.11838644\n",
      "Iteration 450, loss = 122.07216384\n",
      "Iteration 451, loss = 122.02402476\n",
      "Iteration 452, loss = 121.97629340\n",
      "Iteration 453, loss = 121.93194735\n",
      "Iteration 454, loss = 121.88566495\n",
      "Iteration 455, loss = 121.83781427\n",
      "Iteration 456, loss = 121.79114233\n",
      "Iteration 457, loss = 121.74433026\n",
      "Iteration 458, loss = 121.69737719\n",
      "Iteration 459, loss = 121.65127980\n",
      "Iteration 460, loss = 121.60498359\n",
      "Iteration 461, loss = 121.55719104\n",
      "Iteration 462, loss = 121.51117953\n",
      "Iteration 463, loss = 121.46592321\n",
      "Iteration 464, loss = 121.42043810\n",
      "Iteration 465, loss = 121.37868827\n",
      "Iteration 466, loss = 121.33016374\n",
      "Iteration 467, loss = 121.28482576\n",
      "Iteration 468, loss = 121.24016878\n",
      "Iteration 469, loss = 121.19523392\n",
      "Iteration 470, loss = 121.15205766\n",
      "Iteration 471, loss = 121.10743237\n",
      "Iteration 472, loss = 121.06304922\n",
      "Iteration 473, loss = 121.01878582\n",
      "Iteration 474, loss = 120.97549368\n",
      "Iteration 475, loss = 120.93074615\n",
      "Iteration 476, loss = 120.88522045\n",
      "Iteration 477, loss = 120.84047357\n",
      "Iteration 478, loss = 120.79696391\n",
      "Iteration 479, loss = 120.75295363\n",
      "Iteration 480, loss = 120.70807068\n",
      "Iteration 481, loss = 120.66441800\n",
      "Iteration 482, loss = 120.62115950\n",
      "Iteration 483, loss = 120.57781451\n",
      "Iteration 484, loss = 120.53315890\n",
      "Iteration 485, loss = 120.48876623\n",
      "Iteration 486, loss = 120.44485048\n",
      "Iteration 487, loss = 120.40184073\n",
      "Iteration 488, loss = 120.35878976\n",
      "Iteration 489, loss = 120.31470811\n",
      "Iteration 490, loss = 120.27190059\n",
      "Iteration 491, loss = 120.22872906\n",
      "Iteration 492, loss = 120.18556705\n",
      "Iteration 493, loss = 120.14365393\n",
      "Iteration 494, loss = 120.10202409\n",
      "Iteration 495, loss = 120.06291828\n",
      "Iteration 496, loss = 120.01678651\n",
      "Iteration 497, loss = 119.97616044\n",
      "Iteration 498, loss = 119.93539354\n",
      "Iteration 499, loss = 119.89380144\n",
      "Iteration 500, loss = 119.85353813\n",
      "Iteration 501, loss = 119.81367486\n",
      "Iteration 502, loss = 119.77179213\n",
      "Iteration 503, loss = 119.73018192\n",
      "Iteration 504, loss = 119.68956912\n",
      "Iteration 505, loss = 119.64927256\n",
      "Iteration 506, loss = 119.60970879\n",
      "Iteration 507, loss = 119.56862049\n",
      "Iteration 508, loss = 119.52766911\n",
      "Iteration 509, loss = 119.48693215\n",
      "Iteration 510, loss = 119.44710417\n",
      "Iteration 511, loss = 119.40761130\n",
      "Iteration 512, loss = 119.36689315\n",
      "Iteration 513, loss = 119.32608618\n",
      "Iteration 514, loss = 119.28576442\n",
      "Iteration 515, loss = 119.24275890\n",
      "Iteration 516, loss = 119.19923203\n",
      "Iteration 517, loss = 119.15657198\n",
      "Iteration 518, loss = 119.11642427\n",
      "Iteration 519, loss = 119.07569802\n",
      "Iteration 520, loss = 119.03529911\n",
      "Iteration 521, loss = 118.99634365\n",
      "Iteration 522, loss = 118.95674890\n",
      "Iteration 523, loss = 118.91671560\n",
      "Iteration 524, loss = 118.87674780\n",
      "Iteration 525, loss = 118.83719789\n",
      "Iteration 526, loss = 118.79816740\n",
      "Iteration 527, loss = 118.75928722\n",
      "Iteration 528, loss = 118.72075960\n",
      "Iteration 529, loss = 118.68063826\n",
      "Iteration 530, loss = 118.64101628\n",
      "Iteration 531, loss = 118.60116200\n",
      "Iteration 532, loss = 118.56324496\n",
      "Iteration 533, loss = 118.52413661\n",
      "Iteration 534, loss = 118.48489040\n",
      "Iteration 535, loss = 118.44632787\n",
      "Iteration 536, loss = 118.40906629\n",
      "Iteration 537, loss = 118.37055351\n",
      "Iteration 538, loss = 118.33035513\n",
      "Iteration 539, loss = 118.29026475\n",
      "Iteration 540, loss = 118.25191896\n",
      "Iteration 541, loss = 118.21356229\n",
      "Iteration 542, loss = 118.17443464\n",
      "Iteration 543, loss = 118.13682369\n",
      "Iteration 544, loss = 118.09910829\n",
      "Iteration 545, loss = 118.07193127\n",
      "Iteration 546, loss = 118.02570985\n",
      "Iteration 547, loss = 117.98887408\n",
      "Iteration 548, loss = 117.95192094\n",
      "Iteration 549, loss = 117.91461805\n",
      "Iteration 550, loss = 117.88075952\n",
      "Iteration 551, loss = 117.84462198\n",
      "Iteration 552, loss = 117.80732180\n",
      "Iteration 553, loss = 117.77118581\n",
      "Iteration 554, loss = 117.73607481\n",
      "Iteration 555, loss = 117.70060573\n",
      "Iteration 556, loss = 117.66472107\n",
      "Iteration 557, loss = 117.62936131\n",
      "Iteration 558, loss = 117.59384171\n",
      "Iteration 559, loss = 117.55712939\n",
      "Iteration 560, loss = 117.52325112\n",
      "Iteration 561, loss = 117.48925101\n",
      "Iteration 562, loss = 117.45387596\n",
      "Iteration 563, loss = 117.42001281\n",
      "Iteration 564, loss = 117.38477070\n",
      "Iteration 565, loss = 117.34853119\n",
      "Iteration 566, loss = 117.31546525\n",
      "Iteration 567, loss = 117.28156317\n",
      "Iteration 568, loss = 117.24632277\n",
      "Iteration 569, loss = 117.21506343\n",
      "Iteration 570, loss = 117.18019287\n",
      "Iteration 571, loss = 117.14728687\n",
      "Iteration 572, loss = 117.11626853\n",
      "Iteration 573, loss = 117.08369468\n",
      "Iteration 574, loss = 117.05140968\n",
      "Iteration 575, loss = 117.01952620\n",
      "Iteration 576, loss = 116.98855311\n",
      "Iteration 577, loss = 116.95745886\n",
      "Iteration 578, loss = 116.92610544\n",
      "Iteration 579, loss = 116.89437482\n",
      "Iteration 580, loss = 116.86209883\n",
      "Iteration 581, loss = 116.83026668\n",
      "Iteration 582, loss = 116.79954575\n",
      "Iteration 583, loss = 116.76765257\n",
      "Iteration 584, loss = 116.73643749\n",
      "Iteration 585, loss = 116.70609160\n",
      "Iteration 586, loss = 116.67476619\n",
      "Iteration 587, loss = 116.64447760\n",
      "Iteration 588, loss = 116.61360969\n",
      "Iteration 589, loss = 116.58360442\n",
      "Iteration 590, loss = 116.55327914\n",
      "Iteration 591, loss = 116.52283160\n",
      "Iteration 592, loss = 116.49317065\n",
      "Iteration 593, loss = 116.46415106\n",
      "Iteration 594, loss = 116.43301707\n",
      "Iteration 595, loss = 116.40402840\n",
      "Iteration 596, loss = 116.37576537\n",
      "Iteration 597, loss = 116.34658180\n",
      "Iteration 598, loss = 116.31708030\n",
      "Iteration 599, loss = 116.28806015\n",
      "Iteration 600, loss = 116.25971928\n",
      "Iteration 601, loss = 116.23300702\n",
      "Iteration 602, loss = 116.20478737\n",
      "Iteration 603, loss = 116.17609711\n",
      "Iteration 604, loss = 116.14864125\n",
      "Iteration 605, loss = 116.12121253\n",
      "Iteration 606, loss = 116.09295814\n",
      "Iteration 607, loss = 116.06555720\n",
      "Iteration 608, loss = 116.03799622\n",
      "Iteration 609, loss = 116.01205775\n",
      "Iteration 610, loss = 115.98574338\n",
      "Iteration 611, loss = 115.95843019\n",
      "Iteration 612, loss = 115.93172145\n",
      "Iteration 613, loss = 115.90585624\n",
      "Iteration 614, loss = 115.87973011\n",
      "Iteration 615, loss = 115.85323506\n",
      "Iteration 616, loss = 115.82815906\n",
      "Iteration 617, loss = 115.80265157\n",
      "Iteration 618, loss = 115.77630431\n",
      "Iteration 619, loss = 115.74989433\n",
      "Iteration 620, loss = 115.72529331\n",
      "Iteration 621, loss = 115.69970255\n",
      "Iteration 622, loss = 115.67534391\n",
      "Iteration 623, loss = 115.65062001\n",
      "Iteration 624, loss = 115.62648920\n",
      "Iteration 625, loss = 115.60150934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 626, loss = 115.57675081\n",
      "Iteration 627, loss = 115.55152197\n",
      "Iteration 628, loss = 115.52697915\n",
      "Iteration 629, loss = 115.50474545\n",
      "Iteration 630, loss = 115.48110268\n",
      "Iteration 631, loss = 115.45678477\n",
      "Iteration 632, loss = 115.43304382\n",
      "Iteration 633, loss = 115.41144420\n",
      "Iteration 634, loss = 115.38850119\n",
      "Iteration 635, loss = 115.36544979\n",
      "Iteration 636, loss = 115.34153224\n",
      "Iteration 637, loss = 115.31947696\n",
      "Iteration 638, loss = 115.29673229\n",
      "Iteration 639, loss = 115.27352461\n",
      "Iteration 640, loss = 115.25016984\n",
      "Iteration 641, loss = 115.22732819\n",
      "Iteration 642, loss = 115.20593986\n",
      "Iteration 643, loss = 115.18363339\n",
      "Iteration 644, loss = 115.16222282\n",
      "Iteration 645, loss = 115.14124106\n",
      "Iteration 646, loss = 115.11845450\n",
      "Iteration 647, loss = 115.09753570\n",
      "Iteration 648, loss = 115.07633171\n",
      "Iteration 649, loss = 115.05466865\n",
      "Iteration 650, loss = 115.03457570\n",
      "Iteration 651, loss = 115.01265275\n",
      "Iteration 652, loss = 114.99267767\n",
      "Iteration 653, loss = 114.97308232\n",
      "Iteration 654, loss = 114.95131343\n",
      "Iteration 655, loss = 114.92988196\n",
      "Iteration 656, loss = 114.91133946\n",
      "Iteration 657, loss = 114.89180772\n",
      "Iteration 658, loss = 114.87124298\n",
      "Iteration 659, loss = 114.84987960\n",
      "Iteration 660, loss = 114.82990931\n",
      "Iteration 661, loss = 114.81044706\n",
      "Iteration 662, loss = 114.79081748\n",
      "Iteration 663, loss = 114.77203131\n",
      "Iteration 664, loss = 114.75104746\n",
      "Iteration 665, loss = 114.73225883\n",
      "Iteration 666, loss = 114.71357871\n",
      "Iteration 667, loss = 114.69458606\n",
      "Iteration 668, loss = 114.67699297\n",
      "Iteration 669, loss = 114.65796806\n",
      "Iteration 670, loss = 114.64015189\n",
      "Iteration 671, loss = 114.62070278\n",
      "Iteration 672, loss = 114.60387783\n",
      "Iteration 673, loss = 114.58495402\n",
      "Iteration 674, loss = 114.56654524\n",
      "Iteration 675, loss = 114.54888907\n",
      "Iteration 676, loss = 114.53200210\n",
      "Iteration 677, loss = 114.51450242\n",
      "Iteration 678, loss = 114.49604695\n",
      "Iteration 679, loss = 114.47945782\n",
      "Iteration 680, loss = 114.46252734\n",
      "Iteration 681, loss = 114.44548208\n",
      "Iteration 682, loss = 114.42775678\n",
      "Iteration 683, loss = 114.41109707\n",
      "Iteration 684, loss = 114.39471169\n",
      "Iteration 685, loss = 114.37771805\n",
      "Iteration 686, loss = 114.36201231\n",
      "Iteration 687, loss = 114.34595903\n",
      "Iteration 688, loss = 114.32794162\n",
      "Iteration 689, loss = 114.31185312\n",
      "Iteration 690, loss = 114.29692736\n",
      "Iteration 691, loss = 114.27998063\n",
      "Iteration 692, loss = 114.26420198\n",
      "Iteration 693, loss = 114.24838942\n",
      "Iteration 694, loss = 114.23296100\n",
      "Iteration 695, loss = 114.21780006\n",
      "Iteration 696, loss = 114.20153464\n",
      "Iteration 697, loss = 114.18722504\n",
      "Iteration 698, loss = 114.17228776\n",
      "Iteration 699, loss = 114.15655426\n",
      "Iteration 700, loss = 114.14088640\n",
      "Iteration 701, loss = 114.12670714\n",
      "Iteration 702, loss = 114.11311866\n",
      "Iteration 703, loss = 114.09753855\n",
      "Iteration 704, loss = 114.08341123\n",
      "Iteration 705, loss = 114.06840828\n",
      "Iteration 706, loss = 114.05365243\n",
      "Iteration 707, loss = 114.03992654\n",
      "Iteration 708, loss = 114.02729839\n",
      "Iteration 709, loss = 114.01240446\n",
      "Iteration 710, loss = 113.99827990\n",
      "Iteration 711, loss = 113.98405895\n",
      "Iteration 712, loss = 113.97060895\n",
      "Iteration 713, loss = 113.95851288\n",
      "Iteration 714, loss = 113.94467190\n",
      "Iteration 715, loss = 113.92919992\n",
      "Iteration 716, loss = 113.91530569\n",
      "Iteration 717, loss = 113.90279855\n",
      "Iteration 718, loss = 113.88913282\n",
      "Iteration 719, loss = 113.87634028\n",
      "Iteration 720, loss = 113.86395204\n",
      "Iteration 721, loss = 113.85036803\n",
      "Iteration 722, loss = 113.83757018\n",
      "Iteration 723, loss = 113.82507143\n",
      "Iteration 724, loss = 113.81265454\n",
      "Iteration 725, loss = 113.79953681\n",
      "Iteration 726, loss = 113.78902331\n",
      "Iteration 727, loss = 113.77661998\n",
      "Iteration 728, loss = 113.76213853\n",
      "Iteration 729, loss = 113.74902525\n",
      "Iteration 730, loss = 113.73848988\n",
      "Iteration 731, loss = 113.72759211\n",
      "Iteration 732, loss = 113.71560962\n",
      "Iteration 733, loss = 113.70294695\n",
      "Iteration 734, loss = 113.69088826\n",
      "Iteration 735, loss = 113.67800140\n",
      "Iteration 736, loss = 113.66697952\n",
      "Iteration 737, loss = 113.65552171\n",
      "Iteration 738, loss = 113.64408352\n",
      "Iteration 739, loss = 113.63276782\n",
      "Iteration 740, loss = 113.62097227\n",
      "Iteration 741, loss = 113.61025953\n",
      "Iteration 742, loss = 113.59897574\n",
      "Iteration 743, loss = 113.58782387\n",
      "Iteration 744, loss = 113.57625373\n",
      "Iteration 745, loss = 113.56393557\n",
      "Iteration 746, loss = 113.55368513\n",
      "Iteration 747, loss = 113.54423819\n",
      "Iteration 748, loss = 113.53324844\n",
      "Iteration 749, loss = 113.52180374\n",
      "Iteration 750, loss = 113.51099078\n",
      "Iteration 751, loss = 113.50031784\n",
      "Iteration 752, loss = 113.48984294\n",
      "Iteration 753, loss = 113.47960029\n",
      "Iteration 754, loss = 113.46953740\n",
      "Iteration 755, loss = 113.45945047\n",
      "Iteration 756, loss = 113.44937407\n",
      "Iteration 757, loss = 113.43840644\n",
      "Iteration 758, loss = 113.42810396\n",
      "Iteration 759, loss = 113.41942380\n",
      "Iteration 760, loss = 113.40962065\n",
      "Iteration 761, loss = 113.40095849\n",
      "Iteration 762, loss = 113.38984782\n",
      "Iteration 763, loss = 113.37938719\n",
      "Iteration 764, loss = 113.36981386\n",
      "Iteration 765, loss = 113.35996754\n",
      "Iteration 766, loss = 113.35169508\n",
      "Iteration 767, loss = 113.34246681\n",
      "Iteration 768, loss = 113.33271820\n",
      "Iteration 769, loss = 113.32227040\n",
      "Iteration 770, loss = 113.31326760\n",
      "Iteration 771, loss = 113.30425264\n",
      "Iteration 772, loss = 113.29502120\n",
      "Iteration 773, loss = 113.28575254\n",
      "Iteration 774, loss = 113.27624069\n",
      "Iteration 775, loss = 113.26782770\n",
      "Iteration 776, loss = 113.25794612\n",
      "Iteration 777, loss = 113.25009038\n",
      "Iteration 778, loss = 113.24095449\n",
      "Iteration 779, loss = 113.23199440\n",
      "Iteration 780, loss = 113.22364219\n",
      "Iteration 781, loss = 113.21507316\n",
      "Iteration 782, loss = 113.20634584\n",
      "Iteration 783, loss = 113.19778045\n",
      "Iteration 784, loss = 113.18945325\n",
      "Iteration 785, loss = 113.18132915\n",
      "Iteration 786, loss = 113.17242693\n",
      "Iteration 787, loss = 113.16415448\n",
      "Iteration 788, loss = 113.15497112\n",
      "Iteration 789, loss = 113.14795991\n",
      "Iteration 790, loss = 113.14047673\n",
      "Iteration 791, loss = 113.13109111\n",
      "Iteration 792, loss = 113.12333889\n",
      "Iteration 793, loss = 113.11525309\n",
      "Iteration 794, loss = 113.10690935\n",
      "Iteration 795, loss = 113.09864269\n",
      "Iteration 796, loss = 113.09121539\n",
      "Iteration 797, loss = 113.08283783\n",
      "Iteration 798, loss = 113.07518600\n",
      "Iteration 799, loss = 113.06715886\n",
      "Iteration 800, loss = 113.06022593\n",
      "Iteration 801, loss = 113.05267393\n",
      "Iteration 802, loss = 113.04526579\n",
      "Iteration 803, loss = 113.03732858\n",
      "Iteration 804, loss = 113.02969417\n",
      "Iteration 805, loss = 113.02185697\n",
      "Iteration 806, loss = 113.01470790\n",
      "Iteration 807, loss = 113.00732489\n",
      "Iteration 808, loss = 112.99895915\n",
      "Iteration 809, loss = 112.99178618\n",
      "Iteration 810, loss = 112.98453275\n",
      "Iteration 811, loss = 112.97709975\n",
      "Iteration 812, loss = 112.97022730\n",
      "Iteration 813, loss = 112.96261969\n",
      "Iteration 814, loss = 112.95553555\n",
      "Iteration 815, loss = 112.94848486\n",
      "Iteration 816, loss = 112.94189460\n",
      "Iteration 817, loss = 112.93527956\n",
      "Iteration 818, loss = 112.92876862\n",
      "Iteration 819, loss = 112.92073457\n",
      "Iteration 820, loss = 112.91417166\n",
      "Iteration 821, loss = 112.90709932\n",
      "Iteration 822, loss = 112.89978692\n",
      "Iteration 823, loss = 112.89291541\n",
      "Iteration 824, loss = 112.88578285\n",
      "Iteration 825, loss = 112.87886833\n",
      "Iteration 826, loss = 112.87240752\n",
      "Iteration 827, loss = 112.86531565\n",
      "Iteration 828, loss = 112.85844342\n",
      "Iteration 829, loss = 112.85271050\n",
      "Iteration 830, loss = 112.84569824\n",
      "Iteration 831, loss = 112.83854724\n",
      "Iteration 832, loss = 112.83185869\n",
      "Iteration 833, loss = 112.82562758\n",
      "Iteration 834, loss = 112.81909891\n",
      "Iteration 835, loss = 112.81303907\n",
      "Iteration 836, loss = 112.80655925\n",
      "Iteration 837, loss = 112.79966747\n",
      "Iteration 838, loss = 112.79345803\n",
      "Iteration 839, loss = 112.78654823\n",
      "Iteration 840, loss = 112.78086560\n",
      "Iteration 841, loss = 112.77432817\n",
      "Iteration 842, loss = 112.76785100\n",
      "Iteration 843, loss = 112.76209622\n",
      "Iteration 844, loss = 112.75558514\n",
      "Iteration 845, loss = 112.74932061\n",
      "Iteration 846, loss = 112.74384587\n",
      "Iteration 847, loss = 112.73733634\n",
      "Iteration 848, loss = 112.73153070\n",
      "Iteration 849, loss = 112.72517814\n",
      "Iteration 850, loss = 112.71981867\n",
      "Iteration 851, loss = 112.71360151\n",
      "Iteration 852, loss = 112.70754943\n",
      "Iteration 853, loss = 112.70144939\n",
      "Iteration 854, loss = 112.69568227\n",
      "Iteration 855, loss = 112.68943160\n",
      "Iteration 856, loss = 112.68363239\n",
      "Iteration 857, loss = 112.67755371\n",
      "Iteration 858, loss = 112.67215184\n",
      "Iteration 859, loss = 112.66614017\n",
      "Iteration 860, loss = 112.66035419\n",
      "Iteration 861, loss = 112.65436288\n",
      "Iteration 862, loss = 112.64880674\n",
      "Iteration 863, loss = 112.64321019\n",
      "Iteration 864, loss = 112.63697522\n",
      "Iteration 865, loss = 112.63165766\n",
      "Iteration 866, loss = 112.62586101\n",
      "Iteration 867, loss = 112.62002655\n",
      "Iteration 868, loss = 112.61443242\n",
      "Iteration 869, loss = 112.60891617\n",
      "Iteration 870, loss = 112.60346052\n",
      "Iteration 871, loss = 112.59778092\n",
      "Iteration 872, loss = 112.59243722\n",
      "Iteration 873, loss = 112.58677149\n",
      "Iteration 874, loss = 112.58142982\n",
      "Iteration 875, loss = 112.57572984\n",
      "Iteration 876, loss = 112.57066627\n",
      "Iteration 877, loss = 112.56528206\n",
      "Iteration 878, loss = 112.56000115\n",
      "Iteration 879, loss = 112.55492896\n",
      "Iteration 880, loss = 112.54976552\n",
      "Iteration 881, loss = 112.54432356\n",
      "Iteration 882, loss = 112.53860367\n",
      "Iteration 883, loss = 112.53373057\n",
      "Iteration 884, loss = 112.52814664\n",
      "Iteration 885, loss = 112.52267339\n",
      "Iteration 886, loss = 112.51750554\n",
      "Iteration 887, loss = 112.51226406\n",
      "Iteration 888, loss = 112.50687365\n",
      "Iteration 889, loss = 112.50165578\n",
      "Iteration 890, loss = 112.49645807\n",
      "Iteration 891, loss = 112.49120611\n",
      "Iteration 892, loss = 112.48626993\n",
      "Iteration 893, loss = 112.48130927\n",
      "Iteration 894, loss = 112.47633338\n",
      "Iteration 895, loss = 112.47113859\n",
      "Iteration 896, loss = 112.46619354\n",
      "Iteration 897, loss = 112.46115517\n",
      "Iteration 898, loss = 112.45573140\n",
      "Iteration 899, loss = 112.45074160\n",
      "Iteration 900, loss = 112.44577538\n",
      "Iteration 901, loss = 112.44079295\n",
      "Iteration 902, loss = 112.43608462\n",
      "Iteration 903, loss = 112.43095275\n",
      "Iteration 904, loss = 112.42618193\n",
      "Iteration 905, loss = 112.42121486\n",
      "Iteration 906, loss = 112.41654919\n",
      "Iteration 907, loss = 112.41173001\n",
      "Iteration 908, loss = 112.40665903\n",
      "Iteration 909, loss = 112.40170046\n",
      "Iteration 910, loss = 112.39731983\n",
      "Iteration 911, loss = 112.39209197\n",
      "Iteration 912, loss = 112.38752342\n",
      "Iteration 913, loss = 112.38256560\n",
      "Iteration 914, loss = 112.37801409\n",
      "Iteration 915, loss = 112.37341123\n",
      "Iteration 916, loss = 112.36864904\n",
      "Iteration 917, loss = 112.36384334\n",
      "Iteration 918, loss = 112.35919518\n",
      "Iteration 919, loss = 112.35484326\n",
      "Iteration 920, loss = 112.34994061\n",
      "Iteration 921, loss = 112.34520851\n",
      "Iteration 922, loss = 112.34049728\n",
      "Iteration 923, loss = 112.33591731\n",
      "Iteration 924, loss = 112.33136966\n",
      "Iteration 925, loss = 112.32694156\n",
      "Iteration 926, loss = 112.32217911\n",
      "Iteration 927, loss = 112.31797219\n",
      "Iteration 928, loss = 112.31330705\n",
      "Iteration 929, loss = 112.30885021\n",
      "Iteration 930, loss = 112.30417629\n",
      "Iteration 931, loss = 112.29968620\n",
      "Iteration 932, loss = 112.29549562\n",
      "Iteration 933, loss = 112.29089175\n",
      "Iteration 934, loss = 112.28646352\n",
      "Iteration 935, loss = 112.28202145\n",
      "Iteration 936, loss = 112.27759494\n",
      "Iteration 937, loss = 112.27351850\n",
      "Iteration 938, loss = 112.26895089\n",
      "Iteration 939, loss = 112.26485176\n",
      "Iteration 940, loss = 112.26039271\n",
      "Iteration 941, loss = 112.25585729\n",
      "Iteration 942, loss = 112.25150648\n",
      "Iteration 943, loss = 112.24724163\n",
      "Iteration 944, loss = 112.24307287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 945, loss = 112.23866014\n",
      "Iteration 946, loss = 112.23451753\n",
      "Iteration 947, loss = 112.23032619\n",
      "Iteration 948, loss = 112.22642099\n",
      "Iteration 949, loss = 112.22204153\n",
      "Iteration 950, loss = 112.21795779\n",
      "Iteration 951, loss = 112.21352439\n",
      "Iteration 952, loss = 112.20930451\n",
      "Iteration 953, loss = 112.20551297\n",
      "Iteration 954, loss = 112.20122499\n",
      "Iteration 955, loss = 112.19715099\n",
      "Iteration 956, loss = 112.19291316\n",
      "Iteration 957, loss = 112.18892672\n",
      "Iteration 958, loss = 112.18482186\n",
      "Iteration 959, loss = 112.18098832\n",
      "Iteration 960, loss = 112.17669657\n",
      "Iteration 961, loss = 112.17291723\n",
      "Iteration 962, loss = 112.16865144\n",
      "Iteration 963, loss = 112.16463015\n",
      "Iteration 964, loss = 112.16068293\n",
      "Iteration 965, loss = 112.15665764\n",
      "Iteration 966, loss = 112.15265505\n",
      "Iteration 967, loss = 112.14871987\n",
      "Iteration 968, loss = 112.14479651\n",
      "Iteration 969, loss = 112.14088740\n",
      "Iteration 970, loss = 112.13703152\n",
      "Iteration 971, loss = 112.13328358\n",
      "Iteration 972, loss = 112.12940135\n",
      "Iteration 973, loss = 112.12560521\n",
      "Iteration 974, loss = 112.12165332\n",
      "Iteration 975, loss = 112.11783030\n",
      "Iteration 976, loss = 112.11386069\n",
      "Iteration 977, loss = 112.11004624\n",
      "Iteration 978, loss = 112.10649213\n",
      "Iteration 979, loss = 112.10247768\n",
      "Iteration 980, loss = 112.09873827\n",
      "Iteration 981, loss = 112.09499529\n",
      "Iteration 982, loss = 112.09131311\n",
      "Iteration 983, loss = 112.08753850\n",
      "Iteration 984, loss = 112.08388692\n",
      "Iteration 985, loss = 112.08011008\n",
      "Iteration 986, loss = 112.07639592\n",
      "Iteration 987, loss = 112.07279875\n",
      "Iteration 988, loss = 112.06934983\n",
      "Iteration 989, loss = 112.06543298\n",
      "Iteration 990, loss = 112.06189339\n",
      "Iteration 991, loss = 112.05829918\n",
      "Iteration 992, loss = 112.05475202\n",
      "Iteration 993, loss = 112.05141958\n",
      "Iteration 994, loss = 112.04740381\n",
      "Iteration 995, loss = 112.04382085\n",
      "Iteration 996, loss = 112.04048295\n",
      "Iteration 997, loss = 112.03670977\n",
      "Iteration 998, loss = 112.03319457\n",
      "Iteration 999, loss = 112.02977139\n",
      "Iteration 1000, loss = 112.02641401\n",
      "Iteration 1001, loss = 112.02278786\n",
      "Iteration 1002, loss = 112.01942204\n",
      "Iteration 1003, loss = 112.01609896\n",
      "Iteration 1004, loss = 112.01272882\n",
      "Iteration 1005, loss = 112.00903951\n",
      "Iteration 1006, loss = 112.00557146\n",
      "Iteration 1007, loss = 112.00213653\n",
      "Iteration 1008, loss = 111.99875975\n",
      "Iteration 1009, loss = 111.99539289\n",
      "Iteration 1010, loss = 111.99199193\n",
      "Iteration 1011, loss = 111.98848513\n",
      "Iteration 1012, loss = 111.98505025\n",
      "Iteration 1013, loss = 111.98192673\n",
      "Iteration 1014, loss = 111.97841331\n",
      "Iteration 1015, loss = 111.97519855\n",
      "Iteration 1016, loss = 111.97187786\n",
      "Iteration 1017, loss = 111.96871573\n",
      "Iteration 1018, loss = 111.96532596\n",
      "Iteration 1019, loss = 111.96219854\n",
      "Iteration 1020, loss = 111.95914518\n",
      "Iteration 1021, loss = 111.95554281\n",
      "Iteration 1022, loss = 111.95218669\n",
      "Iteration 1023, loss = 111.94899605\n",
      "Iteration 1024, loss = 111.94620207\n",
      "Iteration 1025, loss = 111.94262997\n",
      "Iteration 1026, loss = 111.93941309\n",
      "Iteration 1027, loss = 111.93615583\n",
      "Iteration 1028, loss = 111.93293048\n",
      "Iteration 1029, loss = 111.92985856\n",
      "Iteration 1030, loss = 111.92661925\n",
      "Iteration 1031, loss = 111.92356384\n",
      "Iteration 1032, loss = 111.92034666\n",
      "Iteration 1033, loss = 111.91725553\n",
      "Iteration 1034, loss = 111.91418367\n",
      "Iteration 1035, loss = 111.91116429\n",
      "Iteration 1036, loss = 111.90799308\n",
      "Iteration 1037, loss = 111.90503407\n",
      "Iteration 1038, loss = 111.90185136\n",
      "Iteration 1039, loss = 111.89876368\n",
      "Iteration 1040, loss = 111.89583830\n",
      "Iteration 1041, loss = 111.89270987\n",
      "Iteration 1042, loss = 111.88985168\n",
      "Iteration 1043, loss = 111.88670124\n",
      "Iteration 1044, loss = 111.88380209\n",
      "Iteration 1045, loss = 111.88063683\n",
      "Iteration 1046, loss = 111.87761912\n",
      "Iteration 1047, loss = 111.87457630\n",
      "Iteration 1048, loss = 111.87201458\n",
      "Iteration 1049, loss = 111.86910718\n",
      "Iteration 1050, loss = 111.86618626\n",
      "Iteration 1051, loss = 111.86327995\n",
      "Iteration 1052, loss = 111.86038150\n",
      "Iteration 1053, loss = 111.85763186\n",
      "Iteration 1054, loss = 111.85465236\n",
      "Iteration 1055, loss = 111.85171877\n",
      "Iteration 1056, loss = 111.84879443\n",
      "Iteration 1057, loss = 111.84594336\n",
      "Iteration 1058, loss = 111.84315975\n",
      "Iteration 1059, loss = 111.84020746\n",
      "Iteration 1060, loss = 111.83739931\n",
      "Iteration 1061, loss = 111.83457505\n",
      "Iteration 1062, loss = 111.83181048\n",
      "Iteration 1063, loss = 111.82893501\n",
      "Iteration 1064, loss = 111.82624726\n",
      "Iteration 1065, loss = 111.82333908\n",
      "Iteration 1066, loss = 111.82055484\n",
      "Iteration 1067, loss = 111.81809044\n",
      "Iteration 1068, loss = 111.81507431\n",
      "Iteration 1069, loss = 111.81236961\n",
      "Iteration 1070, loss = 111.80961969\n",
      "Iteration 1071, loss = 111.80684641\n",
      "Iteration 1072, loss = 111.80437180\n",
      "Iteration 1073, loss = 111.80154826\n",
      "Iteration 1074, loss = 111.79875441\n",
      "Iteration 1075, loss = 111.79610563\n",
      "Iteration 1076, loss = 111.79355181\n",
      "Iteration 1077, loss = 111.79080339\n",
      "Iteration 1078, loss = 111.78812712\n",
      "Iteration 1079, loss = 111.78550254\n",
      "Iteration 1080, loss = 111.78298452\n",
      "Iteration 1081, loss = 111.78024751\n",
      "Iteration 1082, loss = 111.77765544\n",
      "Iteration 1083, loss = 111.77518355\n",
      "Iteration 1084, loss = 111.77260280\n",
      "Iteration 1085, loss = 111.76999771\n",
      "Iteration 1086, loss = 111.76738040\n",
      "Iteration 1087, loss = 111.76527495\n",
      "Iteration 1088, loss = 111.76226436\n",
      "Iteration 1089, loss = 111.75975153\n",
      "Iteration 1090, loss = 111.75735387\n",
      "Iteration 1091, loss = 111.75469536\n",
      "Iteration 1092, loss = 111.75225427\n",
      "Iteration 1093, loss = 111.74971329\n",
      "Iteration 1094, loss = 111.74722935\n",
      "Iteration 1095, loss = 111.74475809\n",
      "Iteration 1096, loss = 111.74227674\n",
      "Iteration 1097, loss = 111.73995299\n",
      "Iteration 1098, loss = 111.73737547\n",
      "Iteration 1099, loss = 111.73495146\n",
      "Iteration 1100, loss = 111.73251619\n",
      "Iteration 1101, loss = 111.73029932\n",
      "Iteration 1102, loss = 111.72774092\n",
      "Iteration 1103, loss = 111.72533173\n",
      "Iteration 1104, loss = 111.72298599\n",
      "Iteration 1105, loss = 111.72057717\n",
      "Iteration 1106, loss = 111.71817310\n",
      "Iteration 1107, loss = 111.71600699\n",
      "Iteration 1108, loss = 111.71348693\n",
      "Iteration 1109, loss = 111.71108969\n",
      "Iteration 1110, loss = 111.70881446\n",
      "Iteration 1111, loss = 111.70648331\n",
      "Iteration 1112, loss = 111.70412443\n",
      "Iteration 1113, loss = 111.70210669\n",
      "Iteration 1114, loss = 111.69948217\n",
      "Iteration 1115, loss = 111.69737601\n",
      "Iteration 1116, loss = 111.69509613\n",
      "Iteration 1117, loss = 111.69277057\n",
      "Iteration 1118, loss = 111.69062816\n",
      "Iteration 1119, loss = 111.68840664\n",
      "Iteration 1120, loss = 111.68589798\n",
      "Iteration 1121, loss = 111.68365207\n",
      "Iteration 1122, loss = 111.68137591\n",
      "Iteration 1123, loss = 111.67928760\n",
      "Iteration 1124, loss = 111.67692552\n",
      "Iteration 1125, loss = 111.67477566\n",
      "Iteration 1126, loss = 111.67256901\n",
      "Iteration 1127, loss = 111.67038135\n",
      "Iteration 1128, loss = 111.66814871\n",
      "Iteration 1129, loss = 111.66613432\n",
      "Iteration 1130, loss = 111.66386257\n",
      "Iteration 1131, loss = 111.66159121\n",
      "Iteration 1132, loss = 111.65949626\n",
      "Iteration 1133, loss = 111.65732487\n",
      "Iteration 1134, loss = 111.65535026\n",
      "Iteration 1135, loss = 111.65304078\n",
      "Iteration 1136, loss = 111.65085001\n",
      "Iteration 1137, loss = 111.64882223\n",
      "Iteration 1138, loss = 111.64686381\n",
      "Iteration 1139, loss = 111.64461847\n",
      "Iteration 1140, loss = 111.64258183\n",
      "Iteration 1141, loss = 111.64052967\n",
      "Iteration 1142, loss = 111.63852359\n",
      "Iteration 1143, loss = 111.63673258\n",
      "Iteration 1144, loss = 111.63463193\n",
      "Iteration 1145, loss = 111.63275083\n",
      "Iteration 1146, loss = 111.63073863\n",
      "Iteration 1147, loss = 111.62881055\n",
      "Iteration 1148, loss = 111.62675589\n",
      "Iteration 1149, loss = 111.62503853\n",
      "Iteration 1150, loss = 111.62277393\n",
      "Iteration 1151, loss = 111.62060536\n",
      "Iteration 1152, loss = 111.61859934\n",
      "Iteration 1153, loss = 111.61639936\n",
      "Iteration 1154, loss = 111.61424944\n",
      "Iteration 1155, loss = 111.61218318\n",
      "Iteration 1156, loss = 111.60996934\n",
      "Iteration 1157, loss = 111.60786999\n",
      "Iteration 1158, loss = 111.60589701\n",
      "Iteration 1159, loss = 111.60383363\n",
      "Iteration 1160, loss = 111.60205005\n",
      "Iteration 1161, loss = 111.60003694\n",
      "Iteration 1162, loss = 111.59849833\n",
      "Iteration 1163, loss = 111.59604620\n",
      "Iteration 1164, loss = 111.59425167\n",
      "Iteration 1165, loss = 111.59243977\n",
      "Iteration 1166, loss = 111.59070346\n",
      "Iteration 1167, loss = 111.58898188\n",
      "Iteration 1168, loss = 111.58724254\n",
      "Iteration 1169, loss = 111.58546986\n",
      "Iteration 1170, loss = 111.58380570\n",
      "Iteration 1171, loss = 111.58203751\n",
      "Iteration 1172, loss = 111.58045063\n",
      "Iteration 1173, loss = 111.57830987\n",
      "Iteration 1174, loss = 111.57634887\n",
      "Iteration 1175, loss = 111.57434743\n",
      "Iteration 1176, loss = 111.57232727\n",
      "Iteration 1177, loss = 111.57046631\n",
      "Iteration 1178, loss = 111.56836723\n",
      "Iteration 1179, loss = 111.56645754\n",
      "Iteration 1180, loss = 111.56464515\n",
      "Iteration 1181, loss = 111.56296962\n",
      "Iteration 1182, loss = 111.56170869\n",
      "Iteration 1183, loss = 111.55964485\n",
      "Iteration 1184, loss = 111.55820724\n",
      "Iteration 1185, loss = 111.55584604\n",
      "Iteration 1186, loss = 111.55448962\n",
      "Iteration 1187, loss = 111.55282272\n",
      "Iteration 1188, loss = 111.55107246\n",
      "Iteration 1189, loss = 111.54953473\n",
      "Iteration 1190, loss = 111.54814652\n",
      "Iteration 1191, loss = 111.54629535\n",
      "Iteration 1192, loss = 111.54520956\n",
      "Iteration 1193, loss = 111.54278564\n",
      "Iteration 1194, loss = 111.54141585\n",
      "Iteration 1195, loss = 111.53930549\n",
      "Iteration 1196, loss = 111.53733168\n",
      "Iteration 1197, loss = 111.53493739\n",
      "Iteration 1198, loss = 111.53194205\n",
      "Iteration 1199, loss = 111.53322108\n",
      "Iteration 1200, loss = 111.52754183\n",
      "Iteration 1201, loss = 111.52572246\n",
      "Iteration 1202, loss = 111.52420335\n",
      "Iteration 1203, loss = 111.52292682\n",
      "Iteration 1204, loss = 111.52038949\n",
      "Iteration 1205, loss = 111.51843279\n",
      "Iteration 1206, loss = 111.51635880\n",
      "Iteration 1207, loss = 111.51639444\n",
      "Iteration 1208, loss = 111.51202115\n",
      "Iteration 1209, loss = 111.51055174\n",
      "Iteration 1210, loss = 111.50855719\n",
      "Iteration 1211, loss = 111.50664590\n",
      "Iteration 1212, loss = 111.50646271\n",
      "Iteration 1213, loss = 111.50485792\n",
      "Iteration 1214, loss = 111.50202980\n",
      "Iteration 1215, loss = 111.50007557\n",
      "Iteration 1216, loss = 111.49940598\n",
      "Iteration 1217, loss = 111.49781222\n",
      "Iteration 1218, loss = 111.49448950\n",
      "Iteration 1219, loss = 111.49236688\n",
      "Iteration 1220, loss = 111.49114672\n",
      "Iteration 1221, loss = 111.48888953\n",
      "Iteration 1222, loss = 111.48617028\n",
      "Iteration 1223, loss = 111.48509727\n",
      "Iteration 1224, loss = 111.48366334\n",
      "Iteration 1225, loss = 111.48093033\n",
      "Iteration 1226, loss = 111.47999728\n",
      "Iteration 1227, loss = 111.47885987\n",
      "Iteration 1228, loss = 111.47467688\n",
      "Iteration 1229, loss = 111.47703346\n",
      "Iteration 1230, loss = 111.47141714\n",
      "Iteration 1231, loss = 111.47166576\n",
      "Iteration 1232, loss = 111.46991395\n",
      "Iteration 1233, loss = 111.46618569\n",
      "Iteration 1234, loss = 111.46872124\n",
      "Iteration 1235, loss = 111.46248125\n",
      "Iteration 1236, loss = 111.46211776\n",
      "Iteration 1237, loss = 111.46002539\n",
      "Iteration 1238, loss = 111.45655765\n",
      "Iteration 1239, loss = 111.45925161\n",
      "Iteration 1240, loss = 111.45323495\n",
      "Iteration 1241, loss = 111.45233431\n",
      "Iteration 1242, loss = 111.44989714\n",
      "Iteration 1243, loss = 111.44751917\n",
      "Iteration 1244, loss = 111.45227596\n",
      "Iteration 1245, loss = 111.44427973\n",
      "Iteration 1246, loss = 111.44377499\n",
      "Iteration 1247, loss = 111.44302525\n",
      "Iteration 1248, loss = 111.44012815\n",
      "Iteration 1249, loss = 111.43781041\n",
      "Iteration 1250, loss = 111.44401916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1251, loss = 111.43592109\n",
      "Iteration 1252, loss = 111.43652302\n",
      "Iteration 1253, loss = 111.43261175\n",
      "Iteration 1254, loss = 111.43111457\n",
      "Iteration 1255, loss = 111.43330507\n",
      "Iteration 1256, loss = 111.43482490\n",
      "Iteration 1257, loss = 111.43012069\n",
      "Iteration 1258, loss = 111.42525094\n",
      "Iteration 1259, loss = 111.42257923\n",
      "Iteration 1260, loss = 111.42414614\n",
      "Iteration 1261, loss = 111.42309543\n",
      "Iteration 1262, loss = 111.42066675\n",
      "Iteration 1263, loss = 111.41717026\n",
      "Iteration 1264, loss = 111.41552877\n",
      "Iteration 1265, loss = 111.41538635\n",
      "Iteration 1266, loss = 111.41413266\n",
      "Iteration 1267, loss = 111.41090494\n",
      "Iteration 1268, loss = 111.40873117\n",
      "Iteration 1269, loss = 111.40893374\n",
      "Iteration 1270, loss = 111.40749878\n",
      "Iteration 1271, loss = 111.40579127\n",
      "Iteration 1272, loss = 111.40235759\n",
      "Iteration 1273, loss = 111.40185168\n",
      "Iteration 1274, loss = 111.40176311\n",
      "Iteration 1275, loss = 111.40077764\n",
      "Iteration 1276, loss = 111.39788515\n",
      "Iteration 1277, loss = 111.39552753\n",
      "Iteration 1278, loss = 111.39312374\n",
      "Iteration 1279, loss = 111.39374548\n",
      "Iteration 1280, loss = 111.39389705\n",
      "Iteration 1281, loss = 111.39027518\n",
      "Iteration 1282, loss = 111.38769095\n",
      "Iteration 1283, loss = 111.38693840\n",
      "Iteration 1284, loss = 111.38617647\n",
      "Iteration 1285, loss = 111.38441327\n",
      "Iteration 1286, loss = 111.38337286\n",
      "Iteration 1287, loss = 111.38125237\n",
      "Iteration 1288, loss = 111.37926515\n",
      "Iteration 1289, loss = 111.37728160\n",
      "Iteration 1290, loss = 111.37696425\n",
      "Iteration 1291, loss = 111.37656358\n",
      "Iteration 1292, loss = 111.37273326\n",
      "Iteration 1293, loss = 111.37160845\n",
      "Iteration 1294, loss = 111.37058352\n",
      "Iteration 1295, loss = 111.36915778\n",
      "Iteration 1296, loss = 111.36737706\n",
      "Iteration 1297, loss = 111.36749465\n",
      "Iteration 1298, loss = 111.36523617\n",
      "Iteration 1299, loss = 111.36310140\n",
      "Iteration 1300, loss = 111.36384580\n",
      "Iteration 1301, loss = 111.36125514\n",
      "Iteration 1302, loss = 111.36023700\n",
      "Iteration 1303, loss = 111.35933669\n",
      "Iteration 1304, loss = 111.35728634\n",
      "Iteration 1305, loss = 111.35565721\n",
      "Iteration 1306, loss = 111.35415922\n",
      "Iteration 1307, loss = 111.35309299\n",
      "Iteration 1308, loss = 111.35278474\n",
      "Iteration 1309, loss = 111.35084290\n",
      "Iteration 1310, loss = 111.34978417\n",
      "Iteration 1311, loss = 111.34852272\n",
      "Iteration 1312, loss = 111.34652802\n",
      "Iteration 1313, loss = 111.34538206\n",
      "Iteration 1314, loss = 111.34440813\n",
      "Iteration 1315, loss = 111.34179516\n",
      "Iteration 1316, loss = 111.34235041\n",
      "Iteration 1317, loss = 111.33968694\n",
      "Iteration 1318, loss = 111.33876903\n",
      "Iteration 1319, loss = 111.33841530\n",
      "Iteration 1320, loss = 111.33739998\n",
      "Iteration 1321, loss = 111.33576884\n",
      "Iteration 1322, loss = 111.33425065\n",
      "Iteration 1323, loss = 111.33286936\n",
      "Iteration 1324, loss = 111.32982242\n",
      "Iteration 1325, loss = 111.33141404\n",
      "Iteration 1326, loss = 111.33054166\n",
      "Iteration 1327, loss = 111.32762148\n",
      "Iteration 1328, loss = 111.32606592\n",
      "Iteration 1329, loss = 111.32715681\n",
      "Iteration 1330, loss = 111.32583909\n",
      "Iteration 1331, loss = 111.32410659\n",
      "Iteration 1332, loss = 111.32309353\n",
      "Iteration 1333, loss = 111.32098375\n",
      "Iteration 1334, loss = 111.31924931\n",
      "Iteration 1335, loss = 111.31605456\n",
      "Iteration 1336, loss = 111.31764072\n",
      "Iteration 1337, loss = 111.31785470\n",
      "Iteration 1338, loss = 111.31411802\n",
      "Iteration 1339, loss = 111.31205188\n",
      "Iteration 1340, loss = 111.31211737\n",
      "Iteration 1341, loss = 111.31195811\n",
      "Iteration 1342, loss = 111.31171705\n",
      "Iteration 1343, loss = 111.31009531\n",
      "Iteration 1344, loss = 111.30743887\n",
      "Iteration 1345, loss = 111.30488223\n",
      "Iteration 1346, loss = 111.30488998\n",
      "Iteration 1347, loss = 111.30355296\n",
      "Iteration 1348, loss = 111.30055879\n",
      "Iteration 1349, loss = 111.30098893\n",
      "Iteration 1350, loss = 111.30099049\n",
      "Iteration 1351, loss = 111.30029914\n",
      "Iteration 1352, loss = 111.29929810\n",
      "Iteration 1353, loss = 111.29749308\n",
      "Iteration 1354, loss = 111.29584358\n",
      "Iteration 1355, loss = 111.29341792\n",
      "Iteration 1356, loss = 111.29038080\n",
      "Iteration 1357, loss = 111.29367675\n",
      "Iteration 1358, loss = 111.29307723\n",
      "Iteration 1359, loss = 111.29040511\n",
      "Iteration 1360, loss = 111.28527612\n",
      "Iteration 1361, loss = 111.28633333\n",
      "Iteration 1362, loss = 111.28547829\n",
      "Iteration 1363, loss = 111.28435797\n",
      "Iteration 1364, loss = 111.28286663\n",
      "Iteration 1365, loss = 111.28034589\n",
      "Iteration 1366, loss = 111.28221861\n",
      "Iteration 1367, loss = 111.28117805\n",
      "Iteration 1368, loss = 111.27750731\n",
      "Iteration 1369, loss = 111.27802446\n",
      "Iteration 1370, loss = 111.27818456\n",
      "Iteration 1371, loss = 111.27778804\n",
      "Iteration 1372, loss = 111.27657749\n",
      "Iteration 1373, loss = 111.27509042\n",
      "Iteration 1374, loss = 111.27406405\n",
      "Iteration 1375, loss = 111.27114584\n",
      "Iteration 1376, loss = 111.26775918\n",
      "Iteration 1377, loss = 111.27133685\n",
      "Iteration 1378, loss = 111.27184813\n",
      "Iteration 1379, loss = 111.26880598\n",
      "Iteration 1380, loss = 111.26406322\n",
      "Iteration 1381, loss = 111.26562783\n",
      "Iteration 1382, loss = 111.26532926\n",
      "Iteration 1383, loss = 111.26756572\n",
      "Iteration 1384, loss = 111.26489967\n",
      "Iteration 1385, loss = 111.26047953\n",
      "Iteration 1386, loss = 111.25711582\n",
      "Iteration 1387, loss = 111.26184508\n",
      "Iteration 1388, loss = 111.26412455\n",
      "Iteration 1389, loss = 111.26363343\n",
      "Iteration 1390, loss = 111.25927305\n",
      "Iteration 1391, loss = 111.25294023\n",
      "Iteration 1392, loss = 111.25401680\n",
      "Iteration 1393, loss = 111.25415873\n",
      "Iteration 1394, loss = 111.25856385\n",
      "Iteration 1395, loss = 111.25601821\n",
      "Iteration 1396, loss = 111.25000041\n",
      "Iteration 1397, loss = 111.24641286\n",
      "Iteration 1398, loss = 111.24534676\n",
      "Iteration 1399, loss = 111.24861380\n",
      "Iteration 1400, loss = 111.24703006\n",
      "Iteration 1401, loss = 111.24201472\n",
      "Iteration 1402, loss = 111.24020021\n",
      "Iteration 1403, loss = 111.24067199\n",
      "Iteration 1404, loss = 111.24067375\n",
      "Iteration 1405, loss = 111.23971212\n",
      "Iteration 1406, loss = 111.23839974\n",
      "Iteration 1407, loss = 111.23594666\n",
      "Iteration 1408, loss = 111.23308383\n",
      "Iteration 1409, loss = 111.23523205\n",
      "Iteration 1410, loss = 111.23500255\n",
      "Iteration 1411, loss = 111.23215841\n",
      "Iteration 1412, loss = 111.22996754\n",
      "Iteration 1413, loss = 111.23015759\n",
      "Iteration 1414, loss = 111.22912055\n",
      "Iteration 1415, loss = 111.22797147\n",
      "Iteration 1416, loss = 111.22633507\n",
      "Iteration 1417, loss = 111.22407080\n",
      "Iteration 1418, loss = 111.22345500\n",
      "Iteration 1419, loss = 111.22308133\n",
      "Iteration 1420, loss = 111.22272811\n",
      "Iteration 1421, loss = 111.22247093\n",
      "Iteration 1422, loss = 111.22046665\n",
      "Iteration 1423, loss = 111.21953550\n",
      "Iteration 1424, loss = 111.21876202\n",
      "Iteration 1425, loss = 111.21600577\n",
      "Iteration 1426, loss = 111.21638352\n",
      "Iteration 1427, loss = 111.21543799\n",
      "Iteration 1428, loss = 111.21289110\n",
      "Iteration 1429, loss = 111.21457698\n",
      "Iteration 1430, loss = 111.21357733\n",
      "Iteration 1431, loss = 111.21015421\n",
      "Iteration 1432, loss = 111.20962635\n",
      "Iteration 1433, loss = 111.20808982\n",
      "Iteration 1434, loss = 111.20726069\n",
      "Iteration 1435, loss = 111.20631274\n",
      "Iteration 1436, loss = 111.20474813\n",
      "Iteration 1437, loss = 111.20474377\n",
      "Iteration 1438, loss = 111.20291753\n",
      "Iteration 1439, loss = 111.20189333\n",
      "Iteration 1440, loss = 111.20230709\n",
      "Iteration 1441, loss = 111.20176089\n",
      "Iteration 1442, loss = 111.20026324\n",
      "Iteration 1443, loss = 111.19873454\n",
      "Iteration 1444, loss = 111.20157388\n",
      "Iteration 1445, loss = 111.20008193\n",
      "Iteration 1446, loss = 111.19686087\n",
      "Iteration 1447, loss = 111.19726264\n",
      "Iteration 1448, loss = 111.19713108\n",
      "Iteration 1449, loss = 111.19696305\n",
      "Iteration 1450, loss = 111.19557319\n",
      "Iteration 1451, loss = 111.19341602\n",
      "Iteration 1452, loss = 111.19172109\n",
      "Iteration 1453, loss = 111.19044094\n",
      "Iteration 1454, loss = 111.19060165\n",
      "Iteration 1455, loss = 111.18805014\n",
      "Iteration 1456, loss = 111.18757995\n",
      "Iteration 1457, loss = 111.18956673\n",
      "Iteration 1458, loss = 111.18907678\n",
      "Iteration 1459, loss = 111.18600907\n",
      "Iteration 1460, loss = 111.18370682\n",
      "Iteration 1461, loss = 111.18218044\n",
      "Iteration 1462, loss = 111.18186859\n",
      "Iteration 1463, loss = 111.18047650\n",
      "Iteration 1464, loss = 111.18092832\n",
      "Iteration 1465, loss = 111.17916121\n",
      "Iteration 1466, loss = 111.17851453\n",
      "Iteration 1467, loss = 111.17659217\n",
      "Iteration 1468, loss = 111.17530237\n",
      "Iteration 1469, loss = 111.17735158\n",
      "Iteration 1470, loss = 111.17673006\n",
      "Iteration 1471, loss = 111.17426542\n",
      "Iteration 1472, loss = 111.17237675\n",
      "Iteration 1473, loss = 111.17176040\n",
      "Iteration 1474, loss = 111.17091523\n",
      "Iteration 1475, loss = 111.17101409\n",
      "Iteration 1476, loss = 111.16973226\n",
      "Iteration 1477, loss = 111.16828180\n",
      "Iteration 1478, loss = 111.16741309\n",
      "Iteration 1479, loss = 111.16664977\n",
      "Iteration 1480, loss = 111.16536288\n",
      "Iteration 1481, loss = 111.16430757\n",
      "Iteration 1482, loss = 111.16170839\n",
      "Iteration 1483, loss = 111.16448511\n",
      "Iteration 1484, loss = 111.16392785\n",
      "Iteration 1485, loss = 111.16078379\n",
      "Iteration 1486, loss = 111.15935173\n",
      "Iteration 1487, loss = 111.15905583\n",
      "Iteration 1488, loss = 111.15798687\n",
      "Iteration 1489, loss = 111.15810488\n",
      "Iteration 1490, loss = 111.15755021\n",
      "Iteration 1491, loss = 111.15670387\n",
      "Iteration 1492, loss = 111.15507759\n",
      "Iteration 1493, loss = 111.15292046\n",
      "Iteration 1494, loss = 111.15528349\n",
      "Iteration 1495, loss = 111.15354982\n",
      "Iteration 1496, loss = 111.15019770\n",
      "Iteration 1497, loss = 111.14979207\n",
      "Iteration 1498, loss = 111.14843726\n",
      "Iteration 1499, loss = 111.14831467\n",
      "Iteration 1500, loss = 111.14703573\n",
      "Iteration 1501, loss = 111.14668811\n",
      "Iteration 1502, loss = 111.14663716\n",
      "Iteration 1503, loss = 111.14553818\n",
      "Iteration 1504, loss = 111.14301496\n",
      "Iteration 1505, loss = 111.14439060\n",
      "Iteration 1506, loss = 111.14362309\n",
      "Iteration 1507, loss = 111.13974388\n",
      "Iteration 1508, loss = 111.14199823\n",
      "Iteration 1509, loss = 111.14344957\n",
      "Iteration 1510, loss = 111.14260815\n",
      "Iteration 1511, loss = 111.14084929\n",
      "Iteration 1512, loss = 111.13811809\n",
      "Iteration 1513, loss = 111.13549113\n",
      "Iteration 1514, loss = 111.13960591\n",
      "Iteration 1515, loss = 111.14039843\n",
      "Iteration 1516, loss = 111.13807158\n",
      "Iteration 1517, loss = 111.13338121\n",
      "Iteration 1518, loss = 111.13356935\n",
      "Iteration 1519, loss = 111.13577764\n",
      "Iteration 1520, loss = 111.13617818\n",
      "Iteration 1521, loss = 111.13461425\n",
      "Iteration 1522, loss = 111.13153204\n",
      "Iteration 1523, loss = 111.12873991\n",
      "Iteration 1524, loss = 111.12736897\n",
      "Iteration 1525, loss = 111.12764949\n",
      "Iteration 1526, loss = 111.12393120\n",
      "Iteration 1527, loss = 111.12441912\n",
      "Iteration 1528, loss = 111.12528097\n",
      "Iteration 1529, loss = 111.12425347\n",
      "Iteration 1530, loss = 111.12178519\n",
      "Iteration 1531, loss = 111.11929355\n",
      "Iteration 1532, loss = 111.11919632\n",
      "Iteration 1533, loss = 111.11952544\n",
      "Iteration 1534, loss = 111.11848521\n",
      "Iteration 1535, loss = 111.11666041\n",
      "Iteration 1536, loss = 111.11702163\n",
      "Iteration 1537, loss = 111.11515985\n",
      "Iteration 1538, loss = 111.11547610\n",
      "Iteration 1539, loss = 111.11569697\n",
      "Iteration 1540, loss = 111.11376684\n",
      "Iteration 1541, loss = 111.11094699\n",
      "Iteration 1542, loss = 111.10987576\n",
      "Iteration 1543, loss = 111.11150508\n",
      "Iteration 1544, loss = 111.11187747\n",
      "Iteration 1545, loss = 111.11033828\n",
      "Iteration 1546, loss = 111.10835664\n",
      "Iteration 1547, loss = 111.10933168\n",
      "Iteration 1548, loss = 111.10903894\n",
      "Iteration 1549, loss = 111.10553293\n",
      "Iteration 1550, loss = 111.10563581\n",
      "Iteration 1551, loss = 111.10679978\n",
      "Iteration 1552, loss = 111.10598346\n",
      "Iteration 1553, loss = 111.10341158\n",
      "Iteration 1554, loss = 111.10038631\n",
      "Iteration 1555, loss = 111.10218132\n",
      "Iteration 1556, loss = 111.10337260\n",
      "Iteration 1557, loss = 111.10069153\n",
      "Iteration 1558, loss = 111.09822202\n",
      "Iteration 1559, loss = 111.09837846\n",
      "Iteration 1560, loss = 111.09713039\n",
      "Iteration 1561, loss = 111.09501550\n",
      "Iteration 1562, loss = 111.09511670\n",
      "Iteration 1563, loss = 111.09408519\n",
      "Iteration 1564, loss = 111.09205096\n",
      "Iteration 1565, loss = 111.09244529\n",
      "Iteration 1566, loss = 111.09036549\n",
      "Iteration 1567, loss = 111.09006326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1568, loss = 111.08806232\n",
      "Iteration 1569, loss = 111.08952161\n",
      "Iteration 1570, loss = 111.08826243\n",
      "Iteration 1571, loss = 111.08764914\n",
      "Iteration 1572, loss = 111.08799937\n",
      "Iteration 1573, loss = 111.08714209\n",
      "Iteration 1574, loss = 111.08461958\n",
      "Iteration 1575, loss = 111.08649348\n",
      "Iteration 1576, loss = 111.08589940\n",
      "Iteration 1577, loss = 111.08321709\n",
      "Iteration 1578, loss = 111.08183495\n",
      "Iteration 1579, loss = 111.08216839\n",
      "Iteration 1580, loss = 111.08171963\n",
      "Iteration 1581, loss = 111.07921071\n",
      "Iteration 1582, loss = 111.07663457\n",
      "Iteration 1583, loss = 111.07589578\n",
      "Iteration 1584, loss = 111.07597693\n",
      "Iteration 1585, loss = 111.07575651\n",
      "Iteration 1586, loss = 111.07344923\n",
      "Iteration 1587, loss = 111.07522112\n",
      "Iteration 1588, loss = 111.07446135\n",
      "Iteration 1589, loss = 111.07145104\n",
      "Iteration 1590, loss = 111.07105996\n",
      "Iteration 1591, loss = 111.07030491\n",
      "Iteration 1592, loss = 111.06854068\n",
      "Iteration 1593, loss = 111.06779939\n",
      "Iteration 1594, loss = 111.06883490\n",
      "Iteration 1595, loss = 111.06784089\n",
      "Iteration 1596, loss = 111.06642655\n",
      "Iteration 1597, loss = 111.06739446\n",
      "Iteration 1598, loss = 111.06560892\n",
      "Iteration 1599, loss = 111.06371533\n",
      "Iteration 1600, loss = 111.06338314\n",
      "Iteration 1601, loss = 111.06427825\n",
      "Iteration 1602, loss = 111.06164681\n",
      "Iteration 1603, loss = 111.06084207\n",
      "Iteration 1604, loss = 111.06021587\n",
      "Iteration 1605, loss = 111.05933223\n",
      "Iteration 1606, loss = 111.05749472\n",
      "Iteration 1607, loss = 111.05982862\n",
      "Iteration 1608, loss = 111.06002725\n",
      "Iteration 1609, loss = 111.05656234\n",
      "Iteration 1610, loss = 111.05732152\n",
      "Iteration 1611, loss = 111.05851171\n",
      "Iteration 1612, loss = 111.05776846\n",
      "Iteration 1613, loss = 111.05504314\n",
      "Iteration 1614, loss = 111.05115400\n",
      "Iteration 1615, loss = 111.05288497\n",
      "Iteration 1616, loss = 111.05410125\n",
      "Iteration 1617, loss = 111.05257129\n",
      "Iteration 1618, loss = 111.04790906\n",
      "Iteration 1619, loss = 111.05118406\n",
      "Iteration 1620, loss = 111.05251475\n",
      "Iteration 1621, loss = 111.05233600\n",
      "Iteration 1622, loss = 111.05139306\n",
      "Iteration 1623, loss = 111.04808299\n",
      "Iteration 1624, loss = 111.04396324\n",
      "Iteration 1625, loss = 111.04508820\n",
      "Iteration 1626, loss = 111.04731962\n",
      "Iteration 1627, loss = 111.04614476\n",
      "Iteration 1628, loss = 111.04130077\n",
      "Iteration 1629, loss = 111.04002049\n",
      "Iteration 1630, loss = 111.04178125\n",
      "Iteration 1631, loss = 111.04118942\n",
      "Iteration 1632, loss = 111.03981727\n",
      "Iteration 1633, loss = 111.03674837\n",
      "Iteration 1634, loss = 111.03612578\n",
      "Iteration 1635, loss = 111.03682870\n",
      "Iteration 1636, loss = 111.03464334\n",
      "Iteration 1637, loss = 111.03297879\n",
      "Iteration 1638, loss = 111.03346706\n",
      "Iteration 1639, loss = 111.03171512\n",
      "Iteration 1640, loss = 111.02971423\n",
      "Iteration 1641, loss = 111.03257608\n",
      "Iteration 1642, loss = 111.03238932\n",
      "Iteration 1643, loss = 111.02903284\n",
      "Iteration 1644, loss = 111.02730937\n",
      "Iteration 1645, loss = 111.02825192\n",
      "Iteration 1646, loss = 111.02698272\n",
      "Iteration 1647, loss = 111.02466205\n",
      "Iteration 1648, loss = 111.02510566\n",
      "Iteration 1649, loss = 111.02587663\n",
      "Iteration 1650, loss = 111.02232694\n",
      "Iteration 1651, loss = 111.02277205\n",
      "Iteration 1652, loss = 111.02472279\n",
      "Iteration 1653, loss = 111.02470662\n",
      "Iteration 1654, loss = 111.02158611\n",
      "Iteration 1655, loss = 111.01824646\n",
      "Iteration 1656, loss = 111.02256267\n",
      "Iteration 1657, loss = 111.02345527\n",
      "Iteration 1658, loss = 111.02195846\n",
      "Iteration 1659, loss = 111.01738858\n",
      "Iteration 1660, loss = 111.01663368\n",
      "Iteration 1661, loss = 111.01770444\n",
      "Iteration 1662, loss = 111.01714341\n",
      "Iteration 1663, loss = 111.01560404\n",
      "Iteration 1664, loss = 111.01284645\n",
      "Iteration 1665, loss = 111.01035026\n",
      "Iteration 1666, loss = 111.01095519\n",
      "Iteration 1667, loss = 111.00879580\n",
      "Iteration 1668, loss = 111.00820874\n",
      "Iteration 1669, loss = 111.00860301\n",
      "Iteration 1670, loss = 111.00769434\n",
      "Iteration 1671, loss = 111.00467229\n",
      "Iteration 1672, loss = 111.00680940\n",
      "Iteration 1673, loss = 111.00775045\n",
      "Iteration 1674, loss = 111.00513819\n",
      "Iteration 1675, loss = 111.00246473\n",
      "Iteration 1676, loss = 111.00330684\n",
      "Iteration 1677, loss = 111.00175727\n",
      "Iteration 1678, loss = 110.99878969\n",
      "Iteration 1679, loss = 111.00207035\n",
      "Iteration 1680, loss = 111.00232237\n",
      "Iteration 1681, loss = 110.99893550\n",
      "Iteration 1682, loss = 110.99708002\n",
      "Iteration 1683, loss = 110.99759188\n",
      "Iteration 1684, loss = 110.99621426\n",
      "Iteration 1685, loss = 110.99356722\n",
      "Iteration 1686, loss = 110.99549972\n",
      "Iteration 1687, loss = 110.99571729\n",
      "Iteration 1688, loss = 110.99193947\n",
      "Iteration 1689, loss = 110.99175248\n",
      "Iteration 1690, loss = 110.99356722\n",
      "Iteration 1691, loss = 110.99304710\n",
      "Iteration 1692, loss = 110.98986147\n",
      "Iteration 1693, loss = 110.98819383\n",
      "Iteration 1694, loss = 110.98835591\n",
      "Iteration 1695, loss = 110.98602250\n",
      "Iteration 1696, loss = 110.98592462\n",
      "Iteration 1697, loss = 110.98667586\n",
      "Iteration 1698, loss = 110.98614732\n",
      "Iteration 1699, loss = 110.98328158\n",
      "Iteration 1700, loss = 110.98194628\n",
      "Iteration 1701, loss = 110.98311124\n",
      "Iteration 1702, loss = 110.98050648\n",
      "Iteration 1703, loss = 110.98053431\n",
      "Iteration 1704, loss = 110.98138932\n",
      "Iteration 1705, loss = 110.98052340\n",
      "Iteration 1706, loss = 110.97748643\n",
      "Iteration 1707, loss = 110.97728449\n",
      "Iteration 1708, loss = 110.97775360\n",
      "Iteration 1709, loss = 110.97581046\n",
      "Iteration 1710, loss = 110.97497038\n",
      "Iteration 1711, loss = 110.97642858\n",
      "Iteration 1712, loss = 110.97515803\n",
      "Iteration 1713, loss = 110.97164114\n",
      "Iteration 1714, loss = 110.97250295\n",
      "Iteration 1715, loss = 110.97344226\n",
      "Iteration 1716, loss = 110.97103591\n",
      "Iteration 1717, loss = 110.96897943\n",
      "Iteration 1718, loss = 110.96941491\n",
      "Iteration 1719, loss = 110.96825926\n",
      "Iteration 1720, loss = 110.96489764\n",
      "Iteration 1721, loss = 110.96768004\n",
      "Iteration 1722, loss = 110.96872762\n",
      "Iteration 1723, loss = 110.96597164\n",
      "Iteration 1724, loss = 110.96164362\n",
      "Iteration 1725, loss = 110.96220767\n",
      "Iteration 1726, loss = 110.96161217\n",
      "Iteration 1727, loss = 110.95948140\n",
      "Iteration 1728, loss = 110.95786388\n",
      "Iteration 1729, loss = 110.95864075\n",
      "Iteration 1730, loss = 110.95717353\n",
      "Iteration 1731, loss = 110.95819619\n",
      "Iteration 1732, loss = 110.95873988\n",
      "Iteration 1733, loss = 110.95675952\n",
      "Iteration 1734, loss = 110.95377040\n",
      "Iteration 1735, loss = 110.95718509\n",
      "Iteration 1736, loss = 110.95808027\n",
      "Iteration 1737, loss = 110.95627059\n",
      "Iteration 1738, loss = 110.95122148\n",
      "Iteration 1739, loss = 110.95407310\n",
      "Iteration 1740, loss = 110.95602458\n",
      "Iteration 1741, loss = 110.95570339\n",
      "Iteration 1742, loss = 110.95412639\n",
      "Iteration 1743, loss = 110.95071672\n",
      "Iteration 1744, loss = 110.94586328\n",
      "Iteration 1745, loss = 110.95073478\n",
      "Iteration 1746, loss = 110.95394574\n",
      "Iteration 1747, loss = 110.95383944\n",
      "Iteration 1748, loss = 110.94997506\n",
      "Iteration 1749, loss = 110.94293116\n",
      "Iteration 1750, loss = 110.94483378\n",
      "Iteration 1751, loss = 110.94794893\n",
      "Iteration 1752, loss = 110.94873552\n",
      "Iteration 1753, loss = 110.94760746\n",
      "Iteration 1754, loss = 110.94449148\n",
      "Iteration 1755, loss = 110.93937322\n",
      "Iteration 1756, loss = 110.93785355\n",
      "Iteration 1757, loss = 110.94048040\n",
      "Iteration 1758, loss = 110.94017502\n",
      "Iteration 1759, loss = 110.93636653\n",
      "Iteration 1760, loss = 110.93483458\n",
      "Iteration 1761, loss = 110.93590858\n",
      "Iteration 1762, loss = 110.93491709\n",
      "Iteration 1763, loss = 110.93253942\n",
      "Iteration 1764, loss = 110.92858548\n",
      "Iteration 1765, loss = 110.92860955\n",
      "Iteration 1766, loss = 110.92793215\n",
      "Iteration 1767, loss = 110.92677451\n",
      "Iteration 1768, loss = 110.92753542\n",
      "Iteration 1769, loss = 110.92613598\n",
      "Iteration 1770, loss = 110.92506237\n",
      "Iteration 1771, loss = 110.92501420\n",
      "Iteration 1772, loss = 110.92317416\n",
      "Iteration 1773, loss = 110.92412979\n",
      "Iteration 1774, loss = 110.92381591\n",
      "Iteration 1775, loss = 110.92035924\n",
      "Iteration 1776, loss = 110.92143365\n",
      "Iteration 1777, loss = 110.92288562\n",
      "Iteration 1778, loss = 110.92157223\n",
      "Iteration 1779, loss = 110.91840696\n",
      "Iteration 1780, loss = 110.91619383\n",
      "Iteration 1781, loss = 110.91687973\n",
      "Iteration 1782, loss = 110.91439466\n",
      "Iteration 1783, loss = 110.91549584\n",
      "Iteration 1784, loss = 110.91637259\n",
      "Iteration 1785, loss = 110.91517500\n",
      "Iteration 1786, loss = 110.91135240\n",
      "Iteration 1787, loss = 110.91339182\n",
      "Iteration 1788, loss = 110.91473642\n",
      "Iteration 1789, loss = 110.91200451\n",
      "Iteration 1790, loss = 110.90759569\n",
      "Iteration 1791, loss = 110.90859457\n",
      "Iteration 1792, loss = 110.90709702\n",
      "Iteration 1793, loss = 110.90704775\n",
      "Iteration 1794, loss = 110.90602362\n",
      "Iteration 1795, loss = 110.90471841\n",
      "Iteration 1796, loss = 110.90427932\n",
      "Iteration 1797, loss = 110.90254253\n",
      "Iteration 1798, loss = 110.90155841\n",
      "Iteration 1799, loss = 110.89998098\n",
      "Iteration 1800, loss = 110.90017872\n",
      "Iteration 1801, loss = 110.89937027\n",
      "Iteration 1802, loss = 110.90014667\n",
      "Iteration 1803, loss = 110.89873404\n",
      "Iteration 1804, loss = 110.89729306\n",
      "Iteration 1805, loss = 110.89708835\n",
      "Iteration 1806, loss = 110.89592271\n",
      "Iteration 1807, loss = 110.89429136\n",
      "Iteration 1808, loss = 110.89279144\n",
      "Iteration 1809, loss = 110.89309692\n",
      "Iteration 1810, loss = 110.89233178\n",
      "Iteration 1811, loss = 110.89378182\n",
      "Iteration 1812, loss = 110.89167441\n",
      "Iteration 1813, loss = 110.88901020\n",
      "Iteration 1814, loss = 110.88895277\n",
      "Iteration 1815, loss = 110.88810939\n",
      "Iteration 1816, loss = 110.88545174\n",
      "Iteration 1817, loss = 110.88573809\n",
      "Iteration 1818, loss = 110.88462138\n",
      "Iteration 1819, loss = 110.88337326\n",
      "Iteration 1820, loss = 110.88380942\n",
      "Iteration 1821, loss = 110.88240332\n",
      "Iteration 1822, loss = 110.88172684\n",
      "Iteration 1823, loss = 110.88085023\n",
      "Iteration 1824, loss = 110.88113835\n",
      "Iteration 1825, loss = 110.88058401\n",
      "Iteration 1826, loss = 110.87836488\n",
      "Iteration 1827, loss = 110.87905867\n",
      "Iteration 1828, loss = 110.87847280\n",
      "Iteration 1829, loss = 110.87583220\n",
      "Iteration 1830, loss = 110.87739418\n",
      "Iteration 1831, loss = 110.87807610\n",
      "Iteration 1832, loss = 110.87656313\n",
      "Iteration 1833, loss = 110.87381249\n",
      "Iteration 1834, loss = 110.87317589\n",
      "Iteration 1835, loss = 110.87398414\n",
      "Iteration 1836, loss = 110.87146460\n",
      "Iteration 1837, loss = 110.87004401\n",
      "Iteration 1838, loss = 110.87045692\n",
      "Iteration 1839, loss = 110.86962216\n",
      "Iteration 1840, loss = 110.86732258\n",
      "Iteration 1841, loss = 110.86589881\n",
      "Iteration 1842, loss = 110.86574821\n",
      "Iteration 1843, loss = 110.86536268\n",
      "Iteration 1844, loss = 110.86308270\n",
      "Iteration 1845, loss = 110.86575562\n",
      "Iteration 1846, loss = 110.86583849\n",
      "Iteration 1847, loss = 110.86294328\n",
      "Iteration 1848, loss = 110.86062348\n",
      "Iteration 1849, loss = 110.86041421\n",
      "Iteration 1850, loss = 110.86019970\n",
      "Iteration 1851, loss = 110.85756897\n",
      "Iteration 1852, loss = 110.86131334\n",
      "Iteration 1853, loss = 110.86126938\n",
      "Iteration 1854, loss = 110.85932586\n",
      "Iteration 1855, loss = 110.85476323\n",
      "Iteration 1856, loss = 110.85846299\n",
      "Iteration 1857, loss = 110.85993746\n",
      "Iteration 1858, loss = 110.85920149\n",
      "Iteration 1859, loss = 110.85750737\n",
      "Iteration 1860, loss = 110.85365083\n",
      "Iteration 1861, loss = 110.84913142\n",
      "Iteration 1862, loss = 110.85017304\n",
      "Iteration 1863, loss = 110.84991889\n",
      "Iteration 1864, loss = 110.84713140\n",
      "Iteration 1865, loss = 110.84684253\n",
      "Iteration 1866, loss = 110.84432711\n",
      "Iteration 1867, loss = 110.84421225\n",
      "Iteration 1868, loss = 110.84276403\n",
      "Iteration 1869, loss = 110.84617895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1870, loss = 110.84525051\n",
      "Iteration 1871, loss = 110.84120440\n",
      "Iteration 1872, loss = 110.84347810\n",
      "Iteration 1873, loss = 110.84441557\n",
      "Iteration 1874, loss = 110.84352291\n",
      "Iteration 1875, loss = 110.84048673\n",
      "Iteration 1876, loss = 110.83570699\n",
      "Iteration 1877, loss = 110.83680140\n",
      "Iteration 1878, loss = 110.83451553\n",
      "Iteration 1879, loss = 110.83682065\n",
      "Iteration 1880, loss = 110.83743811\n",
      "Iteration 1881, loss = 110.83538660\n",
      "Iteration 1882, loss = 110.83064242\n",
      "Iteration 1883, loss = 110.83415642\n",
      "Iteration 1884, loss = 110.83608053\n",
      "Iteration 1885, loss = 110.83362827\n",
      "Iteration 1886, loss = 110.82804127\n",
      "Iteration 1887, loss = 110.83109995\n",
      "Iteration 1888, loss = 110.83206239\n",
      "Iteration 1889, loss = 110.83275715\n",
      "Iteration 1890, loss = 110.83092441\n",
      "Iteration 1891, loss = 110.82630399\n",
      "Iteration 1892, loss = 110.82496936\n",
      "Iteration 1893, loss = 110.82698540\n",
      "Iteration 1894, loss = 110.82474276\n",
      "Iteration 1895, loss = 110.81936728\n",
      "Iteration 1896, loss = 110.82320090\n",
      "Iteration 1897, loss = 110.82523652\n",
      "Iteration 1898, loss = 110.82459548\n",
      "Iteration 1899, loss = 110.82097943\n",
      "Iteration 1900, loss = 110.81660173\n",
      "Iteration 1901, loss = 110.81938822\n",
      "Iteration 1902, loss = 110.82166880\n",
      "Iteration 1903, loss = 110.81983979\n",
      "Iteration 1904, loss = 110.81590222\n",
      "Iteration 1905, loss = 110.81334854\n",
      "Iteration 1906, loss = 110.81487636\n",
      "Iteration 1907, loss = 110.81386843\n",
      "Iteration 1908, loss = 110.81043946\n",
      "Iteration 1909, loss = 110.80806735\n",
      "Iteration 1910, loss = 110.80872892\n",
      "Iteration 1911, loss = 110.80580493\n",
      "Iteration 1912, loss = 110.80760715\n",
      "Iteration 1913, loss = 110.80784746\n",
      "Iteration 1914, loss = 110.80632032\n",
      "Iteration 1915, loss = 110.80283487\n",
      "Iteration 1916, loss = 110.80420056\n",
      "Iteration 1917, loss = 110.80468719\n",
      "Iteration 1918, loss = 110.80249589\n",
      "Iteration 1919, loss = 110.79751224\n",
      "Iteration 1920, loss = 110.80121327\n",
      "Iteration 1921, loss = 110.80320184\n",
      "Iteration 1922, loss = 110.80281508\n",
      "Iteration 1923, loss = 110.80019521\n",
      "Iteration 1924, loss = 110.79577584\n",
      "Iteration 1925, loss = 110.79373474\n",
      "Iteration 1926, loss = 110.79488353\n",
      "Iteration 1927, loss = 110.79400281\n",
      "Iteration 1928, loss = 110.78984635\n",
      "Iteration 1929, loss = 110.79205735\n",
      "Iteration 1930, loss = 110.79231174\n",
      "Iteration 1931, loss = 110.79272832\n",
      "Iteration 1932, loss = 110.79027861\n",
      "Iteration 1933, loss = 110.78513792\n",
      "Iteration 1934, loss = 110.78733436\n",
      "Iteration 1935, loss = 110.78944209\n",
      "Iteration 1936, loss = 110.78902985\n",
      "Iteration 1937, loss = 110.78439942\n",
      "Iteration 1938, loss = 110.78078703\n",
      "Iteration 1939, loss = 110.78170683\n",
      "Iteration 1940, loss = 110.78188371\n",
      "Iteration 1941, loss = 110.77868924\n",
      "Iteration 1942, loss = 110.77629618\n",
      "Iteration 1943, loss = 110.77675680\n",
      "Iteration 1944, loss = 110.77300641\n",
      "Iteration 1945, loss = 110.77564016\n",
      "Iteration 1946, loss = 110.77593564\n",
      "Iteration 1947, loss = 110.77504446\n",
      "Iteration 1948, loss = 110.77193566\n",
      "Iteration 1949, loss = 110.77049015\n",
      "Iteration 1950, loss = 110.77027456\n",
      "Iteration 1951, loss = 110.76795059\n",
      "Iteration 1952, loss = 110.76657753\n",
      "Iteration 1953, loss = 110.76798582\n",
      "Iteration 1954, loss = 110.76625347\n",
      "Iteration 1955, loss = 110.76284370\n",
      "Iteration 1956, loss = 110.76144031\n",
      "Iteration 1957, loss = 110.76219879\n",
      "Iteration 1958, loss = 110.76165725\n",
      "Iteration 1959, loss = 110.75884005\n",
      "Iteration 1960, loss = 110.75966393\n",
      "Iteration 1961, loss = 110.75957909\n",
      "Iteration 1962, loss = 110.75586188\n",
      "Iteration 1963, loss = 110.75655458\n",
      "Iteration 1964, loss = 110.75755093\n",
      "Iteration 1965, loss = 110.75597560\n",
      "Iteration 1966, loss = 110.75271879\n",
      "Iteration 1967, loss = 110.75199712\n",
      "Iteration 1968, loss = 110.75151981\n",
      "Iteration 1969, loss = 110.74863146\n",
      "Iteration 1970, loss = 110.75016305\n",
      "Iteration 1971, loss = 110.75084647\n",
      "Iteration 1972, loss = 110.74915313\n",
      "Iteration 1973, loss = 110.74594290\n",
      "Iteration 1974, loss = 110.74644838\n",
      "Iteration 1975, loss = 110.74667959\n",
      "Iteration 1976, loss = 110.74447246\n",
      "Iteration 1977, loss = 110.74289869\n",
      "Iteration 1978, loss = 110.74286560\n",
      "Iteration 1979, loss = 110.74201121\n",
      "Iteration 1980, loss = 110.73903088\n",
      "Iteration 1981, loss = 110.74160420\n",
      "Iteration 1982, loss = 110.74133777\n",
      "Iteration 1983, loss = 110.73877093\n",
      "Iteration 1984, loss = 110.73484672\n",
      "Iteration 1985, loss = 110.73568934\n",
      "Iteration 1986, loss = 110.73417004\n",
      "Iteration 1987, loss = 110.73135896\n",
      "Iteration 1988, loss = 110.73018016\n",
      "Iteration 1989, loss = 110.73244506\n",
      "Iteration 1990, loss = 110.73134705\n",
      "Iteration 1991, loss = 110.72934347\n",
      "Iteration 1992, loss = 110.72822156\n",
      "Iteration 1993, loss = 110.72782029\n",
      "Iteration 1994, loss = 110.72566047\n",
      "Iteration 1995, loss = 110.72493602\n",
      "Iteration 1996, loss = 110.72483358\n",
      "Iteration 1997, loss = 110.72241030\n",
      "Iteration 1998, loss = 110.72512329\n",
      "Iteration 1999, loss = 110.72540096\n",
      "Iteration 2000, loss = 110.72338810\n",
      "Iteration 2001, loss = 110.71918814\n",
      "Iteration 2002, loss = 110.72248651\n",
      "Iteration 2003, loss = 110.72360668\n",
      "Iteration 2004, loss = 110.72128426\n",
      "Iteration 2005, loss = 110.71595497\n",
      "Iteration 2006, loss = 110.71909214\n",
      "Iteration 2007, loss = 110.72111786\n",
      "Iteration 2008, loss = 110.72183787\n",
      "Iteration 2009, loss = 110.71978991\n",
      "Iteration 2010, loss = 110.71620348\n",
      "Iteration 2011, loss = 110.71054112\n",
      "Iteration 2012, loss = 110.71453772\n",
      "Iteration 2013, loss = 110.71809044\n",
      "Iteration 2014, loss = 110.71796965\n",
      "Iteration 2015, loss = 110.71304626\n",
      "Iteration 2016, loss = 110.70480570\n",
      "Iteration 2017, loss = 110.70961712\n",
      "Iteration 2018, loss = 110.71293104\n",
      "Iteration 2019, loss = 110.71409465\n",
      "Iteration 2020, loss = 110.71266273\n",
      "Iteration 2021, loss = 110.70884627\n",
      "Iteration 2022, loss = 110.70402781\n",
      "Iteration 2023, loss = 110.69839790\n",
      "Iteration 2024, loss = 110.70029493\n",
      "Iteration 2025, loss = 110.69876130\n",
      "Iteration 2026, loss = 110.69478158\n",
      "Iteration 2027, loss = 110.69447953\n",
      "Iteration 2028, loss = 110.69417225\n",
      "Iteration 2029, loss = 110.69233424\n",
      "Iteration 2030, loss = 110.69174908\n",
      "Iteration 2031, loss = 110.69157330\n",
      "Iteration 2032, loss = 110.69048753\n",
      "Iteration 2033, loss = 110.69051880\n",
      "Iteration 2034, loss = 110.68831459\n",
      "Iteration 2035, loss = 110.68871439\n",
      "Iteration 2036, loss = 110.68888549\n",
      "Iteration 2037, loss = 110.68717610\n",
      "Iteration 2038, loss = 110.68548742\n",
      "Iteration 2039, loss = 110.68464409\n",
      "Iteration 2040, loss = 110.68274799\n",
      "Iteration 2041, loss = 110.68194207\n",
      "Iteration 2042, loss = 110.68150797\n",
      "Iteration 2043, loss = 110.67930140\n",
      "Iteration 2044, loss = 110.68205765\n",
      "Iteration 2045, loss = 110.68143860\n",
      "Iteration 2046, loss = 110.67970221\n",
      "Iteration 2047, loss = 110.67602911\n",
      "Iteration 2048, loss = 110.67503409\n",
      "Iteration 2049, loss = 110.67661085\n",
      "Iteration 2050, loss = 110.67519914\n",
      "Iteration 2051, loss = 110.67360888\n",
      "Iteration 2052, loss = 110.67510555\n",
      "Iteration 2053, loss = 110.67459377\n",
      "Iteration 2054, loss = 110.67143811\n",
      "Iteration 2055, loss = 110.67064696\n",
      "Iteration 2056, loss = 110.67195445\n",
      "Iteration 2057, loss = 110.66990286\n",
      "Iteration 2058, loss = 110.66735358\n",
      "Iteration 2059, loss = 110.66778072\n",
      "Iteration 2060, loss = 110.66809459\n",
      "Iteration 2061, loss = 110.66441945\n",
      "Iteration 2062, loss = 110.66407688\n",
      "Iteration 2063, loss = 110.66514736\n",
      "Iteration 2064, loss = 110.66325903\n",
      "Iteration 2065, loss = 110.65946737\n",
      "Iteration 2066, loss = 110.66291552\n",
      "Iteration 2067, loss = 110.66355086\n",
      "Iteration 2068, loss = 110.66031479\n",
      "Iteration 2069, loss = 110.65522360\n",
      "Iteration 2070, loss = 110.65664071\n",
      "Iteration 2071, loss = 110.65494119\n",
      "Iteration 2072, loss = 110.65394391\n",
      "Iteration 2073, loss = 110.65346065\n",
      "Iteration 2074, loss = 110.65238484\n",
      "Iteration 2075, loss = 110.65164937\n",
      "Iteration 2076, loss = 110.64881400\n",
      "Iteration 2077, loss = 110.65216593\n",
      "Iteration 2078, loss = 110.65244207\n",
      "Iteration 2079, loss = 110.64890101\n",
      "Iteration 2080, loss = 110.64818445\n",
      "Iteration 2081, loss = 110.64932363\n",
      "Iteration 2082, loss = 110.64755937\n",
      "Iteration 2083, loss = 110.64424733\n",
      "Iteration 2084, loss = 110.64411835\n",
      "Iteration 2085, loss = 110.64438772\n",
      "Iteration 2086, loss = 110.64188284\n",
      "Iteration 2087, loss = 110.64013653\n",
      "Iteration 2088, loss = 110.64091555\n",
      "Iteration 2089, loss = 110.63942851\n",
      "Iteration 2090, loss = 110.63535494\n",
      "Iteration 2091, loss = 110.64051210\n",
      "Iteration 2092, loss = 110.64132721\n",
      "Iteration 2093, loss = 110.63879883\n",
      "Iteration 2094, loss = 110.63277671\n",
      "Iteration 2095, loss = 110.63627984\n",
      "Iteration 2096, loss = 110.63923217\n",
      "Iteration 2097, loss = 110.63967340\n",
      "Iteration 2098, loss = 110.63752066\n",
      "Iteration 2099, loss = 110.63242414\n",
      "Iteration 2100, loss = 110.62713090\n",
      "Iteration 2101, loss = 110.62896552\n",
      "Iteration 2102, loss = 110.62698364\n",
      "Iteration 2103, loss = 110.62566615\n",
      "Iteration 2104, loss = 110.62536928\n",
      "Iteration 2105, loss = 110.62341446\n",
      "Iteration 2106, loss = 110.62369433\n",
      "Iteration 2107, loss = 110.62296198\n",
      "Iteration 2108, loss = 110.61899825\n",
      "Iteration 2109, loss = 110.62312774\n",
      "Iteration 2110, loss = 110.62440772\n",
      "Iteration 2111, loss = 110.62294282\n",
      "Iteration 2112, loss = 110.61939451\n",
      "Iteration 2113, loss = 110.61480636\n",
      "Iteration 2114, loss = 110.61564885\n",
      "Iteration 2115, loss = 110.61302788\n",
      "Iteration 2116, loss = 110.61229440\n",
      "Iteration 2117, loss = 110.61198731\n",
      "Iteration 2118, loss = 110.61128038\n",
      "Iteration 2119, loss = 110.61085957\n",
      "Iteration 2120, loss = 110.60898498\n",
      "Iteration 2121, loss = 110.60939135\n",
      "Iteration 2122, loss = 110.60962207\n",
      "Iteration 2123, loss = 110.60703266\n",
      "Iteration 2124, loss = 110.60718511\n",
      "Iteration 2125, loss = 110.60700831\n",
      "Iteration 2126, loss = 110.60364120\n",
      "Iteration 2127, loss = 110.60580945\n",
      "Iteration 2128, loss = 110.60682620\n",
      "Iteration 2129, loss = 110.60556949\n",
      "Iteration 2130, loss = 110.60283552\n",
      "Iteration 2131, loss = 110.60027690\n",
      "Iteration 2132, loss = 110.60057588\n",
      "Iteration 2133, loss = 110.59801179\n",
      "Iteration 2134, loss = 110.59841969\n",
      "Iteration 2135, loss = 110.59868874\n",
      "Iteration 2136, loss = 110.59728645\n",
      "Iteration 2137, loss = 110.59354374\n",
      "Iteration 2138, loss = 110.59798220\n",
      "Iteration 2139, loss = 110.59858010\n",
      "Iteration 2140, loss = 110.59650592\n",
      "Iteration 2141, loss = 110.59128167\n",
      "Iteration 2142, loss = 110.59320602\n",
      "Iteration 2143, loss = 110.59525985\n",
      "Iteration 2144, loss = 110.59519162\n",
      "Iteration 2145, loss = 110.59276110\n",
      "Iteration 2146, loss = 110.58830181\n",
      "Iteration 2147, loss = 110.58656652\n",
      "Iteration 2148, loss = 110.58844783\n",
      "Iteration 2149, loss = 110.58665113\n",
      "Iteration 2150, loss = 110.58133765\n",
      "Iteration 2151, loss = 110.58540343\n",
      "Iteration 2152, loss = 110.58788998\n",
      "Iteration 2153, loss = 110.58736232\n",
      "Iteration 2154, loss = 110.58367602\n",
      "Iteration 2155, loss = 110.57869554\n",
      "Iteration 2156, loss = 110.58068857\n",
      "Iteration 2157, loss = 110.58347711\n",
      "Iteration 2158, loss = 110.58185113\n",
      "Iteration 2159, loss = 110.57682190\n",
      "Iteration 2160, loss = 110.57501145\n",
      "Iteration 2161, loss = 110.57669819\n",
      "Iteration 2162, loss = 110.57629403\n",
      "Iteration 2163, loss = 110.57321371\n",
      "Iteration 2164, loss = 110.56868056\n",
      "Iteration 2165, loss = 110.56924028\n",
      "Iteration 2166, loss = 110.56761493\n",
      "Iteration 2167, loss = 110.56595730\n",
      "Iteration 2168, loss = 110.56696073\n",
      "Iteration 2169, loss = 110.56580831\n",
      "Iteration 2170, loss = 110.56377163\n",
      "Iteration 2171, loss = 110.56221256\n",
      "Iteration 2172, loss = 110.56380204\n",
      "Iteration 2173, loss = 110.56407148\n",
      "Iteration 2174, loss = 110.56179079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2175, loss = 110.56155117\n",
      "Iteration 2176, loss = 110.56125670\n",
      "Iteration 2177, loss = 110.55843646\n",
      "Iteration 2178, loss = 110.56047025\n",
      "Iteration 2179, loss = 110.56169730\n",
      "Iteration 2180, loss = 110.55961576\n",
      "Iteration 2181, loss = 110.55578089\n",
      "Iteration 2182, loss = 110.55619214\n",
      "Iteration 2183, loss = 110.55738192\n",
      "Iteration 2184, loss = 110.55523743\n",
      "Iteration 2185, loss = 110.55088923\n",
      "Iteration 2186, loss = 110.55112096\n",
      "Iteration 2187, loss = 110.54908970\n",
      "Iteration 2188, loss = 110.54987177\n",
      "Iteration 2189, loss = 110.54953618\n",
      "Iteration 2190, loss = 110.54579393\n",
      "Iteration 2191, loss = 110.54939457\n",
      "Iteration 2192, loss = 110.55044741\n",
      "Iteration 2193, loss = 110.54868951\n",
      "Iteration 2194, loss = 110.54538166\n",
      "Iteration 2195, loss = 110.54105286\n",
      "Iteration 2196, loss = 110.54182345\n",
      "Iteration 2197, loss = 110.53894612\n",
      "Iteration 2198, loss = 110.54092340\n",
      "Iteration 2199, loss = 110.54120723\n",
      "Iteration 2200, loss = 110.53871778\n",
      "Iteration 2201, loss = 110.53488680\n",
      "Iteration 2202, loss = 110.53468111\n",
      "Iteration 2203, loss = 110.53537240\n",
      "Iteration 2204, loss = 110.53467212\n",
      "Iteration 2205, loss = 110.53229304\n",
      "Iteration 2206, loss = 110.53008545\n",
      "Iteration 2207, loss = 110.53113860\n",
      "Iteration 2208, loss = 110.52909079\n",
      "Iteration 2209, loss = 110.53085091\n",
      "Iteration 2210, loss = 110.53055408\n",
      "Iteration 2211, loss = 110.52878940\n",
      "Iteration 2212, loss = 110.52598296\n",
      "Iteration 2213, loss = 110.52524011\n",
      "Iteration 2214, loss = 110.52425949\n",
      "Iteration 2215, loss = 110.52320402\n",
      "Iteration 2216, loss = 110.52302453\n",
      "Iteration 2217, loss = 110.52142395\n",
      "Iteration 2218, loss = 110.52155849\n",
      "Iteration 2219, loss = 110.52089903\n",
      "Iteration 2220, loss = 110.51860952\n",
      "Iteration 2221, loss = 110.52036193\n",
      "Iteration 2222, loss = 110.52044076\n",
      "Iteration 2223, loss = 110.51766241\n",
      "Iteration 2224, loss = 110.51687271\n",
      "Iteration 2225, loss = 110.51726414\n",
      "Iteration 2226, loss = 110.51603131\n",
      "Iteration 2227, loss = 110.51247855\n",
      "Iteration 2228, loss = 110.51587064\n",
      "Iteration 2229, loss = 110.51755754\n",
      "Iteration 2230, loss = 110.51532164\n",
      "Iteration 2231, loss = 110.50994363\n",
      "Iteration 2232, loss = 110.51217157\n",
      "Iteration 2233, loss = 110.51481937\n",
      "Iteration 2234, loss = 110.51398617\n",
      "Iteration 2235, loss = 110.51065098\n",
      "Iteration 2236, loss = 110.50593710\n",
      "Iteration 2237, loss = 110.50714686\n",
      "Iteration 2238, loss = 110.50903515\n",
      "Iteration 2239, loss = 110.50811733\n",
      "Iteration 2240, loss = 110.50377842\n",
      "Iteration 2241, loss = 110.50094373\n",
      "Iteration 2242, loss = 110.50297323\n",
      "Iteration 2243, loss = 110.50250174\n",
      "Iteration 2244, loss = 110.49953475\n",
      "Iteration 2245, loss = 110.49859701\n",
      "Iteration 2246, loss = 110.49907131\n",
      "Iteration 2247, loss = 110.49598461\n",
      "Iteration 2248, loss = 110.49479767\n",
      "Iteration 2249, loss = 110.49587878\n",
      "Iteration 2250, loss = 110.49373382\n",
      "Iteration 2251, loss = 110.49121780\n",
      "Iteration 2252, loss = 110.49069298\n",
      "Iteration 2253, loss = 110.48979563\n",
      "Iteration 2254, loss = 110.48883147\n",
      "Iteration 2255, loss = 110.48855469\n",
      "Iteration 2256, loss = 110.48634565\n",
      "Iteration 2257, loss = 110.48652668\n",
      "Iteration 2258, loss = 110.48608117\n",
      "Iteration 2259, loss = 110.48302120\n",
      "Iteration 2260, loss = 110.48598859\n",
      "Iteration 2261, loss = 110.48672795\n",
      "Iteration 2262, loss = 110.48360319\n",
      "Iteration 2263, loss = 110.48137258\n",
      "Iteration 2264, loss = 110.48250417\n",
      "Iteration 2265, loss = 110.48009183\n",
      "Iteration 2266, loss = 110.47720708\n",
      "Iteration 2267, loss = 110.47649520\n",
      "Iteration 2268, loss = 110.47660855\n",
      "Iteration 2269, loss = 110.47577210\n",
      "Iteration 2270, loss = 110.47382109\n",
      "Iteration 2271, loss = 110.47288053\n",
      "Iteration 2272, loss = 110.47445832\n",
      "Iteration 2273, loss = 110.47355223\n",
      "Iteration 2274, loss = 110.47111407\n",
      "Iteration 2275, loss = 110.47228877\n",
      "Iteration 2276, loss = 110.47233447\n",
      "Iteration 2277, loss = 110.47015590\n",
      "Iteration 2278, loss = 110.46913391\n",
      "Iteration 2279, loss = 110.46880952\n",
      "Iteration 2280, loss = 110.46678150\n",
      "Iteration 2281, loss = 110.46454442\n",
      "Iteration 2282, loss = 110.46377820\n",
      "Iteration 2283, loss = 110.46385461\n",
      "Iteration 2284, loss = 110.46198527\n",
      "Iteration 2285, loss = 110.46057777\n",
      "Iteration 2286, loss = 110.45961467\n",
      "Iteration 2287, loss = 110.46099088\n",
      "Iteration 2288, loss = 110.46057427\n",
      "Iteration 2289, loss = 110.45772411\n",
      "Iteration 2290, loss = 110.45952346\n",
      "Iteration 2291, loss = 110.45981411\n",
      "Iteration 2292, loss = 110.45658100\n",
      "Iteration 2293, loss = 110.45405024\n",
      "Iteration 2294, loss = 110.45485118\n",
      "Iteration 2295, loss = 110.45329728\n",
      "Iteration 2296, loss = 110.45145367\n",
      "Iteration 2297, loss = 110.45075919\n",
      "Iteration 2298, loss = 110.44886009\n",
      "Iteration 2299, loss = 110.44758315\n",
      "Iteration 2300, loss = 110.44815037\n",
      "Iteration 2301, loss = 110.44765042\n",
      "Iteration 2302, loss = 110.44630521\n",
      "Iteration 2303, loss = 110.44510986\n",
      "Iteration 2304, loss = 110.44333895\n",
      "Iteration 2305, loss = 110.44304373\n",
      "Iteration 2306, loss = 110.44277826\n",
      "Iteration 2307, loss = 110.44070412\n",
      "Iteration 2308, loss = 110.44094298\n",
      "Iteration 2309, loss = 110.43856598\n",
      "Iteration 2310, loss = 110.43851813\n",
      "Iteration 2311, loss = 110.43816794\n",
      "Iteration 2312, loss = 110.43611388\n",
      "Iteration 2313, loss = 110.43787840\n",
      "Iteration 2314, loss = 110.43722235\n",
      "Iteration 2315, loss = 110.43389935\n",
      "Iteration 2316, loss = 110.43778080\n",
      "Iteration 2317, loss = 110.43915838\n",
      "Iteration 2318, loss = 110.43697378\n",
      "Iteration 2319, loss = 110.43248232\n",
      "Iteration 2320, loss = 110.43124767\n",
      "Iteration 2321, loss = 110.43316742\n",
      "Iteration 2322, loss = 110.43041032\n",
      "Iteration 2323, loss = 110.42699272\n",
      "Iteration 2324, loss = 110.42727467\n",
      "Iteration 2325, loss = 110.42474777\n",
      "Iteration 2326, loss = 110.42794523\n",
      "Iteration 2327, loss = 110.42832673\n",
      "Iteration 2328, loss = 110.42484043\n",
      "Iteration 2329, loss = 110.42265000\n",
      "Iteration 2330, loss = 110.42387026\n",
      "Iteration 2331, loss = 110.42215601\n",
      "Iteration 2332, loss = 110.41871974\n",
      "Iteration 2333, loss = 110.41826402\n",
      "Iteration 2334, loss = 110.41834508\n",
      "Iteration 2335, loss = 110.41670848\n",
      "Iteration 2336, loss = 110.41588823\n",
      "Iteration 2337, loss = 110.41509442\n",
      "Iteration 2338, loss = 110.41518390\n",
      "Iteration 2339, loss = 110.41480664\n",
      "Iteration 2340, loss = 110.41150928\n",
      "Iteration 2341, loss = 110.41489595\n",
      "Iteration 2342, loss = 110.41585182\n",
      "Iteration 2343, loss = 110.41252827\n",
      "Iteration 2344, loss = 110.40799766\n",
      "Iteration 2345, loss = 110.40878105\n",
      "Iteration 2346, loss = 110.40628742\n",
      "Iteration 2347, loss = 110.40800937\n",
      "Iteration 2348, loss = 110.40812868\n",
      "Iteration 2349, loss = 110.40547723\n",
      "Iteration 2350, loss = 110.40616098\n",
      "Iteration 2351, loss = 110.40698762\n",
      "Iteration 2352, loss = 110.40514200\n",
      "Iteration 2353, loss = 110.40071403\n",
      "Iteration 2354, loss = 110.40401333\n",
      "Iteration 2355, loss = 110.40538271\n",
      "Iteration 2356, loss = 110.40326138\n",
      "Iteration 2357, loss = 110.39826507\n",
      "Iteration 2358, loss = 110.39913262\n",
      "Iteration 2359, loss = 110.40061160\n",
      "Iteration 2360, loss = 110.39968402\n",
      "Iteration 2361, loss = 110.39661047\n",
      "Iteration 2362, loss = 110.39301028\n",
      "Iteration 2363, loss = 110.39382622\n",
      "Iteration 2364, loss = 110.39131117\n",
      "Iteration 2365, loss = 110.39127176\n",
      "Iteration 2366, loss = 110.39202132\n",
      "Iteration 2367, loss = 110.38929413\n",
      "Iteration 2368, loss = 110.38845756\n",
      "Iteration 2369, loss = 110.38875787\n",
      "Iteration 2370, loss = 110.38538797\n",
      "Iteration 2371, loss = 110.38723592\n",
      "Iteration 2372, loss = 110.38889108\n",
      "Iteration 2373, loss = 110.38647713\n",
      "Iteration 2374, loss = 110.38141175\n",
      "Iteration 2375, loss = 110.38584894\n",
      "Iteration 2376, loss = 110.38761652\n",
      "Iteration 2377, loss = 110.38658253\n",
      "Iteration 2378, loss = 110.38213319\n",
      "Iteration 2379, loss = 110.37903174\n",
      "Iteration 2380, loss = 110.38070362\n",
      "Iteration 2381, loss = 110.37993598\n",
      "Iteration 2382, loss = 110.37666620\n",
      "Iteration 2383, loss = 110.37640307\n",
      "Iteration 2384, loss = 110.37692758\n",
      "Iteration 2385, loss = 110.37429765\n",
      "Iteration 2386, loss = 110.37116256\n",
      "Iteration 2387, loss = 110.37109429\n",
      "Iteration 2388, loss = 110.36861683\n",
      "Iteration 2389, loss = 110.37173454\n",
      "Iteration 2390, loss = 110.37206996\n",
      "Iteration 2391, loss = 110.36892829\n",
      "Iteration 2392, loss = 110.36695809\n",
      "Iteration 2393, loss = 110.36756720\n",
      "Iteration 2394, loss = 110.36527384\n",
      "Iteration 2395, loss = 110.36354238\n",
      "Iteration 2396, loss = 110.36351232\n",
      "Iteration 2397, loss = 110.36102723\n",
      "Iteration 2398, loss = 110.35957457\n",
      "Iteration 2399, loss = 110.36138984\n",
      "Iteration 2400, loss = 110.36003365\n",
      "Iteration 2401, loss = 110.35864731\n",
      "Iteration 2402, loss = 110.35782698\n",
      "Iteration 2403, loss = 110.35735088\n",
      "Iteration 2404, loss = 110.35628810\n",
      "Iteration 2405, loss = 110.35528201\n",
      "Iteration 2406, loss = 110.35356990\n",
      "Iteration 2407, loss = 110.35405857\n",
      "Iteration 2408, loss = 110.35415359\n",
      "Iteration 2409, loss = 110.35116575\n",
      "Iteration 2410, loss = 110.35408304\n",
      "Iteration 2411, loss = 110.35525627\n",
      "Iteration 2412, loss = 110.35272880\n",
      "Iteration 2413, loss = 110.34748091\n",
      "Iteration 2414, loss = 110.35230535\n",
      "Iteration 2415, loss = 110.35485423\n",
      "Iteration 2416, loss = 110.35391506\n",
      "Iteration 2417, loss = 110.35037714\n",
      "Iteration 2418, loss = 110.34413436\n",
      "Iteration 2419, loss = 110.34688706\n",
      "Iteration 2420, loss = 110.35017640\n",
      "Iteration 2421, loss = 110.34922944\n",
      "Iteration 2422, loss = 110.34610212\n",
      "Iteration 2423, loss = 110.33959311\n",
      "Iteration 2424, loss = 110.34392775\n",
      "Iteration 2425, loss = 110.34738658\n",
      "Iteration 2426, loss = 110.34796895\n",
      "Iteration 2427, loss = 110.34475380\n",
      "Iteration 2428, loss = 110.33875681\n",
      "Iteration 2429, loss = 110.33446577\n",
      "Iteration 2430, loss = 110.33738086\n",
      "Iteration 2431, loss = 110.33647997\n",
      "Iteration 2432, loss = 110.33214345\n",
      "Iteration 2433, loss = 110.33255285\n",
      "Iteration 2434, loss = 110.33380682\n",
      "Iteration 2435, loss = 110.33215589\n",
      "Iteration 2436, loss = 110.32819574\n",
      "Iteration 2437, loss = 110.33060666\n",
      "Iteration 2438, loss = 110.33212074\n",
      "Iteration 2439, loss = 110.33038610\n",
      "Iteration 2440, loss = 110.32583927\n",
      "Iteration 2441, loss = 110.32685171\n",
      "Iteration 2442, loss = 110.32864320\n",
      "Iteration 2443, loss = 110.32748658\n",
      "Iteration 2444, loss = 110.32349412\n",
      "Iteration 2445, loss = 110.32066338\n",
      "Iteration 2446, loss = 110.32174999\n",
      "Iteration 2447, loss = 110.31935698\n",
      "Iteration 2448, loss = 110.31733715\n",
      "Iteration 2449, loss = 110.31781156\n",
      "Iteration 2450, loss = 110.31506298\n",
      "Iteration 2451, loss = 110.31772430\n",
      "Iteration 2452, loss = 110.31817072\n",
      "Iteration 2453, loss = 110.31548767\n",
      "Iteration 2454, loss = 110.31166635\n",
      "Iteration 2455, loss = 110.31212293\n",
      "Iteration 2456, loss = 110.31001323\n",
      "Iteration 2457, loss = 110.31145323\n",
      "Iteration 2458, loss = 110.31143028\n",
      "Iteration 2459, loss = 110.30888694\n",
      "Iteration 2460, loss = 110.30886553\n",
      "Iteration 2461, loss = 110.30987856\n",
      "Iteration 2462, loss = 110.30767001\n",
      "Iteration 2463, loss = 110.30411132\n",
      "Iteration 2464, loss = 110.30369651\n",
      "Iteration 2465, loss = 110.30240600\n",
      "Iteration 2466, loss = 110.30071991\n",
      "Iteration 2467, loss = 110.30287247\n",
      "Iteration 2468, loss = 110.30276669\n",
      "Iteration 2469, loss = 110.29970678\n",
      "Iteration 2470, loss = 110.30155631\n",
      "Iteration 2471, loss = 110.30247763\n",
      "Iteration 2472, loss = 110.30130184\n",
      "Iteration 2473, loss = 110.29671398\n",
      "Iteration 2474, loss = 110.29902757\n",
      "Iteration 2475, loss = 110.30113208\n",
      "Iteration 2476, loss = 110.29962155\n",
      "Iteration 2477, loss = 110.29514387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2478, loss = 110.29340318\n",
      "Iteration 2479, loss = 110.29476902\n",
      "Iteration 2480, loss = 110.29307037\n",
      "Iteration 2481, loss = 110.28971614\n",
      "Iteration 2482, loss = 110.29160938\n",
      "Iteration 2483, loss = 110.29243194\n",
      "Iteration 2484, loss = 110.29064649\n",
      "Iteration 2485, loss = 110.28611375\n",
      "Iteration 2486, loss = 110.28819842\n",
      "Iteration 2487, loss = 110.29001116\n",
      "Iteration 2488, loss = 110.28887904\n",
      "Iteration 2489, loss = 110.28510576\n",
      "Iteration 2490, loss = 110.28211464\n",
      "Iteration 2491, loss = 110.28302066\n",
      "Iteration 2492, loss = 110.28136690\n",
      "Iteration 2493, loss = 110.27916970\n",
      "Iteration 2494, loss = 110.27908756\n",
      "Iteration 2495, loss = 110.27749102\n",
      "Iteration 2496, loss = 110.27556176\n",
      "Iteration 2497, loss = 110.27794196\n",
      "Iteration 2498, loss = 110.27767102\n",
      "Iteration 2499, loss = 110.27433984\n",
      "Iteration 2500, loss = 110.27558323\n",
      "Iteration 2501, loss = 110.27608864\n",
      "Iteration 2502, loss = 110.27378052\n",
      "Iteration 2503, loss = 110.26963901\n",
      "Iteration 2504, loss = 110.26911150\n",
      "Iteration 2505, loss = 110.26799039\n",
      "Iteration 2506, loss = 110.26749833\n",
      "Iteration 2507, loss = 110.26686531\n",
      "Iteration 2508, loss = 110.26539595\n",
      "Iteration 2509, loss = 110.26549693\n",
      "Iteration 2510, loss = 110.26333521\n",
      "Iteration 2511, loss = 110.26357964\n",
      "Iteration 2512, loss = 110.26134869\n",
      "Iteration 2513, loss = 110.26129182\n",
      "Iteration 2514, loss = 110.25919218\n",
      "Iteration 2515, loss = 110.26061210\n",
      "Iteration 2516, loss = 110.25937413\n",
      "Iteration 2517, loss = 110.25755803\n",
      "Iteration 2518, loss = 110.25712557\n",
      "Iteration 2519, loss = 110.25591886\n",
      "Iteration 2520, loss = 110.25615182\n",
      "Iteration 2521, loss = 110.25411026\n",
      "Iteration 2522, loss = 110.25775586\n",
      "Iteration 2523, loss = 110.25756038\n",
      "Iteration 2524, loss = 110.25362032\n",
      "Iteration 2525, loss = 110.25265401\n",
      "Iteration 2526, loss = 110.25297769\n",
      "Iteration 2527, loss = 110.25091226\n",
      "Iteration 2528, loss = 110.24841435\n",
      "Iteration 2529, loss = 110.24820742\n",
      "Iteration 2530, loss = 110.24516350\n",
      "Iteration 2531, loss = 110.24916300\n",
      "Iteration 2532, loss = 110.24976968\n",
      "Iteration 2533, loss = 110.24709867\n",
      "Iteration 2534, loss = 110.24162849\n",
      "Iteration 2535, loss = 110.24805448\n",
      "Iteration 2536, loss = 110.24997920\n",
      "Iteration 2537, loss = 110.24878390\n",
      "Iteration 2538, loss = 110.24495946\n",
      "Iteration 2539, loss = 110.23855050\n",
      "Iteration 2540, loss = 110.24435701\n",
      "Iteration 2541, loss = 110.24796094\n",
      "Iteration 2542, loss = 110.24790222\n",
      "Iteration 2543, loss = 110.24471235\n",
      "Iteration 2544, loss = 110.23878462\n",
      "Iteration 2545, loss = 110.23420365\n",
      "Iteration 2546, loss = 110.23694040\n",
      "Iteration 2547, loss = 110.23641379\n",
      "Iteration 2548, loss = 110.23289770\n",
      "Iteration 2549, loss = 110.23127029\n",
      "Iteration 2550, loss = 110.23245773\n",
      "Iteration 2551, loss = 110.23024247\n",
      "Iteration 2552, loss = 110.22763816\n",
      "Iteration 2553, loss = 110.22778898\n",
      "Iteration 2554, loss = 110.22501885\n",
      "Iteration 2555, loss = 110.22442654\n",
      "Iteration 2556, loss = 110.22466323\n",
      "Iteration 2557, loss = 110.22315257\n",
      "Iteration 2558, loss = 110.22450172\n",
      "Iteration 2559, loss = 110.22396840\n",
      "Iteration 2560, loss = 110.22050801\n",
      "Iteration 2561, loss = 110.22297768\n",
      "Iteration 2562, loss = 110.22391261\n",
      "Iteration 2563, loss = 110.22189117\n",
      "Iteration 2564, loss = 110.21688253\n",
      "Iteration 2565, loss = 110.22093118\n",
      "Iteration 2566, loss = 110.22380122\n",
      "Iteration 2567, loss = 110.22258206\n",
      "Iteration 2568, loss = 110.21802555\n",
      "Iteration 2569, loss = 110.21227071\n",
      "Iteration 2570, loss = 110.21364130\n",
      "Iteration 2571, loss = 110.21243044\n",
      "Iteration 2572, loss = 110.21020397\n",
      "Iteration 2573, loss = 110.21009746\n",
      "Iteration 2574, loss = 110.20871563\n",
      "Iteration 2575, loss = 110.20806941\n",
      "Iteration 2576, loss = 110.20990934\n",
      "Iteration 2577, loss = 110.20887020\n",
      "Iteration 2578, loss = 110.20509296\n",
      "Iteration 2579, loss = 110.20844411\n",
      "Iteration 2580, loss = 110.20905907\n",
      "Iteration 2581, loss = 110.20684728\n",
      "Iteration 2582, loss = 110.20231459\n",
      "Iteration 2583, loss = 110.20527515\n",
      "Iteration 2584, loss = 110.20705155\n",
      "Iteration 2585, loss = 110.20625107\n",
      "Iteration 2586, loss = 110.20227823\n",
      "Iteration 2587, loss = 110.19823718\n",
      "Iteration 2588, loss = 110.19967026\n",
      "Iteration 2589, loss = 110.19781423\n",
      "Iteration 2590, loss = 110.19581349\n",
      "Iteration 2591, loss = 110.19522545\n",
      "Iteration 2592, loss = 110.19320265\n",
      "Iteration 2593, loss = 110.19184193\n",
      "Iteration 2594, loss = 110.19365903\n",
      "Iteration 2595, loss = 110.19296408\n",
      "Iteration 2596, loss = 110.18944945\n",
      "Iteration 2597, loss = 110.19261794\n",
      "Iteration 2598, loss = 110.19359163\n",
      "Iteration 2599, loss = 110.19135529\n",
      "Iteration 2600, loss = 110.18629855\n",
      "Iteration 2601, loss = 110.19019970\n",
      "Iteration 2602, loss = 110.19242518\n",
      "Iteration 2603, loss = 110.19199486\n",
      "Iteration 2604, loss = 110.18770277\n",
      "Iteration 2605, loss = 110.18139712\n",
      "Iteration 2606, loss = 110.18283371\n",
      "Iteration 2607, loss = 110.18099559\n",
      "Iteration 2608, loss = 110.17966642\n",
      "Iteration 2609, loss = 110.18057086\n",
      "Iteration 2610, loss = 110.17767881\n",
      "Iteration 2611, loss = 110.17561406\n",
      "Iteration 2612, loss = 110.17707852\n",
      "Iteration 2613, loss = 110.17556132\n",
      "Iteration 2614, loss = 110.17621302\n",
      "Iteration 2615, loss = 110.17590908\n",
      "Iteration 2616, loss = 110.17258171\n",
      "Iteration 2617, loss = 110.17589230\n",
      "Iteration 2618, loss = 110.17634402\n",
      "Iteration 2619, loss = 110.17375574\n",
      "Iteration 2620, loss = 110.16836412\n",
      "Iteration 2621, loss = 110.17386621\n",
      "Iteration 2622, loss = 110.17583640\n",
      "Iteration 2623, loss = 110.17486704\n",
      "Iteration 2624, loss = 110.17166725\n",
      "Iteration 2625, loss = 110.16523610\n",
      "Iteration 2626, loss = 110.17066239\n",
      "Iteration 2627, loss = 110.17453347\n",
      "Iteration 2628, loss = 110.17421472\n",
      "Iteration 2629, loss = 110.17083286\n",
      "Iteration 2630, loss = 110.16485326\n",
      "Iteration 2631, loss = 110.16236592\n",
      "Iteration 2632, loss = 110.16549312\n",
      "Iteration 2633, loss = 110.16508179\n",
      "Iteration 2634, loss = 110.16165906\n",
      "Iteration 2635, loss = 110.15573017\n",
      "Iteration 2636, loss = 110.15618530\n",
      "Iteration 2637, loss = 110.15387676\n",
      "Iteration 2638, loss = 110.15681031\n",
      "Iteration 2639, loss = 110.15663387\n",
      "Iteration 2640, loss = 110.15315029\n",
      "Iteration 2641, loss = 110.15239795\n",
      "Iteration 2642, loss = 110.15300930\n",
      "Iteration 2643, loss = 110.15044958\n",
      "Iteration 2644, loss = 110.14972601\n",
      "Iteration 2645, loss = 110.14978655\n",
      "Iteration 2646, loss = 110.14765807\n",
      "Iteration 2647, loss = 110.14757450\n",
      "Iteration 2648, loss = 110.14773772\n",
      "Iteration 2649, loss = 110.14515614\n",
      "Iteration 2650, loss = 110.14463220\n",
      "Iteration 2651, loss = 110.14530267\n",
      "Iteration 2652, loss = 110.14268131\n",
      "Iteration 2653, loss = 110.14224673\n",
      "Iteration 2654, loss = 110.14220799\n",
      "Iteration 2655, loss = 110.13908445\n",
      "Iteration 2656, loss = 110.14061689\n",
      "Iteration 2657, loss = 110.14080773\n",
      "Iteration 2658, loss = 110.13795268\n",
      "Iteration 2659, loss = 110.13586708\n",
      "Iteration 2660, loss = 110.13624693\n",
      "Iteration 2661, loss = 110.13317842\n",
      "Iteration 2662, loss = 110.13627433\n",
      "Iteration 2663, loss = 110.13697526\n",
      "Iteration 2664, loss = 110.13417759\n",
      "Iteration 2665, loss = 110.12923523\n",
      "Iteration 2666, loss = 110.13029787\n",
      "Iteration 2667, loss = 110.12849215\n",
      "Iteration 2668, loss = 110.12701055\n",
      "Iteration 2669, loss = 110.12765334\n",
      "Iteration 2670, loss = 110.12608489\n",
      "Iteration 2671, loss = 110.12793378\n",
      "Iteration 2672, loss = 110.12741614\n",
      "Iteration 2673, loss = 110.12372261\n",
      "Iteration 2674, loss = 110.12619226\n",
      "Iteration 2675, loss = 110.12679892\n",
      "Iteration 2676, loss = 110.12446209\n",
      "Iteration 2677, loss = 110.12014970\n",
      "Iteration 2678, loss = 110.12448458\n",
      "Iteration 2679, loss = 110.12665329\n",
      "Iteration 2680, loss = 110.12529412\n",
      "Iteration 2681, loss = 110.12061007\n",
      "Iteration 2682, loss = 110.11639575\n",
      "Iteration 2683, loss = 110.11804070\n",
      "Iteration 2684, loss = 110.11696682\n",
      "Iteration 2685, loss = 110.11224991\n",
      "Iteration 2686, loss = 110.11700546\n",
      "Iteration 2687, loss = 110.11845610\n",
      "Iteration 2688, loss = 110.11683574\n",
      "Iteration 2689, loss = 110.11220017\n",
      "Iteration 2690, loss = 110.11191235\n",
      "Iteration 2691, loss = 110.11380228\n",
      "Iteration 2692, loss = 110.11259253\n",
      "Iteration 2693, loss = 110.10831491\n",
      "Iteration 2694, loss = 110.10763230\n",
      "Iteration 2695, loss = 110.10906891\n",
      "Iteration 2696, loss = 110.10705967\n",
      "Iteration 2697, loss = 110.10237070\n",
      "Iteration 2698, loss = 110.10742183\n",
      "Iteration 2699, loss = 110.10924050\n",
      "Iteration 2700, loss = 110.10826083\n",
      "Iteration 2701, loss = 110.10399990\n",
      "Iteration 2702, loss = 110.09856236\n",
      "Iteration 2703, loss = 110.09995051\n",
      "Iteration 2704, loss = 110.09793539\n",
      "Iteration 2705, loss = 110.09710053\n",
      "Iteration 2706, loss = 110.09626957\n",
      "Iteration 2707, loss = 110.09378847\n",
      "Iteration 2708, loss = 110.09338813\n",
      "Iteration 2709, loss = 110.09393025\n",
      "Iteration 2710, loss = 110.09209126\n",
      "Iteration 2711, loss = 110.09405871\n",
      "Iteration 2712, loss = 110.09353122\n",
      "Iteration 2713, loss = 110.09025102\n",
      "Iteration 2714, loss = 110.09115697\n",
      "Iteration 2715, loss = 110.09176613\n",
      "Iteration 2716, loss = 110.08921974\n",
      "Iteration 2717, loss = 110.08688247\n",
      "Iteration 2718, loss = 110.08740746\n",
      "Iteration 2719, loss = 110.08439981\n",
      "Iteration 2720, loss = 110.08636998\n",
      "Iteration 2721, loss = 110.08650159\n",
      "Iteration 2722, loss = 110.08354406\n",
      "Iteration 2723, loss = 110.08280713\n",
      "Iteration 2724, loss = 110.08325540\n",
      "Iteration 2725, loss = 110.08131824\n",
      "Iteration 2726, loss = 110.07938129\n",
      "Iteration 2727, loss = 110.07890835\n",
      "Iteration 2728, loss = 110.07615414\n",
      "Iteration 2729, loss = 110.08036469\n",
      "Iteration 2730, loss = 110.08088111\n",
      "Iteration 2731, loss = 110.07856584\n",
      "Iteration 2732, loss = 110.07369887\n",
      "Iteration 2733, loss = 110.07784884\n",
      "Iteration 2734, loss = 110.07990335\n",
      "Iteration 2735, loss = 110.07844200\n",
      "Iteration 2736, loss = 110.07391482\n",
      "Iteration 2737, loss = 110.06945359\n",
      "Iteration 2738, loss = 110.07092448\n",
      "Iteration 2739, loss = 110.06963293\n",
      "Iteration 2740, loss = 110.06568954\n",
      "Iteration 2741, loss = 110.06503722\n",
      "Iteration 2742, loss = 110.06685412\n",
      "Iteration 2743, loss = 110.06631809\n",
      "Iteration 2744, loss = 110.06298873\n",
      "Iteration 2745, loss = 110.06136093\n",
      "Iteration 2746, loss = 110.06294479\n",
      "Iteration 2747, loss = 110.06225387\n",
      "Iteration 2748, loss = 110.05871629\n",
      "Iteration 2749, loss = 110.05830763\n",
      "Iteration 2750, loss = 110.05924028\n",
      "Iteration 2751, loss = 110.05765413\n",
      "Iteration 2752, loss = 110.05743432\n",
      "Iteration 2753, loss = 110.05726240\n",
      "Iteration 2754, loss = 110.05392760\n",
      "Iteration 2755, loss = 110.05336552\n",
      "Iteration 2756, loss = 110.05333900\n",
      "Iteration 2757, loss = 110.05155050\n",
      "Iteration 2758, loss = 110.05341819\n",
      "Iteration 2759, loss = 110.05338358\n",
      "Iteration 2760, loss = 110.04997261\n",
      "Iteration 2761, loss = 110.05076227\n",
      "Iteration 2762, loss = 110.05145914\n",
      "Iteration 2763, loss = 110.04879716\n",
      "Iteration 2764, loss = 110.04693142\n",
      "Iteration 2765, loss = 110.04724492\n",
      "Iteration 2766, loss = 110.04459066\n",
      "Iteration 2767, loss = 110.04539907\n",
      "Iteration 2768, loss = 110.04592484\n",
      "Iteration 2769, loss = 110.04241392\n",
      "Iteration 2770, loss = 110.04352613\n",
      "Iteration 2771, loss = 110.04422918\n",
      "Iteration 2772, loss = 110.04196807\n",
      "Iteration 2773, loss = 110.03816334\n",
      "Iteration 2774, loss = 110.03809944\n",
      "Iteration 2775, loss = 110.03720052\n",
      "Iteration 2776, loss = 110.03561377\n",
      "Iteration 2777, loss = 110.03735514\n",
      "Iteration 2778, loss = 110.03674548\n",
      "Iteration 2779, loss = 110.03254602\n",
      "Iteration 2780, loss = 110.03684902\n",
      "Iteration 2781, loss = 110.03791704\n",
      "Iteration 2782, loss = 110.03588286\n",
      "Iteration 2783, loss = 110.03127449\n",
      "Iteration 2784, loss = 110.03166343\n",
      "Iteration 2785, loss = 110.03367975\n",
      "Iteration 2786, loss = 110.03187970\n",
      "Iteration 2787, loss = 110.02698102\n",
      "Iteration 2788, loss = 110.02916689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2789, loss = 110.03117675\n",
      "Iteration 2790, loss = 110.02995232\n",
      "Iteration 2791, loss = 110.02607536\n",
      "Iteration 2792, loss = 110.02283891\n",
      "Iteration 2793, loss = 110.02431943\n",
      "Iteration 2794, loss = 110.02254631\n",
      "Iteration 2795, loss = 110.01964494\n",
      "Iteration 2796, loss = 110.01991310\n",
      "Iteration 2797, loss = 110.01704855\n",
      "Iteration 2798, loss = 110.01708461\n",
      "Iteration 2799, loss = 110.01654990\n",
      "Iteration 2800, loss = 110.01507643\n",
      "Iteration 2801, loss = 110.01475770\n",
      "Iteration 2802, loss = 110.01314686\n",
      "Iteration 2803, loss = 110.01307460\n",
      "Iteration 2804, loss = 110.01188624\n",
      "Iteration 2805, loss = 110.01041869\n",
      "Iteration 2806, loss = 110.01071792\n",
      "Iteration 2807, loss = 110.00876198\n",
      "Iteration 2808, loss = 110.01145147\n",
      "Iteration 2809, loss = 110.01088238\n",
      "Iteration 2810, loss = 110.00692435\n",
      "Iteration 2811, loss = 110.00952571\n",
      "Iteration 2812, loss = 110.01085486\n",
      "Iteration 2813, loss = 110.00853008\n",
      "Iteration 2814, loss = 110.00457597\n",
      "Iteration 2815, loss = 110.00777416\n",
      "Iteration 2816, loss = 110.00947870\n",
      "Iteration 2817, loss = 110.00784584\n",
      "Iteration 2818, loss = 110.00283908\n",
      "Iteration 2819, loss = 110.00283651\n",
      "Iteration 2820, loss = 110.00474884\n",
      "Iteration 2821, loss = 110.00314827\n",
      "Iteration 2822, loss = 109.99894994\n",
      "Iteration 2823, loss = 109.99752887\n",
      "Iteration 2824, loss = 109.99940427\n",
      "Iteration 2825, loss = 109.99832169\n",
      "Iteration 2826, loss = 109.99361130\n",
      "Iteration 2827, loss = 109.99293844\n",
      "Iteration 2828, loss = 109.99118042\n",
      "Iteration 2829, loss = 109.99074991\n",
      "Iteration 2830, loss = 109.99031935\n",
      "Iteration 2831, loss = 109.98875376\n",
      "Iteration 2832, loss = 109.98843976\n",
      "Iteration 2833, loss = 109.98709333\n",
      "Iteration 2834, loss = 109.98675496\n",
      "Iteration 2835, loss = 109.98636001\n",
      "Iteration 2836, loss = 109.98411460\n",
      "Iteration 2837, loss = 109.98572576\n",
      "Iteration 2838, loss = 109.98390994\n",
      "Iteration 2839, loss = 109.98508195\n",
      "Iteration 2840, loss = 109.98467389\n",
      "Iteration 2841, loss = 109.98077888\n",
      "Iteration 2842, loss = 109.98454410\n",
      "Iteration 2843, loss = 109.98558494\n",
      "Iteration 2844, loss = 109.98321088\n",
      "Iteration 2845, loss = 109.97833578\n",
      "Iteration 2846, loss = 109.97991027\n",
      "Iteration 2847, loss = 109.98172071\n",
      "Iteration 2848, loss = 109.98065538\n",
      "Iteration 2849, loss = 109.97577864\n",
      "Iteration 2850, loss = 109.97664796\n",
      "Iteration 2851, loss = 109.97895575\n",
      "Iteration 2852, loss = 109.97793789\n",
      "Iteration 2853, loss = 109.97422655\n",
      "Iteration 2854, loss = 109.97085545\n",
      "Iteration 2855, loss = 109.97208094\n",
      "Iteration 2856, loss = 109.96978552\n",
      "Iteration 2857, loss = 109.96792902\n",
      "Iteration 2858, loss = 109.96844332\n",
      "Iteration 2859, loss = 109.96600706\n",
      "Iteration 2860, loss = 109.96803019\n",
      "Iteration 2861, loss = 109.96802980\n",
      "Iteration 2862, loss = 109.96490000\n",
      "Iteration 2863, loss = 109.96358874\n",
      "Iteration 2864, loss = 109.96424148\n",
      "Iteration 2865, loss = 109.96229452\n",
      "Iteration 2866, loss = 109.96006377\n",
      "Iteration 2867, loss = 109.95972743\n",
      "Iteration 2868, loss = 109.95730243\n",
      "Iteration 2869, loss = 109.95619249\n",
      "Iteration 2870, loss = 109.95895674\n",
      "Iteration 2871, loss = 109.95768053\n",
      "Iteration 2872, loss = 109.95385034\n",
      "Iteration 2873, loss = 109.95335743\n",
      "Iteration 2874, loss = 109.95481225\n",
      "Iteration 2875, loss = 109.95356502\n",
      "Iteration 2876, loss = 109.95219641\n",
      "Iteration 2877, loss = 109.95236264\n",
      "Iteration 2878, loss = 109.94909386\n",
      "Iteration 2879, loss = 109.94852674\n",
      "Iteration 2880, loss = 109.94839505\n",
      "Iteration 2881, loss = 109.94592911\n",
      "Iteration 2882, loss = 109.94928176\n",
      "Iteration 2883, loss = 109.94872336\n",
      "Iteration 2884, loss = 109.94598238\n",
      "Iteration 2885, loss = 109.94451400\n",
      "Iteration 2886, loss = 109.94508043\n",
      "Iteration 2887, loss = 109.94232589\n",
      "Iteration 2888, loss = 109.94353332\n",
      "Iteration 2889, loss = 109.94490004\n",
      "Iteration 2890, loss = 109.94225116\n",
      "Iteration 2891, loss = 109.93783095\n",
      "Iteration 2892, loss = 109.93767467\n",
      "Iteration 2893, loss = 109.93739657\n",
      "Iteration 2894, loss = 109.93649809\n",
      "Iteration 2895, loss = 109.93705363\n",
      "Iteration 2896, loss = 109.93653482\n",
      "Iteration 2897, loss = 109.93332558\n",
      "Iteration 2898, loss = 109.93295220\n",
      "Iteration 2899, loss = 109.93342862\n",
      "Iteration 2900, loss = 109.93207392\n",
      "Iteration 2901, loss = 109.93195682\n",
      "Iteration 2902, loss = 109.93148608\n",
      "Iteration 2903, loss = 109.92801816\n",
      "Iteration 2904, loss = 109.93175813\n",
      "Iteration 2905, loss = 109.93226744\n",
      "Iteration 2906, loss = 109.92962578\n",
      "Iteration 2907, loss = 109.92573600\n",
      "Iteration 2908, loss = 109.92577741\n",
      "Iteration 2909, loss = 109.92340516\n",
      "Iteration 2910, loss = 109.92509650\n",
      "Iteration 2911, loss = 109.92487071\n",
      "Iteration 2912, loss = 109.92138545\n",
      "Iteration 2913, loss = 109.92335285\n",
      "Iteration 2914, loss = 109.92446361\n",
      "Iteration 2915, loss = 109.92239867\n",
      "Iteration 2916, loss = 109.91756399\n",
      "Iteration 2917, loss = 109.92053464\n",
      "Iteration 2918, loss = 109.92319042\n",
      "Iteration 2919, loss = 109.92156334\n",
      "Iteration 2920, loss = 109.91596274\n",
      "Iteration 2921, loss = 109.91641193\n",
      "Iteration 2922, loss = 109.91879361\n",
      "Iteration 2923, loss = 109.91797357\n",
      "Iteration 2924, loss = 109.91403790\n",
      "Iteration 2925, loss = 109.91107498\n",
      "Iteration 2926, loss = 109.91168298\n",
      "Iteration 2927, loss = 109.90944775\n",
      "Iteration 2928, loss = 109.90911531\n",
      "Iteration 2929, loss = 109.90929867\n",
      "Iteration 2930, loss = 109.90664334\n",
      "Iteration 2931, loss = 109.90693552\n",
      "Iteration 2932, loss = 109.90779049\n",
      "Iteration 2933, loss = 109.90426653\n",
      "Iteration 2934, loss = 109.90550372\n",
      "Iteration 2935, loss = 109.90636597\n",
      "Iteration 2936, loss = 109.90429105\n",
      "Iteration 2937, loss = 109.89946632\n",
      "Iteration 2938, loss = 109.90444606\n",
      "Iteration 2939, loss = 109.90630702\n",
      "Iteration 2940, loss = 109.90497463\n",
      "Iteration 2941, loss = 109.89954524\n",
      "Iteration 2942, loss = 109.89779428\n",
      "Iteration 2943, loss = 109.90004457\n",
      "Iteration 2944, loss = 109.89952990\n",
      "Iteration 2945, loss = 109.89586963\n",
      "Iteration 2946, loss = 109.89370785\n",
      "Iteration 2947, loss = 109.89491910\n",
      "Iteration 2948, loss = 109.89235398\n",
      "Iteration 2949, loss = 109.89198675\n",
      "Iteration 2950, loss = 109.89223424\n",
      "Iteration 2951, loss = 109.88992522\n",
      "Iteration 2952, loss = 109.88913399\n",
      "Iteration 2953, loss = 109.88905531\n",
      "Iteration 2954, loss = 109.88538554\n",
      "Iteration 2955, loss = 109.88815477\n",
      "Iteration 2956, loss = 109.88913593\n",
      "Iteration 2957, loss = 109.88774640\n",
      "Iteration 2958, loss = 109.88296884\n",
      "Iteration 2959, loss = 109.88552238\n",
      "Iteration 2960, loss = 109.88758867\n",
      "Iteration 2961, loss = 109.88572735\n",
      "Iteration 2962, loss = 109.88068171\n",
      "Iteration 2963, loss = 109.88136489\n",
      "Iteration 2964, loss = 109.88421486\n",
      "Iteration 2965, loss = 109.88302690\n",
      "Iteration 2966, loss = 109.87928519\n",
      "Iteration 2967, loss = 109.87466077\n",
      "Iteration 2968, loss = 109.87563275\n",
      "Iteration 2969, loss = 109.87374718\n",
      "Iteration 2970, loss = 109.87575028\n",
      "Iteration 2971, loss = 109.87566837\n",
      "Iteration 2972, loss = 109.87298214\n",
      "Iteration 2973, loss = 109.87063183\n",
      "Iteration 2974, loss = 109.87072393\n",
      "Iteration 2975, loss = 109.86806434\n",
      "Iteration 2976, loss = 109.86705058\n",
      "Iteration 2977, loss = 109.86831715\n",
      "Iteration 2978, loss = 109.86684014\n",
      "Iteration 2979, loss = 109.86546673\n",
      "Iteration 2980, loss = 109.86414821\n",
      "Iteration 2981, loss = 109.86423466\n",
      "Iteration 2982, loss = 109.86363108\n",
      "Iteration 2983, loss = 109.86022590\n",
      "Iteration 2984, loss = 109.86113307\n",
      "Iteration 2985, loss = 109.86082066\n",
      "Iteration 2986, loss = 109.85829196\n",
      "Iteration 2987, loss = 109.86133908\n",
      "Iteration 2988, loss = 109.86114855\n",
      "Iteration 2989, loss = 109.85887554\n",
      "Iteration 2990, loss = 109.85626485\n",
      "Iteration 2991, loss = 109.85666314\n",
      "Iteration 2992, loss = 109.85326220\n",
      "Iteration 2993, loss = 109.85690794\n",
      "Iteration 2994, loss = 109.85789521\n",
      "Iteration 2995, loss = 109.85562884\n",
      "Iteration 2996, loss = 109.85114748\n",
      "Iteration 2997, loss = 109.85418904\n",
      "Iteration 2998, loss = 109.85606313\n",
      "Iteration 2999, loss = 109.85384400\n",
      "Iteration 3000, loss = 109.84842475\n",
      "Iteration 3001, loss = 109.85060846\n",
      "Iteration 3002, loss = 109.85250989\n",
      "Iteration 3003, loss = 109.85234769\n",
      "Iteration 3004, loss = 109.84826904\n",
      "Iteration 3005, loss = 109.84221741\n",
      "Iteration 3006, loss = 109.84982215\n",
      "Iteration 3007, loss = 109.85301743\n",
      "Iteration 3008, loss = 109.85184036\n",
      "Iteration 3009, loss = 109.84743612\n",
      "Iteration 3010, loss = 109.83958032\n",
      "Iteration 3011, loss = 109.84478504\n",
      "Iteration 3012, loss = 109.84870590\n",
      "Iteration 3013, loss = 109.84918110\n",
      "Iteration 3014, loss = 109.84681647\n",
      "Iteration 3015, loss = 109.84178215\n",
      "Iteration 3016, loss = 109.83402890\n",
      "Iteration 3017, loss = 109.84122270\n",
      "Iteration 3018, loss = 109.84569809\n",
      "Iteration 3019, loss = 109.84592025\n",
      "Iteration 3020, loss = 109.84226385\n",
      "Iteration 3021, loss = 109.83549482\n",
      "Iteration 3022, loss = 109.83038689\n",
      "Iteration 3023, loss = 109.83395445\n",
      "Iteration 3024, loss = 109.83485061\n",
      "Iteration 3025, loss = 109.83216315\n",
      "Iteration 3026, loss = 109.82679942\n",
      "Iteration 3027, loss = 109.82938462\n",
      "Iteration 3028, loss = 109.83185290\n",
      "Iteration 3029, loss = 109.83048787\n",
      "Iteration 3030, loss = 109.82539742\n",
      "Iteration 3031, loss = 109.82432396\n",
      "Iteration 3032, loss = 109.82614502\n",
      "Iteration 3033, loss = 109.82527260\n",
      "Iteration 3034, loss = 109.82137500\n",
      "Iteration 3035, loss = 109.82011336\n",
      "Iteration 3036, loss = 109.82107565\n",
      "Iteration 3037, loss = 109.81825556\n",
      "Iteration 3038, loss = 109.81689932\n",
      "Iteration 3039, loss = 109.81820946\n",
      "Iteration 3040, loss = 109.81539126\n",
      "Iteration 3041, loss = 109.81473304\n",
      "Iteration 3042, loss = 109.81487564\n",
      "Iteration 3043, loss = 109.81129585\n",
      "Iteration 3044, loss = 109.81554452\n",
      "Iteration 3045, loss = 109.81689313\n",
      "Iteration 3046, loss = 109.81560858\n",
      "Iteration 3047, loss = 109.81069042\n",
      "Iteration 3048, loss = 109.81092395\n",
      "Iteration 3049, loss = 109.81272385\n",
      "Iteration 3050, loss = 109.81076499\n",
      "Iteration 3051, loss = 109.80493421\n",
      "Iteration 3052, loss = 109.81033096\n",
      "Iteration 3053, loss = 109.81280318\n",
      "Iteration 3054, loss = 109.81227671\n",
      "Iteration 3055, loss = 109.80848569\n",
      "Iteration 3056, loss = 109.80244443\n",
      "Iteration 3057, loss = 109.80568617\n",
      "Iteration 3058, loss = 109.80876040\n",
      "Iteration 3059, loss = 109.80784989\n",
      "Iteration 3060, loss = 109.80319095\n",
      "Iteration 3061, loss = 109.79717761\n",
      "Iteration 3062, loss = 109.79923512\n",
      "Iteration 3063, loss = 109.79787668\n",
      "Iteration 3064, loss = 109.79420410\n",
      "Iteration 3065, loss = 109.79825024\n",
      "Iteration 3066, loss = 109.79932780\n",
      "Iteration 3067, loss = 109.79698408\n",
      "Iteration 3068, loss = 109.79063853\n",
      "Iteration 3069, loss = 109.79110116\n",
      "Iteration 3070, loss = 109.78908921\n",
      "Iteration 3071, loss = 109.79196502\n",
      "Iteration 3072, loss = 109.79207564\n",
      "Iteration 3073, loss = 109.78759444\n",
      "Iteration 3074, loss = 109.79009964\n",
      "Iteration 3075, loss = 109.79159086\n",
      "Iteration 3076, loss = 109.79009362\n",
      "Iteration 3077, loss = 109.78560515\n",
      "Iteration 3078, loss = 109.78565907\n",
      "Iteration 3079, loss = 109.78718502\n",
      "Iteration 3080, loss = 109.78490072\n",
      "Iteration 3081, loss = 109.77965551\n",
      "Iteration 3082, loss = 109.78026984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3083, loss = 109.77863214\n",
      "Iteration 3084, loss = 109.77866660\n",
      "Iteration 3085, loss = 109.77626488\n",
      "Iteration 3086, loss = 109.77728390\n",
      "Iteration 3087, loss = 109.77555256\n",
      "Iteration 3088, loss = 109.77715717\n",
      "Iteration 3089, loss = 109.77614511\n",
      "Iteration 3090, loss = 109.77260473\n",
      "Iteration 3091, loss = 109.77178809\n",
      "Iteration 3092, loss = 109.77325362\n",
      "Iteration 3093, loss = 109.77166207\n",
      "Iteration 3094, loss = 109.77113156\n",
      "Iteration 3095, loss = 109.77081046\n",
      "Iteration 3096, loss = 109.76773997\n",
      "Iteration 3097, loss = 109.77053282\n",
      "Iteration 3098, loss = 109.77168560\n",
      "Iteration 3099, loss = 109.76795278\n",
      "Iteration 3100, loss = 109.76668203\n",
      "Iteration 3101, loss = 109.76785915\n",
      "Iteration 3102, loss = 109.76629383\n",
      "Iteration 3103, loss = 109.76166260\n",
      "Iteration 3104, loss = 109.76757481\n",
      "Iteration 3105, loss = 109.76930789\n",
      "Iteration 3106, loss = 109.76702283\n",
      "Iteration 3107, loss = 109.76113403\n",
      "Iteration 3108, loss = 109.76206223\n",
      "Iteration 3109, loss = 109.76484164\n",
      "Iteration 3110, loss = 109.76436895\n",
      "Iteration 3111, loss = 109.76094890\n",
      "Iteration 3112, loss = 109.75553371\n",
      "Iteration 3113, loss = 109.76017071\n",
      "Iteration 3114, loss = 109.76304823\n",
      "Iteration 3115, loss = 109.76193779\n",
      "Iteration 3116, loss = 109.75701488\n",
      "Iteration 3117, loss = 109.75127191\n",
      "Iteration 3118, loss = 109.75372571\n",
      "Iteration 3119, loss = 109.75278587\n",
      "Iteration 3120, loss = 109.74907824\n",
      "Iteration 3121, loss = 109.75181033\n",
      "Iteration 3122, loss = 109.75233916\n",
      "Iteration 3123, loss = 109.74947932\n",
      "Iteration 3124, loss = 109.74609012\n",
      "Iteration 3125, loss = 109.74718961\n",
      "Iteration 3126, loss = 109.74487944\n",
      "Iteration 3127, loss = 109.74448258\n",
      "Iteration 3128, loss = 109.74436063\n",
      "Iteration 3129, loss = 109.74149574\n",
      "Iteration 3130, loss = 109.74038644\n",
      "Iteration 3131, loss = 109.74157792\n",
      "Iteration 3132, loss = 109.74005831\n",
      "Iteration 3133, loss = 109.73960085\n",
      "Iteration 3134, loss = 109.73944348\n",
      "Iteration 3135, loss = 109.73658253\n",
      "Iteration 3136, loss = 109.73946239\n",
      "Iteration 3137, loss = 109.73970612\n",
      "Iteration 3138, loss = 109.73606306\n",
      "Iteration 3139, loss = 109.73577339\n",
      "Iteration 3140, loss = 109.73665882\n",
      "Iteration 3141, loss = 109.73437785\n",
      "Iteration 3142, loss = 109.73004134\n",
      "Iteration 3143, loss = 109.73557736\n",
      "Iteration 3144, loss = 109.73736632\n",
      "Iteration 3145, loss = 109.73536353\n",
      "Iteration 3146, loss = 109.72920292\n",
      "Iteration 3147, loss = 109.73091487\n",
      "Iteration 3148, loss = 109.73387858\n",
      "Iteration 3149, loss = 109.73358331\n",
      "Iteration 3150, loss = 109.73033033\n",
      "Iteration 3151, loss = 109.72490995\n",
      "Iteration 3152, loss = 109.72784792\n",
      "Iteration 3153, loss = 109.73076215\n",
      "Iteration 3154, loss = 109.72974991\n",
      "Iteration 3155, loss = 109.72439442\n",
      "Iteration 3156, loss = 109.72073469\n",
      "Iteration 3157, loss = 109.72294550\n",
      "Iteration 3158, loss = 109.72254071\n",
      "Iteration 3159, loss = 109.71855769\n",
      "Iteration 3160, loss = 109.71858322\n",
      "Iteration 3161, loss = 109.71951150\n",
      "Iteration 3162, loss = 109.71623605\n",
      "Iteration 3163, loss = 109.71571168\n",
      "Iteration 3164, loss = 109.71668165\n",
      "Iteration 3165, loss = 109.71492306\n",
      "Iteration 3166, loss = 109.71111149\n",
      "Iteration 3167, loss = 109.71075005\n",
      "Iteration 3168, loss = 109.71126587\n",
      "Iteration 3169, loss = 109.71042551\n",
      "Iteration 3170, loss = 109.70792720\n",
      "Iteration 3171, loss = 109.70722986\n",
      "Iteration 3172, loss = 109.70801208\n",
      "Iteration 3173, loss = 109.70522846\n",
      "Iteration 3174, loss = 109.70808326\n",
      "Iteration 3175, loss = 109.70840342\n",
      "Iteration 3176, loss = 109.70580263\n",
      "Iteration 3177, loss = 109.70225515\n",
      "Iteration 3178, loss = 109.70277077\n",
      "Iteration 3179, loss = 109.70198363\n",
      "Iteration 3180, loss = 109.70047692\n",
      "Iteration 3181, loss = 109.70028549\n",
      "Iteration 3182, loss = 109.69883474\n",
      "Iteration 3183, loss = 109.69946200\n",
      "Iteration 3184, loss = 109.69927905\n",
      "Iteration 3185, loss = 109.69679305\n",
      "Iteration 3186, loss = 109.69814018\n",
      "Iteration 3187, loss = 109.69845099\n",
      "Iteration 3188, loss = 109.69465832\n",
      "Iteration 3189, loss = 109.69527355\n",
      "Iteration 3190, loss = 109.69648229\n",
      "Iteration 3191, loss = 109.69495613\n",
      "Iteration 3192, loss = 109.69055854\n",
      "Iteration 3193, loss = 109.69330339\n",
      "Iteration 3194, loss = 109.69511932\n",
      "Iteration 3195, loss = 109.69241658\n",
      "Iteration 3196, loss = 109.68626341\n",
      "Iteration 3197, loss = 109.69244848\n",
      "Iteration 3198, loss = 109.69546630\n",
      "Iteration 3199, loss = 109.69526593\n",
      "Iteration 3200, loss = 109.69209647\n",
      "Iteration 3201, loss = 109.68646660\n",
      "Iteration 3202, loss = 109.68428198\n",
      "Iteration 3203, loss = 109.68721138\n",
      "Iteration 3204, loss = 109.68638991\n",
      "Iteration 3205, loss = 109.68089102\n",
      "Iteration 3206, loss = 109.68316409\n",
      "Iteration 3207, loss = 109.68547001\n",
      "Iteration 3208, loss = 109.68472530\n",
      "Iteration 3209, loss = 109.68160389\n",
      "Iteration 3210, loss = 109.67532344\n",
      "Iteration 3211, loss = 109.68285552\n",
      "Iteration 3212, loss = 109.68566809\n",
      "Iteration 3213, loss = 109.68430680\n",
      "Iteration 3214, loss = 109.67909286\n",
      "Iteration 3215, loss = 109.67235517\n",
      "Iteration 3216, loss = 109.67491238\n",
      "Iteration 3217, loss = 109.67430464\n",
      "Iteration 3218, loss = 109.67103388\n",
      "Iteration 3219, loss = 109.67140321\n",
      "Iteration 3220, loss = 109.67197647\n",
      "Iteration 3221, loss = 109.66843997\n",
      "Iteration 3222, loss = 109.66911990\n",
      "Iteration 3223, loss = 109.67020309\n",
      "Iteration 3224, loss = 109.66835537\n",
      "Iteration 3225, loss = 109.66395646\n",
      "Iteration 3226, loss = 109.66770153\n",
      "Iteration 3227, loss = 109.66915758\n",
      "Iteration 3228, loss = 109.66676632\n",
      "Iteration 3229, loss = 109.66042803\n",
      "Iteration 3230, loss = 109.66585168\n",
      "Iteration 3231, loss = 109.66870181\n",
      "Iteration 3232, loss = 109.66822916\n",
      "Iteration 3233, loss = 109.66519156\n",
      "Iteration 3234, loss = 109.65956694\n",
      "Iteration 3235, loss = 109.65900413\n",
      "Iteration 3236, loss = 109.66152853\n",
      "Iteration 3237, loss = 109.66029476\n",
      "Iteration 3238, loss = 109.65488836\n",
      "Iteration 3239, loss = 109.65683898\n",
      "Iteration 3240, loss = 109.65874572\n",
      "Iteration 3241, loss = 109.65811018\n",
      "Iteration 3242, loss = 109.65467340\n",
      "Iteration 3243, loss = 109.64878457\n",
      "Iteration 3244, loss = 109.65607383\n",
      "Iteration 3245, loss = 109.65870986\n",
      "Iteration 3246, loss = 109.65741227\n",
      "Iteration 3247, loss = 109.65180567\n",
      "Iteration 3248, loss = 109.64614807\n",
      "Iteration 3249, loss = 109.64832245\n",
      "Iteration 3250, loss = 109.64771798\n",
      "Iteration 3251, loss = 109.64472832\n",
      "Iteration 3252, loss = 109.64424793\n",
      "Iteration 3253, loss = 109.64480679\n",
      "Iteration 3254, loss = 109.64116635\n",
      "Iteration 3255, loss = 109.64324061\n",
      "Iteration 3256, loss = 109.64490466\n",
      "Iteration 3257, loss = 109.64312643\n",
      "Iteration 3258, loss = 109.63863526\n",
      "Iteration 3259, loss = 109.64067353\n",
      "Iteration 3260, loss = 109.64216386\n",
      "Iteration 3261, loss = 109.63937454\n",
      "Iteration 3262, loss = 109.63372784\n",
      "Iteration 3263, loss = 109.63467585\n",
      "Iteration 3264, loss = 109.63221851\n",
      "Iteration 3265, loss = 109.63481766\n",
      "Iteration 3266, loss = 109.63374157\n",
      "Iteration 3267, loss = 109.62943059\n",
      "Iteration 3268, loss = 109.63437501\n",
      "Iteration 3269, loss = 109.63632924\n",
      "Iteration 3270, loss = 109.63516231\n",
      "Iteration 3271, loss = 109.63123553\n",
      "Iteration 3272, loss = 109.62570094\n",
      "Iteration 3273, loss = 109.62690358\n",
      "Iteration 3274, loss = 109.62382135\n",
      "Iteration 3275, loss = 109.62773088\n",
      "Iteration 3276, loss = 109.62833130\n",
      "Iteration 3277, loss = 109.62627103\n",
      "Iteration 3278, loss = 109.62184253\n",
      "Iteration 3279, loss = 109.62584440\n",
      "Iteration 3280, loss = 109.62761882\n",
      "Iteration 3281, loss = 109.62500000\n",
      "Iteration 3282, loss = 109.61867001\n",
      "Iteration 3283, loss = 109.62275354\n",
      "Iteration 3284, loss = 109.62600888\n",
      "Iteration 3285, loss = 109.62586896\n",
      "Iteration 3286, loss = 109.62325181\n",
      "Iteration 3287, loss = 109.61806305\n",
      "Iteration 3288, loss = 109.61630444\n",
      "Iteration 3289, loss = 109.61919742\n",
      "Iteration 3290, loss = 109.61724401\n",
      "Iteration 3291, loss = 109.61155352\n",
      "Iteration 3292, loss = 109.61597177\n",
      "Iteration 3293, loss = 109.61836224\n",
      "Iteration 3294, loss = 109.61782744\n",
      "Iteration 3295, loss = 109.61519253\n",
      "Iteration 3296, loss = 109.60927497\n",
      "Iteration 3297, loss = 109.61074062\n",
      "Iteration 3298, loss = 109.61353357\n",
      "Iteration 3299, loss = 109.61160005\n",
      "Iteration 3300, loss = 109.60599938\n",
      "Iteration 3301, loss = 109.60737787\n",
      "Iteration 3302, loss = 109.60971294\n",
      "Iteration 3303, loss = 109.60960469\n",
      "Iteration 3304, loss = 109.60666542\n",
      "Iteration 3305, loss = 109.60099707\n",
      "Iteration 3306, loss = 109.60530483\n",
      "Iteration 3307, loss = 109.60805793\n",
      "Iteration 3308, loss = 109.60616064\n",
      "Iteration 3309, loss = 109.60062288\n",
      "Iteration 3310, loss = 109.59959433\n",
      "Iteration 3311, loss = 109.60215851\n",
      "Iteration 3312, loss = 109.60205838\n",
      "Iteration 3313, loss = 109.59893340\n",
      "Iteration 3314, loss = 109.59347995\n",
      "Iteration 3315, loss = 109.59844467\n",
      "Iteration 3316, loss = 109.60106532\n",
      "Iteration 3317, loss = 109.59913199\n",
      "Iteration 3318, loss = 109.59361889\n",
      "Iteration 3319, loss = 109.59201146\n",
      "Iteration 3320, loss = 109.59476405\n",
      "Iteration 3321, loss = 109.59490574\n",
      "Iteration 3322, loss = 109.59182336\n",
      "Iteration 3323, loss = 109.58647558\n",
      "Iteration 3324, loss = 109.59180338\n",
      "Iteration 3325, loss = 109.59472374\n",
      "Iteration 3326, loss = 109.59226677\n",
      "Iteration 3327, loss = 109.58694067\n",
      "Iteration 3328, loss = 109.58516087\n",
      "Iteration 3329, loss = 109.58751948\n",
      "Iteration 3330, loss = 109.58725502\n",
      "Iteration 3331, loss = 109.58439825\n",
      "Iteration 3332, loss = 109.57908192\n",
      "Iteration 3333, loss = 109.58557234\n",
      "Iteration 3334, loss = 109.58811241\n",
      "Iteration 3335, loss = 109.58607794\n",
      "Iteration 3336, loss = 109.58015616\n",
      "Iteration 3337, loss = 109.57755215\n",
      "Iteration 3338, loss = 109.58007817\n",
      "Iteration 3339, loss = 109.58040013\n",
      "Iteration 3340, loss = 109.57750020\n",
      "Iteration 3341, loss = 109.57242658\n",
      "Iteration 3342, loss = 109.57753377\n",
      "Iteration 3343, loss = 109.58011383\n",
      "Iteration 3344, loss = 109.57806939\n",
      "Iteration 3345, loss = 109.57217088\n",
      "Iteration 3346, loss = 109.57108481\n",
      "Iteration 3347, loss = 109.57381547\n",
      "Iteration 3348, loss = 109.57441118\n",
      "Iteration 3349, loss = 109.57098852\n",
      "Iteration 3350, loss = 109.56556136\n",
      "Iteration 3351, loss = 109.57035072\n",
      "Iteration 3352, loss = 109.57314297\n",
      "Iteration 3353, loss = 109.57132050\n",
      "Iteration 3354, loss = 109.56584044\n",
      "Iteration 3355, loss = 109.56464959\n",
      "Iteration 3356, loss = 109.56747634\n",
      "Iteration 3357, loss = 109.56741127\n",
      "Iteration 3358, loss = 109.56442633\n",
      "Iteration 3359, loss = 109.55868525\n",
      "Iteration 3360, loss = 109.56366078\n",
      "Iteration 3361, loss = 109.56681664\n",
      "Iteration 3362, loss = 109.56402405\n",
      "Iteration 3363, loss = 109.55835353\n",
      "Iteration 3364, loss = 109.55764044\n",
      "Iteration 3365, loss = 109.56044571\n",
      "Iteration 3366, loss = 109.56040719\n",
      "Iteration 3367, loss = 109.55762684\n",
      "Iteration 3368, loss = 109.55245526\n",
      "Iteration 3369, loss = 109.55601176\n",
      "Iteration 3370, loss = 109.55837500\n",
      "Iteration 3371, loss = 109.55610175\n",
      "Iteration 3372, loss = 109.55035404\n",
      "Iteration 3373, loss = 109.55251416\n",
      "Iteration 3374, loss = 109.55464676\n",
      "Iteration 3375, loss = 109.55449594\n",
      "Iteration 3376, loss = 109.55186333\n",
      "Iteration 3377, loss = 109.54670800\n",
      "Iteration 3378, loss = 109.54776099\n",
      "Iteration 3379, loss = 109.55027290\n",
      "Iteration 3380, loss = 109.54846166\n",
      "Iteration 3381, loss = 109.54269362\n",
      "Iteration 3382, loss = 109.54601059\n",
      "Iteration 3383, loss = 109.54858461\n",
      "Iteration 3384, loss = 109.54848450\n",
      "Iteration 3385, loss = 109.54578797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3386, loss = 109.54026372\n",
      "Iteration 3387, loss = 109.53964917\n",
      "Iteration 3388, loss = 109.54248510\n",
      "Iteration 3389, loss = 109.53984829\n",
      "Iteration 3390, loss = 109.53432683\n",
      "Iteration 3391, loss = 109.53976528\n",
      "Iteration 3392, loss = 109.54211872\n",
      "Iteration 3393, loss = 109.54260233\n",
      "Iteration 3394, loss = 109.53959225\n",
      "Iteration 3395, loss = 109.53432000\n",
      "Iteration 3396, loss = 109.53156426\n",
      "Iteration 3397, loss = 109.53419934\n",
      "Iteration 3398, loss = 109.53190866\n",
      "Iteration 3399, loss = 109.52732832\n",
      "Iteration 3400, loss = 109.52797533\n",
      "Iteration 3401, loss = 109.52561703\n",
      "Iteration 3402, loss = 109.52838016\n",
      "Iteration 3403, loss = 109.52753793\n",
      "Iteration 3404, loss = 109.52285868\n",
      "Iteration 3405, loss = 109.52298151\n",
      "Iteration 3406, loss = 109.52320770\n",
      "Iteration 3407, loss = 109.52096859\n",
      "Iteration 3408, loss = 109.52421900\n",
      "Iteration 3409, loss = 109.52506740\n",
      "Iteration 3410, loss = 109.52315606\n",
      "Iteration 3411, loss = 109.51865462\n",
      "Iteration 3412, loss = 109.52282309\n",
      "Iteration 3413, loss = 109.52444394\n",
      "Iteration 3414, loss = 109.52196163\n",
      "Iteration 3415, loss = 109.51456125\n",
      "Iteration 3416, loss = 109.52096720\n",
      "Iteration 3417, loss = 109.52414697\n",
      "Iteration 3418, loss = 109.52498857\n",
      "Iteration 3419, loss = 109.52211115\n",
      "Iteration 3420, loss = 109.51698420\n",
      "Iteration 3421, loss = 109.51014342\n",
      "Iteration 3422, loss = 109.51975346\n",
      "Iteration 3423, loss = 109.52426060\n",
      "Iteration 3424, loss = 109.52376118\n",
      "Iteration 3425, loss = 109.51854482\n",
      "Iteration 3426, loss = 109.50950216\n",
      "Iteration 3427, loss = 109.51136940\n",
      "Iteration 3428, loss = 109.51663439\n",
      "Iteration 3429, loss = 109.51776533\n",
      "Iteration 3430, loss = 109.51647863\n",
      "Iteration 3431, loss = 109.51221562\n",
      "Iteration 3432, loss = 109.50581461\n",
      "Iteration 3433, loss = 109.50514121\n",
      "Iteration 3434, loss = 109.50874003\n",
      "Iteration 3435, loss = 109.50791623\n",
      "Iteration 3436, loss = 109.50271663\n",
      "Iteration 3437, loss = 109.50113853\n",
      "Iteration 3438, loss = 109.50362491\n",
      "Iteration 3439, loss = 109.50333983\n",
      "Iteration 3440, loss = 109.50033291\n",
      "Iteration 3441, loss = 109.49506290\n",
      "Iteration 3442, loss = 109.50207512\n",
      "Iteration 3443, loss = 109.50487536\n",
      "Iteration 3444, loss = 109.50224519\n",
      "Iteration 3445, loss = 109.49599689\n",
      "Iteration 3446, loss = 109.49449870\n",
      "Iteration 3447, loss = 109.49792062\n",
      "Iteration 3448, loss = 109.49805024\n",
      "Iteration 3449, loss = 109.49569844\n",
      "Iteration 3450, loss = 109.49093036\n",
      "Iteration 3451, loss = 109.49153620\n",
      "Iteration 3452, loss = 109.49380730\n",
      "Iteration 3453, loss = 109.49129831\n",
      "Iteration 3454, loss = 109.48547915\n",
      "Iteration 3455, loss = 109.48623974\n",
      "Iteration 3456, loss = 109.48434704\n",
      "Iteration 3457, loss = 109.48606732\n",
      "Iteration 3458, loss = 109.48522084\n",
      "Iteration 3459, loss = 109.48196179\n",
      "Iteration 3460, loss = 109.48201515\n",
      "Iteration 3461, loss = 109.48043310\n",
      "Iteration 3462, loss = 109.48031826\n",
      "Iteration 3463, loss = 109.47844267\n",
      "Iteration 3464, loss = 109.48187244\n",
      "Iteration 3465, loss = 109.48148995\n",
      "Iteration 3466, loss = 109.47661452\n",
      "Iteration 3467, loss = 109.47642657\n",
      "Iteration 3468, loss = 109.47615234\n",
      "Iteration 3469, loss = 109.47511168\n",
      "Iteration 3470, loss = 109.47415531\n",
      "Iteration 3471, loss = 109.47482755\n",
      "Iteration 3472, loss = 109.47315136\n",
      "Iteration 3473, loss = 109.47276700\n",
      "Iteration 3474, loss = 109.47132580\n",
      "Iteration 3475, loss = 109.47230791\n",
      "Iteration 3476, loss = 109.47341920\n",
      "Iteration 3477, loss = 109.47068023\n",
      "Iteration 3478, loss = 109.46868575\n",
      "Iteration 3479, loss = 109.46821583\n",
      "Iteration 3480, loss = 109.46710299\n",
      "Iteration 3481, loss = 109.46640312\n",
      "Iteration 3482, loss = 109.46497349\n",
      "Iteration 3483, loss = 109.46441640\n",
      "Iteration 3484, loss = 109.46393928\n",
      "Iteration 3485, loss = 109.46331297\n",
      "Iteration 3486, loss = 109.46200173\n",
      "Iteration 3487, loss = 109.46357651\n",
      "Iteration 3488, loss = 109.46204946\n",
      "Iteration 3489, loss = 109.46097516\n",
      "Iteration 3490, loss = 109.46119445\n",
      "Iteration 3491, loss = 109.45860349\n",
      "Iteration 3492, loss = 109.45960296\n",
      "Iteration 3493, loss = 109.45949280\n",
      "Iteration 3494, loss = 109.45528357\n",
      "Iteration 3495, loss = 109.45477992\n",
      "Iteration 3496, loss = 109.45573649\n",
      "Iteration 3497, loss = 109.45280309\n",
      "Iteration 3498, loss = 109.45617011\n",
      "Iteration 3499, loss = 109.45750233\n",
      "Iteration 3500, loss = 109.45512987\n",
      "Iteration 3501, loss = 109.45089618\n",
      "Iteration 3502, loss = 109.45565094\n",
      "Iteration 3503, loss = 109.45696905\n",
      "Iteration 3504, loss = 109.45375272\n",
      "Iteration 3505, loss = 109.44743380\n",
      "Iteration 3506, loss = 109.44857318\n",
      "Iteration 3507, loss = 109.44710357\n",
      "Iteration 3508, loss = 109.44651389\n",
      "Iteration 3509, loss = 109.44485421\n",
      "Iteration 3510, loss = 109.44616483\n",
      "Iteration 3511, loss = 109.44701348\n",
      "Iteration 3512, loss = 109.44402446\n",
      "Iteration 3513, loss = 109.44296415\n",
      "Iteration 3514, loss = 109.44329838\n",
      "Iteration 3515, loss = 109.44062032\n",
      "Iteration 3516, loss = 109.43978112\n",
      "Iteration 3517, loss = 109.43945881\n",
      "Iteration 3518, loss = 109.43805507\n",
      "Iteration 3519, loss = 109.43830468\n",
      "Iteration 3520, loss = 109.43720969\n",
      "Iteration 3521, loss = 109.43596123\n",
      "Iteration 3522, loss = 109.43765611\n",
      "Iteration 3523, loss = 109.43623298\n",
      "Iteration 3524, loss = 109.43557689\n",
      "Iteration 3525, loss = 109.43596148\n",
      "Iteration 3526, loss = 109.43380572\n",
      "Iteration 3527, loss = 109.43327355\n",
      "Iteration 3528, loss = 109.43253314\n",
      "Iteration 3529, loss = 109.43068999\n",
      "Iteration 3530, loss = 109.43010049\n",
      "Iteration 3531, loss = 109.42842699\n",
      "Iteration 3532, loss = 109.42873198\n",
      "Iteration 3533, loss = 109.42645649\n",
      "Iteration 3534, loss = 109.42983886\n",
      "Iteration 3535, loss = 109.42888062\n",
      "Iteration 3536, loss = 109.42474443\n",
      "Iteration 3537, loss = 109.42454417\n",
      "Iteration 3538, loss = 109.42357891\n",
      "Iteration 3539, loss = 109.42325459\n",
      "Iteration 3540, loss = 109.42201499\n",
      "Iteration 3541, loss = 109.42441508\n",
      "Iteration 3542, loss = 109.42320518\n",
      "Iteration 3543, loss = 109.42078636\n",
      "Iteration 3544, loss = 109.42092253\n",
      "Iteration 3545, loss = 109.41856127\n",
      "Iteration 3546, loss = 109.42154127\n",
      "Iteration 3547, loss = 109.42072274\n",
      "Iteration 3548, loss = 109.41612691\n",
      "Iteration 3549, loss = 109.42058565\n",
      "Iteration 3550, loss = 109.42250795\n",
      "Iteration 3551, loss = 109.42193627\n",
      "Iteration 3552, loss = 109.41916580\n",
      "Iteration 3553, loss = 109.41358413\n",
      "Iteration 3554, loss = 109.41694356\n",
      "Iteration 3555, loss = 109.41953850\n",
      "Iteration 3556, loss = 109.41812421\n",
      "Iteration 3557, loss = 109.41137041\n",
      "Iteration 3558, loss = 109.41303727\n",
      "Iteration 3559, loss = 109.41628918\n",
      "Iteration 3560, loss = 109.41719180\n",
      "Iteration 3561, loss = 109.41447150\n",
      "Iteration 3562, loss = 109.40980563\n",
      "Iteration 3563, loss = 109.40600894\n",
      "Iteration 3564, loss = 109.40843787\n",
      "Iteration 3565, loss = 109.40564455\n",
      "Iteration 3566, loss = 109.40502841\n",
      "Iteration 3567, loss = 109.40612235\n",
      "Iteration 3568, loss = 109.40465365\n",
      "Iteration 3569, loss = 109.40081311\n",
      "Iteration 3570, loss = 109.40533581\n",
      "Iteration 3571, loss = 109.40635140\n",
      "Iteration 3572, loss = 109.40265150\n",
      "Iteration 3573, loss = 109.39842270\n",
      "Iteration 3574, loss = 109.40020881\n",
      "Iteration 3575, loss = 109.39965794\n",
      "Iteration 3576, loss = 109.39578101\n",
      "Iteration 3577, loss = 109.39885465\n",
      "Iteration 3578, loss = 109.39972344\n",
      "Iteration 3579, loss = 109.39546380\n",
      "Iteration 3580, loss = 109.39493441\n",
      "Iteration 3581, loss = 109.39678356\n",
      "Iteration 3582, loss = 109.39620889\n",
      "Iteration 3583, loss = 109.39282073\n",
      "Iteration 3584, loss = 109.39122605\n",
      "Iteration 3585, loss = 109.39182354\n",
      "Iteration 3586, loss = 109.38788645\n",
      "Iteration 3587, loss = 109.39243989\n",
      "Iteration 3588, loss = 109.39420847\n",
      "Iteration 3589, loss = 109.39324392\n",
      "Iteration 3590, loss = 109.39010318\n",
      "Iteration 3591, loss = 109.38464283\n",
      "Iteration 3592, loss = 109.39189161\n",
      "Iteration 3593, loss = 109.39481156\n",
      "Iteration 3594, loss = 109.39254198\n",
      "Iteration 3595, loss = 109.38572983\n",
      "Iteration 3596, loss = 109.38415582\n",
      "Iteration 3597, loss = 109.38772564\n",
      "Iteration 3598, loss = 109.38824760\n",
      "Iteration 3599, loss = 109.38616454\n",
      "Iteration 3600, loss = 109.38143109\n",
      "Iteration 3601, loss = 109.37895861\n",
      "Iteration 3602, loss = 109.38097483\n",
      "Iteration 3603, loss = 109.37827695\n",
      "Iteration 3604, loss = 109.37700346\n",
      "Iteration 3605, loss = 109.37814806\n",
      "Iteration 3606, loss = 109.37683542\n",
      "Iteration 3607, loss = 109.37293624\n",
      "Iteration 3608, loss = 109.37781843\n",
      "Iteration 3609, loss = 109.37877715\n",
      "Iteration 3610, loss = 109.37503746\n",
      "Iteration 3611, loss = 109.37161171\n",
      "Iteration 3612, loss = 109.37341394\n",
      "Iteration 3613, loss = 109.37254212\n",
      "Iteration 3614, loss = 109.36950124\n",
      "Iteration 3615, loss = 109.37056544\n",
      "Iteration 3616, loss = 109.37093832\n",
      "Iteration 3617, loss = 109.36657820\n",
      "Iteration 3618, loss = 109.36927139\n",
      "Iteration 3619, loss = 109.37118195\n",
      "Iteration 3620, loss = 109.37038956\n",
      "Iteration 3621, loss = 109.36713892\n",
      "Iteration 3622, loss = 109.36176043\n",
      "Iteration 3623, loss = 109.36975644\n",
      "Iteration 3624, loss = 109.37255968\n",
      "Iteration 3625, loss = 109.37013621\n",
      "Iteration 3626, loss = 109.36371589\n",
      "Iteration 3627, loss = 109.36168849\n",
      "Iteration 3628, loss = 109.36514880\n",
      "Iteration 3629, loss = 109.36563631\n",
      "Iteration 3630, loss = 109.36359376\n",
      "Iteration 3631, loss = 109.35918359\n",
      "Iteration 3632, loss = 109.35662771\n",
      "Iteration 3633, loss = 109.35849165\n",
      "Iteration 3634, loss = 109.35546247\n",
      "Iteration 3635, loss = 109.35569987\n",
      "Iteration 3636, loss = 109.35682118\n",
      "Iteration 3637, loss = 109.35563897\n",
      "Iteration 3638, loss = 109.35219594\n",
      "Iteration 3639, loss = 109.35401481\n",
      "Iteration 3640, loss = 109.35473645\n",
      "Iteration 3641, loss = 109.35065598\n",
      "Iteration 3642, loss = 109.35140840\n",
      "Iteration 3643, loss = 109.35332432\n",
      "Iteration 3644, loss = 109.35254398\n",
      "Iteration 3645, loss = 109.34942594\n",
      "Iteration 3646, loss = 109.34545555\n",
      "Iteration 3647, loss = 109.34591384\n",
      "Iteration 3648, loss = 109.34459775\n",
      "Iteration 3649, loss = 109.34395758\n",
      "Iteration 3650, loss = 109.34349397\n",
      "Iteration 3651, loss = 109.34149351\n",
      "Iteration 3652, loss = 109.34214735\n",
      "Iteration 3653, loss = 109.34128094\n",
      "Iteration 3654, loss = 109.33991578\n",
      "Iteration 3655, loss = 109.34119123\n",
      "Iteration 3656, loss = 109.33952982\n",
      "Iteration 3657, loss = 109.33990350\n",
      "Iteration 3658, loss = 109.34059681\n",
      "Iteration 3659, loss = 109.33910417\n",
      "Iteration 3660, loss = 109.33484977\n",
      "Iteration 3661, loss = 109.33403036\n",
      "Iteration 3662, loss = 109.33608411\n",
      "Iteration 3663, loss = 109.33322621\n",
      "Iteration 3664, loss = 109.33577735\n",
      "Iteration 3665, loss = 109.33676244\n",
      "Iteration 3666, loss = 109.33518031\n",
      "Iteration 3667, loss = 109.33175249\n",
      "Iteration 3668, loss = 109.33354922\n",
      "Iteration 3669, loss = 109.33453604\n",
      "Iteration 3670, loss = 109.33089662\n",
      "Iteration 3671, loss = 109.33041700\n",
      "Iteration 3672, loss = 109.33219095\n",
      "Iteration 3673, loss = 109.33149909\n",
      "Iteration 3674, loss = 109.32854015\n",
      "Iteration 3675, loss = 109.32579678\n",
      "Iteration 3676, loss = 109.32606393\n",
      "Iteration 3677, loss = 109.32398427\n",
      "Iteration 3678, loss = 109.32338585\n",
      "Iteration 3679, loss = 109.32286545\n",
      "Iteration 3680, loss = 109.32111757\n",
      "Iteration 3681, loss = 109.32109590\n",
      "Iteration 3682, loss = 109.32128060\n",
      "Iteration 3683, loss = 109.32016153\n",
      "Iteration 3684, loss = 109.31946704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3685, loss = 109.31762119\n",
      "Iteration 3686, loss = 109.31900483\n",
      "Iteration 3687, loss = 109.31660027\n",
      "Iteration 3688, loss = 109.31529110\n",
      "Iteration 3689, loss = 109.31827734\n",
      "Iteration 3690, loss = 109.31629222\n",
      "Iteration 3691, loss = 109.31519255\n",
      "Iteration 3692, loss = 109.31618055\n",
      "Iteration 3693, loss = 109.31463095\n",
      "Iteration 3694, loss = 109.31190407\n",
      "Iteration 3695, loss = 109.31086894\n",
      "Iteration 3696, loss = 109.31258857\n",
      "Iteration 3697, loss = 109.31259604\n",
      "Iteration 3698, loss = 109.31008800\n",
      "Iteration 3699, loss = 109.30953145\n",
      "Iteration 3700, loss = 109.30884324\n",
      "Iteration 3701, loss = 109.30781054\n",
      "Iteration 3702, loss = 109.30765792\n",
      "Iteration 3703, loss = 109.30543982\n",
      "Iteration 3704, loss = 109.30791408\n",
      "Iteration 3705, loss = 109.30720700\n",
      "Iteration 3706, loss = 109.30300319\n",
      "Iteration 3707, loss = 109.30315096\n",
      "Iteration 3708, loss = 109.30147376\n",
      "Iteration 3709, loss = 109.30200377\n",
      "Iteration 3710, loss = 109.30110493\n",
      "Iteration 3711, loss = 109.30150780\n",
      "Iteration 3712, loss = 109.29958537\n",
      "Iteration 3713, loss = 109.30107171\n",
      "Iteration 3714, loss = 109.30178096\n",
      "Iteration 3715, loss = 109.29999549\n",
      "Iteration 3716, loss = 109.29605195\n",
      "Iteration 3717, loss = 109.30215301\n",
      "Iteration 3718, loss = 109.30334352\n",
      "Iteration 3719, loss = 109.29959713\n",
      "Iteration 3720, loss = 109.29433049\n",
      "Iteration 3721, loss = 109.29596705\n",
      "Iteration 3722, loss = 109.29527915\n",
      "Iteration 3723, loss = 109.29232813\n",
      "Iteration 3724, loss = 109.29467236\n",
      "Iteration 3725, loss = 109.29477366\n",
      "Iteration 3726, loss = 109.29023538\n",
      "Iteration 3727, loss = 109.29275921\n",
      "Iteration 3728, loss = 109.29485235\n",
      "Iteration 3729, loss = 109.29429836\n",
      "Iteration 3730, loss = 109.29125441\n",
      "Iteration 3731, loss = 109.28622014\n",
      "Iteration 3732, loss = 109.29195981\n",
      "Iteration 3733, loss = 109.29444208\n",
      "Iteration 3734, loss = 109.29176292\n",
      "Iteration 3735, loss = 109.28451551\n",
      "Iteration 3736, loss = 109.28758292\n",
      "Iteration 3737, loss = 109.29142702\n",
      "Iteration 3738, loss = 109.29230111\n",
      "Iteration 3739, loss = 109.29037647\n",
      "Iteration 3740, loss = 109.28642734\n",
      "Iteration 3741, loss = 109.28025674\n",
      "Iteration 3742, loss = 109.28495415\n",
      "Iteration 3743, loss = 109.28877503\n",
      "Iteration 3744, loss = 109.28701814\n",
      "Iteration 3745, loss = 109.28026347\n",
      "Iteration 3746, loss = 109.27858756\n",
      "Iteration 3747, loss = 109.28197607\n",
      "Iteration 3748, loss = 109.28264946\n",
      "Iteration 3749, loss = 109.28099279\n",
      "Iteration 3750, loss = 109.27676125\n",
      "Iteration 3751, loss = 109.27223061\n",
      "Iteration 3752, loss = 109.27385948\n",
      "Iteration 3753, loss = 109.27040175\n",
      "Iteration 3754, loss = 109.26985262\n",
      "Iteration 3755, loss = 109.27172425\n",
      "Iteration 3756, loss = 109.27167104\n",
      "Iteration 3757, loss = 109.26904740\n",
      "Iteration 3758, loss = 109.27049426\n",
      "Iteration 3759, loss = 109.26981061\n",
      "Iteration 3760, loss = 109.26658315\n",
      "Iteration 3761, loss = 109.26644177\n",
      "Iteration 3762, loss = 109.26425895\n",
      "Iteration 3763, loss = 109.26554241\n",
      "Iteration 3764, loss = 109.26441706\n",
      "Iteration 3765, loss = 109.26407813\n",
      "Iteration 3766, loss = 109.26224084\n",
      "Iteration 3767, loss = 109.26468081\n",
      "Iteration 3768, loss = 109.26531496\n",
      "Iteration 3769, loss = 109.26374966\n",
      "Iteration 3770, loss = 109.26011044\n",
      "Iteration 3771, loss = 109.26393591\n",
      "Iteration 3772, loss = 109.26503243\n",
      "Iteration 3773, loss = 109.26100216\n",
      "Iteration 3774, loss = 109.25877727\n",
      "Iteration 3775, loss = 109.26056768\n",
      "Iteration 3776, loss = 109.25985636\n",
      "Iteration 3777, loss = 109.25696251\n",
      "Iteration 3778, loss = 109.25539528\n",
      "Iteration 3779, loss = 109.25552264\n",
      "Iteration 3780, loss = 109.25288482\n",
      "Iteration 3781, loss = 109.25245636\n",
      "Iteration 3782, loss = 109.25160939\n",
      "Iteration 3783, loss = 109.25085294\n",
      "Iteration 3784, loss = 109.24996528\n",
      "Iteration 3785, loss = 109.25107495\n",
      "Iteration 3786, loss = 109.25002475\n",
      "Iteration 3787, loss = 109.24736232\n",
      "Iteration 3788, loss = 109.25281837\n",
      "Iteration 3789, loss = 109.25248246\n",
      "Iteration 3790, loss = 109.24749955\n",
      "Iteration 3791, loss = 109.24894133\n",
      "Iteration 3792, loss = 109.25127396\n",
      "Iteration 3793, loss = 109.25083200\n",
      "Iteration 3794, loss = 109.24834003\n",
      "Iteration 3795, loss = 109.24369325\n",
      "Iteration 3796, loss = 109.24685199\n",
      "Iteration 3797, loss = 109.24916488\n",
      "Iteration 3798, loss = 109.24620116\n",
      "Iteration 3799, loss = 109.23978248\n",
      "Iteration 3800, loss = 109.24101991\n",
      "Iteration 3801, loss = 109.24027225\n",
      "Iteration 3802, loss = 109.23732631\n",
      "Iteration 3803, loss = 109.24238659\n",
      "Iteration 3804, loss = 109.24255138\n",
      "Iteration 3805, loss = 109.23757707\n",
      "Iteration 3806, loss = 109.23840346\n",
      "Iteration 3807, loss = 109.24090095\n",
      "Iteration 3808, loss = 109.24083471\n",
      "Iteration 3809, loss = 109.23830350\n",
      "Iteration 3810, loss = 109.23378734\n",
      "Iteration 3811, loss = 109.23615558\n",
      "Iteration 3812, loss = 109.23803897\n",
      "Iteration 3813, loss = 109.23484524\n",
      "Iteration 3814, loss = 109.23053270\n",
      "Iteration 3815, loss = 109.23203842\n",
      "Iteration 3816, loss = 109.23119022\n",
      "Iteration 3817, loss = 109.22830912\n",
      "Iteration 3818, loss = 109.23103822\n",
      "Iteration 3819, loss = 109.23121074\n",
      "Iteration 3820, loss = 109.22606624\n",
      "Iteration 3821, loss = 109.22945761\n",
      "Iteration 3822, loss = 109.23189778\n",
      "Iteration 3823, loss = 109.23170235\n",
      "Iteration 3824, loss = 109.22905950\n",
      "Iteration 3825, loss = 109.22438200\n",
      "Iteration 3826, loss = 109.22555314\n",
      "Iteration 3827, loss = 109.22780890\n",
      "Iteration 3828, loss = 109.22463459\n",
      "Iteration 3829, loss = 109.22065666\n",
      "Iteration 3830, loss = 109.22210671\n",
      "Iteration 3831, loss = 109.22124757\n",
      "Iteration 3832, loss = 109.21839429\n",
      "Iteration 3833, loss = 109.22086447\n",
      "Iteration 3834, loss = 109.22104548\n",
      "Iteration 3835, loss = 109.21604536\n",
      "Iteration 3836, loss = 109.21967928\n",
      "Iteration 3837, loss = 109.22212999\n",
      "Iteration 3838, loss = 109.22196000\n",
      "Iteration 3839, loss = 109.21941424\n",
      "Iteration 3840, loss = 109.21505993\n",
      "Iteration 3841, loss = 109.21491607\n",
      "Iteration 3842, loss = 109.21690167\n",
      "Iteration 3843, loss = 109.21365234\n",
      "Iteration 3844, loss = 109.21163950\n",
      "Iteration 3845, loss = 109.21318933\n",
      "Iteration 3846, loss = 109.21237415\n",
      "Iteration 3847, loss = 109.20951197\n",
      "Iteration 3848, loss = 109.20966293\n",
      "Iteration 3849, loss = 109.20969194\n",
      "Iteration 3850, loss = 109.20571902\n",
      "Iteration 3851, loss = 109.20557411\n",
      "Iteration 3852, loss = 109.20525626\n",
      "Iteration 3853, loss = 109.20429205\n",
      "Iteration 3854, loss = 109.20328407\n",
      "Iteration 3855, loss = 109.20545752\n",
      "Iteration 3856, loss = 109.20326404\n",
      "Iteration 3857, loss = 109.20356931\n",
      "Iteration 3858, loss = 109.20429578\n",
      "Iteration 3859, loss = 109.20274776\n",
      "Iteration 3860, loss = 109.19923881\n",
      "Iteration 3861, loss = 109.20444108\n",
      "Iteration 3862, loss = 109.20522330\n",
      "Iteration 3863, loss = 109.20090231\n",
      "Iteration 3864, loss = 109.19876933\n",
      "Iteration 3865, loss = 109.20086895\n",
      "Iteration 3866, loss = 109.20035768\n",
      "Iteration 3867, loss = 109.19778177\n",
      "Iteration 3868, loss = 109.19396688\n",
      "Iteration 3869, loss = 109.19375751\n",
      "Iteration 3870, loss = 109.19443591\n",
      "Iteration 3871, loss = 109.19447281\n",
      "Iteration 3872, loss = 109.19223932\n",
      "Iteration 3873, loss = 109.19375890\n",
      "Iteration 3874, loss = 109.19297165\n",
      "Iteration 3875, loss = 109.18994078\n",
      "Iteration 3876, loss = 109.19022869\n",
      "Iteration 3877, loss = 109.18809031\n",
      "Iteration 3878, loss = 109.19107129\n",
      "Iteration 3879, loss = 109.18996153\n",
      "Iteration 3880, loss = 109.18680122\n",
      "Iteration 3881, loss = 109.18719394\n",
      "Iteration 3882, loss = 109.18540580\n",
      "Iteration 3883, loss = 109.18657908\n",
      "Iteration 3884, loss = 109.18518973\n",
      "Iteration 3885, loss = 109.18450636\n",
      "Iteration 3886, loss = 109.18503857\n",
      "Iteration 3887, loss = 109.18334019\n",
      "Iteration 3888, loss = 109.18144527\n",
      "Iteration 3889, loss = 109.18006559\n",
      "Iteration 3890, loss = 109.18250181\n",
      "Iteration 3891, loss = 109.18306911\n",
      "Iteration 3892, loss = 109.18136243\n",
      "Iteration 3893, loss = 109.17765202\n",
      "Iteration 3894, loss = 109.18319281\n",
      "Iteration 3895, loss = 109.18425182\n",
      "Iteration 3896, loss = 109.17990163\n",
      "Iteration 3897, loss = 109.17699504\n",
      "Iteration 3898, loss = 109.17908853\n",
      "Iteration 3899, loss = 109.17880075\n",
      "Iteration 3900, loss = 109.17631815\n",
      "Iteration 3901, loss = 109.17217327\n",
      "Iteration 3902, loss = 109.17178150\n",
      "Iteration 3903, loss = 109.17351061\n",
      "Iteration 3904, loss = 109.17365248\n",
      "Iteration 3905, loss = 109.17158546\n",
      "Iteration 3906, loss = 109.17088472\n",
      "Iteration 3907, loss = 109.16988550\n",
      "Iteration 3908, loss = 109.16980024\n",
      "Iteration 3909, loss = 109.17028244\n",
      "Iteration 3910, loss = 109.16839639\n",
      "Iteration 3911, loss = 109.16719103\n",
      "Iteration 3912, loss = 109.16589671\n",
      "Iteration 3913, loss = 109.16731125\n",
      "Iteration 3914, loss = 109.16790178\n",
      "Iteration 3915, loss = 109.16612884\n",
      "Iteration 3916, loss = 109.16249990\n",
      "Iteration 3917, loss = 109.16906918\n",
      "Iteration 3918, loss = 109.16999509\n",
      "Iteration 3919, loss = 109.16531910\n",
      "Iteration 3920, loss = 109.16222147\n",
      "Iteration 3921, loss = 109.16449608\n",
      "Iteration 3922, loss = 109.16435761\n",
      "Iteration 3923, loss = 109.16194883\n",
      "Iteration 3924, loss = 109.15769101\n",
      "Iteration 3925, loss = 109.16430930\n",
      "Iteration 3926, loss = 109.16605693\n",
      "Iteration 3927, loss = 109.16214848\n",
      "Iteration 3928, loss = 109.15554503\n",
      "Iteration 3929, loss = 109.15744080\n",
      "Iteration 3930, loss = 109.15706038\n",
      "Iteration 3931, loss = 109.15455163\n",
      "Iteration 3932, loss = 109.15536059\n",
      "Iteration 3933, loss = 109.15494579\n",
      "Iteration 3934, loss = 109.15183241\n",
      "Iteration 3935, loss = 109.15200941\n",
      "Iteration 3936, loss = 109.15006393\n",
      "Iteration 3937, loss = 109.15358463\n",
      "Iteration 3938, loss = 109.15239331\n",
      "Iteration 3939, loss = 109.14871999\n",
      "Iteration 3940, loss = 109.14932017\n",
      "Iteration 3941, loss = 109.14765428\n",
      "Iteration 3942, loss = 109.14876899\n",
      "Iteration 3943, loss = 109.14724467\n",
      "Iteration 3944, loss = 109.14700968\n",
      "Iteration 3945, loss = 109.14774784\n",
      "Iteration 3946, loss = 109.14611329\n",
      "Iteration 3947, loss = 109.14298371\n",
      "Iteration 3948, loss = 109.14246062\n",
      "Iteration 3949, loss = 109.14316591\n",
      "Iteration 3950, loss = 109.14162024\n",
      "Iteration 3951, loss = 109.14066815\n",
      "Iteration 3952, loss = 109.14250309\n",
      "Iteration 3953, loss = 109.14012747\n",
      "Iteration 3954, loss = 109.14181376\n",
      "Iteration 3955, loss = 109.14288855\n",
      "Iteration 3956, loss = 109.14170647\n",
      "Iteration 3957, loss = 109.13852649\n",
      "Iteration 3958, loss = 109.13931446\n",
      "Iteration 3959, loss = 109.13973305\n",
      "Iteration 3960, loss = 109.13497399\n",
      "Iteration 3961, loss = 109.13937490\n",
      "Iteration 3962, loss = 109.14176223\n",
      "Iteration 3963, loss = 109.14152034\n",
      "Iteration 3964, loss = 109.13909009\n",
      "Iteration 3965, loss = 109.13481359\n",
      "Iteration 3966, loss = 109.13390544\n",
      "Iteration 3967, loss = 109.13569592\n",
      "Iteration 3968, loss = 109.13212533\n",
      "Iteration 3969, loss = 109.13239833\n",
      "Iteration 3970, loss = 109.13415760\n",
      "Iteration 3971, loss = 109.13348197\n",
      "Iteration 3972, loss = 109.13087253\n",
      "Iteration 3973, loss = 109.12661606\n",
      "Iteration 3974, loss = 109.13439917\n",
      "Iteration 3975, loss = 109.13616619\n",
      "Iteration 3976, loss = 109.13235349\n",
      "Iteration 3977, loss = 109.12435866\n",
      "Iteration 3978, loss = 109.12624835\n",
      "Iteration 3979, loss = 109.12582589\n",
      "Iteration 3980, loss = 109.12352673\n",
      "Iteration 3981, loss = 109.12480017\n",
      "Iteration 3982, loss = 109.12413805\n",
      "Iteration 3983, loss = 109.12147047\n",
      "Iteration 3984, loss = 109.12185914\n",
      "Iteration 3985, loss = 109.12033079\n",
      "Iteration 3986, loss = 109.12140383\n",
      "Iteration 3987, loss = 109.11976823\n",
      "Iteration 3988, loss = 109.11944047\n",
      "Iteration 3989, loss = 109.12041413\n",
      "Iteration 3990, loss = 109.11898132\n",
      "Iteration 3991, loss = 109.11531865\n",
      "Iteration 3992, loss = 109.12170062\n",
      "Iteration 3993, loss = 109.12263300\n",
      "Iteration 3994, loss = 109.11787978\n",
      "Iteration 3995, loss = 109.11576514\n",
      "Iteration 3996, loss = 109.11801488\n",
      "Iteration 3997, loss = 109.11808220\n",
      "Iteration 3998, loss = 109.11591689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3999, loss = 109.11167172\n",
      "Iteration 4000, loss = 109.11568592\n",
      "Iteration 4001, loss = 109.11744427\n",
      "Iteration 4002, loss = 109.11357086\n",
      "Iteration 4003, loss = 109.10992781\n",
      "Iteration 4004, loss = 109.11176570\n",
      "Iteration 4005, loss = 109.11143247\n",
      "Iteration 4006, loss = 109.10904686\n",
      "Iteration 4007, loss = 109.10655449\n",
      "Iteration 4008, loss = 109.10605212\n",
      "Iteration 4009, loss = 109.10634593\n",
      "Iteration 4010, loss = 109.10671179\n",
      "Iteration 4011, loss = 109.10487469\n",
      "Iteration 4012, loss = 109.10454444\n",
      "Iteration 4013, loss = 109.10320523\n",
      "Iteration 4014, loss = 109.10362200\n",
      "Iteration 4015, loss = 109.10442411\n",
      "Iteration 4016, loss = 109.10276823\n",
      "Iteration 4017, loss = 109.09929011\n",
      "Iteration 4018, loss = 109.09911925\n",
      "Iteration 4019, loss = 109.09927120\n",
      "Iteration 4020, loss = 109.09847061\n",
      "Iteration 4021, loss = 109.09771456\n",
      "Iteration 4022, loss = 109.09804996\n",
      "Iteration 4023, loss = 109.09567382\n",
      "Iteration 4024, loss = 109.09623173\n",
      "Iteration 4025, loss = 109.09664497\n",
      "Iteration 4026, loss = 109.09607180\n",
      "Iteration 4027, loss = 109.09349226\n",
      "Iteration 4028, loss = 109.09752372\n",
      "Iteration 4029, loss = 109.09679409\n",
      "Iteration 4030, loss = 109.09174813\n",
      "Iteration 4031, loss = 109.09207737\n",
      "Iteration 4032, loss = 109.09049637\n",
      "Iteration 4033, loss = 109.09333651\n",
      "Iteration 4034, loss = 109.09158372\n",
      "Iteration 4035, loss = 109.09027210\n",
      "Iteration 4036, loss = 109.09113046\n",
      "Iteration 4037, loss = 109.08981806\n",
      "Iteration 4038, loss = 109.08660760\n",
      "Iteration 4039, loss = 109.09239023\n",
      "Iteration 4040, loss = 109.09279842\n",
      "Iteration 4041, loss = 109.08774003\n",
      "Iteration 4042, loss = 109.08766922\n",
      "Iteration 4043, loss = 109.09025250\n",
      "Iteration 4044, loss = 109.09033738\n",
      "Iteration 4045, loss = 109.08815839\n",
      "Iteration 4046, loss = 109.08410598\n",
      "Iteration 4047, loss = 109.08495359\n",
      "Iteration 4048, loss = 109.08646892\n",
      "Iteration 4049, loss = 109.08243764\n",
      "Iteration 4050, loss = 109.08265993\n",
      "Iteration 4051, loss = 109.08471689\n",
      "Iteration 4052, loss = 109.08434345\n",
      "Iteration 4053, loss = 109.08187361\n",
      "Iteration 4054, loss = 109.07768653\n",
      "Iteration 4055, loss = 109.08317879\n",
      "Iteration 4056, loss = 109.08486761\n",
      "Iteration 4057, loss = 109.08087666\n",
      "Iteration 4058, loss = 109.07593621\n",
      "Iteration 4059, loss = 109.07798617\n",
      "Iteration 4060, loss = 109.07769229\n",
      "Iteration 4061, loss = 109.07541864\n",
      "Iteration 4062, loss = 109.07291230\n",
      "Iteration 4063, loss = 109.07227303\n",
      "Iteration 4064, loss = 109.07360527\n",
      "Iteration 4065, loss = 109.07399312\n",
      "Iteration 4066, loss = 109.07251296\n",
      "Iteration 4067, loss = 109.06944973\n",
      "Iteration 4068, loss = 109.06904763\n",
      "Iteration 4069, loss = 109.06946068\n",
      "Iteration 4070, loss = 109.06811487\n",
      "Iteration 4071, loss = 109.06731050\n",
      "Iteration 4072, loss = 109.06812679\n",
      "Iteration 4073, loss = 109.06564469\n",
      "Iteration 4074, loss = 109.06617573\n",
      "Iteration 4075, loss = 109.06631611\n",
      "Iteration 4076, loss = 109.06585923\n",
      "Iteration 4077, loss = 109.06361091\n",
      "Iteration 4078, loss = 109.06696696\n",
      "Iteration 4079, loss = 109.06596707\n",
      "Iteration 4080, loss = 109.06212575\n",
      "Iteration 4081, loss = 109.06265935\n",
      "Iteration 4082, loss = 109.06119571\n",
      "Iteration 4083, loss = 109.06190827\n",
      "Iteration 4084, loss = 109.06006653\n",
      "Iteration 4085, loss = 109.06129377\n",
      "Iteration 4086, loss = 109.06224109\n",
      "Iteration 4087, loss = 109.06100287\n",
      "Iteration 4088, loss = 109.05784792\n",
      "Iteration 4089, loss = 109.06048280\n",
      "Iteration 4090, loss = 109.06085864\n",
      "Iteration 4091, loss = 109.05575355\n",
      "Iteration 4092, loss = 109.05921010\n",
      "Iteration 4093, loss = 109.06182938\n",
      "Iteration 4094, loss = 109.06191154\n",
      "Iteration 4095, loss = 109.05975680\n",
      "Iteration 4096, loss = 109.05574194\n",
      "Iteration 4097, loss = 109.05280850\n",
      "Iteration 4098, loss = 109.05430477\n",
      "Iteration 4099, loss = 109.05027343\n",
      "Iteration 4100, loss = 109.05447792\n",
      "Iteration 4101, loss = 109.05657998\n",
      "Iteration 4102, loss = 109.05617653\n",
      "Iteration 4103, loss = 109.05371612\n",
      "Iteration 4104, loss = 109.04961838\n",
      "Iteration 4105, loss = 109.05086159\n",
      "Iteration 4106, loss = 109.05245507\n",
      "Iteration 4107, loss = 109.04837544\n",
      "Iteration 4108, loss = 109.04812660\n",
      "Iteration 4109, loss = 109.05030475\n",
      "Iteration 4110, loss = 109.05001220\n",
      "Iteration 4111, loss = 109.04771977\n",
      "Iteration 4112, loss = 109.04389730\n",
      "Iteration 4113, loss = 109.04768602\n",
      "Iteration 4114, loss = 109.04889943\n",
      "Iteration 4115, loss = 109.04439406\n",
      "Iteration 4116, loss = 109.04319871\n",
      "Iteration 4117, loss = 109.04562359\n",
      "Iteration 4118, loss = 109.04551855\n",
      "Iteration 4119, loss = 109.04331753\n",
      "Iteration 4120, loss = 109.03956788\n",
      "Iteration 4121, loss = 109.04222720\n",
      "Iteration 4122, loss = 109.04332920\n",
      "Iteration 4123, loss = 109.03874568\n",
      "Iteration 4124, loss = 109.03911674\n",
      "Iteration 4125, loss = 109.04161260\n",
      "Iteration 4126, loss = 109.04151749\n",
      "Iteration 4127, loss = 109.03923992\n",
      "Iteration 4128, loss = 109.03538718\n",
      "Iteration 4129, loss = 109.03684587\n",
      "Iteration 4130, loss = 109.03807656\n",
      "Iteration 4131, loss = 109.03362548\n",
      "Iteration 4132, loss = 109.03467624\n",
      "Iteration 4133, loss = 109.03711963\n",
      "Iteration 4134, loss = 109.03696812\n",
      "Iteration 4135, loss = 109.03461883\n",
      "Iteration 4136, loss = 109.03070475\n",
      "Iteration 4137, loss = 109.03238413\n",
      "Iteration 4138, loss = 109.03368941\n",
      "Iteration 4139, loss = 109.02926976\n",
      "Iteration 4140, loss = 109.02987201\n",
      "Iteration 4141, loss = 109.03230387\n",
      "Iteration 4142, loss = 109.03216738\n",
      "Iteration 4143, loss = 109.02983670\n",
      "Iteration 4144, loss = 109.02594739\n",
      "Iteration 4145, loss = 109.02799170\n",
      "Iteration 4146, loss = 109.02926006\n",
      "Iteration 4147, loss = 109.02476770\n",
      "Iteration 4148, loss = 109.02524722\n",
      "Iteration 4149, loss = 109.02771913\n",
      "Iteration 4150, loss = 109.02764029\n",
      "Iteration 4151, loss = 109.02535706\n",
      "Iteration 4152, loss = 109.02150074\n",
      "Iteration 4153, loss = 109.02309427\n",
      "Iteration 4154, loss = 109.02431675\n",
      "Iteration 4155, loss = 109.01977471\n",
      "Iteration 4156, loss = 109.02093884\n",
      "Iteration 4157, loss = 109.02344189\n",
      "Iteration 4158, loss = 109.02339407\n",
      "Iteration 4159, loss = 109.02112724\n",
      "Iteration 4160, loss = 109.01727299\n",
      "Iteration 4161, loss = 109.01792780\n",
      "Iteration 4162, loss = 109.01914729\n",
      "Iteration 4163, loss = 109.01460644\n",
      "Iteration 4164, loss = 109.01674380\n",
      "Iteration 4165, loss = 109.01925464\n",
      "Iteration 4166, loss = 109.01920879\n",
      "Iteration 4167, loss = 109.01693509\n",
      "Iteration 4168, loss = 109.01307128\n",
      "Iteration 4169, loss = 109.01280332\n",
      "Iteration 4170, loss = 109.01403524\n",
      "Iteration 4171, loss = 109.00950141\n",
      "Iteration 4172, loss = 109.01254079\n",
      "Iteration 4173, loss = 109.01505762\n",
      "Iteration 4174, loss = 109.01501417\n",
      "Iteration 4175, loss = 109.01273749\n",
      "Iteration 4176, loss = 109.00887333\n",
      "Iteration 4177, loss = 109.00771795\n",
      "Iteration 4178, loss = 109.00894937\n",
      "Iteration 4179, loss = 109.00440658\n",
      "Iteration 4180, loss = 109.00837100\n",
      "Iteration 4181, loss = 109.01090312\n",
      "Iteration 4182, loss = 109.01087007\n",
      "Iteration 4183, loss = 109.00859643\n",
      "Iteration 4184, loss = 109.00473753\n",
      "Iteration 4185, loss = 109.00257008\n",
      "Iteration 4186, loss = 109.00379289\n",
      "Iteration 4187, loss = 108.99956250\n",
      "Iteration 4188, loss = 108.99925898\n",
      "Iteration 4189, loss = 109.00029554\n",
      "Iteration 4190, loss = 108.99857223\n",
      "Iteration 4191, loss = 108.99792500\n",
      "Iteration 4192, loss = 108.99907395\n",
      "Iteration 4193, loss = 108.99620064\n",
      "Iteration 4194, loss = 108.99662614\n",
      "Iteration 4195, loss = 108.99815531\n",
      "Iteration 4196, loss = 108.99794714\n",
      "Iteration 4197, loss = 108.99555930\n",
      "Iteration 4198, loss = 108.99550943\n",
      "Iteration 4199, loss = 108.99426106\n",
      "Iteration 4200, loss = 108.99535464\n",
      "Iteration 4201, loss = 108.99604798\n",
      "Iteration 4202, loss = 108.99467945\n",
      "Iteration 4203, loss = 108.99176782\n",
      "Iteration 4204, loss = 108.99503928\n",
      "Iteration 4205, loss = 108.99522164\n",
      "Iteration 4206, loss = 108.98953425\n",
      "Iteration 4207, loss = 108.99361158\n",
      "Iteration 4208, loss = 108.99648898\n",
      "Iteration 4209, loss = 108.99693227\n",
      "Iteration 4210, loss = 108.99505892\n",
      "Iteration 4211, loss = 108.99126686\n",
      "Iteration 4212, loss = 108.98607098\n",
      "Iteration 4213, loss = 108.99444475\n",
      "Iteration 4214, loss = 108.99758380\n",
      "Iteration 4215, loss = 108.99442157\n",
      "Iteration 4216, loss = 108.98585536\n",
      "Iteration 4217, loss = 108.98869523\n",
      "Iteration 4218, loss = 108.99315796\n",
      "Iteration 4219, loss = 108.99495279\n",
      "Iteration 4220, loss = 108.99414630\n",
      "Iteration 4221, loss = 108.99117133\n",
      "Iteration 4222, loss = 108.98652545\n",
      "Iteration 4223, loss = 108.98061765\n",
      "Iteration 4224, loss = 108.98913278\n",
      "Iteration 4225, loss = 108.99326801\n",
      "Iteration 4226, loss = 108.99094388\n",
      "Iteration 4227, loss = 108.98298790\n",
      "Iteration 4228, loss = 108.98078027\n",
      "Iteration 4229, loss = 108.98484548\n",
      "Iteration 4230, loss = 108.98639781\n",
      "Iteration 4231, loss = 108.98553710\n",
      "Iteration 4232, loss = 108.98271007\n",
      "Iteration 4233, loss = 108.97839134\n",
      "Iteration 4234, loss = 108.97562634\n",
      "Iteration 4235, loss = 108.97772579\n",
      "Iteration 4236, loss = 108.97355712\n",
      "Iteration 4237, loss = 108.97618713\n",
      "Iteration 4238, loss = 108.97835468\n",
      "Iteration 4239, loss = 108.97831313\n",
      "Iteration 4240, loss = 108.97614223\n",
      "Iteration 4241, loss = 108.97221085\n",
      "Iteration 4242, loss = 108.97296404\n",
      "Iteration 4243, loss = 108.97433580\n",
      "Iteration 4244, loss = 108.96973480\n",
      "Iteration 4245, loss = 108.97213096\n",
      "Iteration 4246, loss = 108.97454580\n",
      "Iteration 4247, loss = 108.97467305\n",
      "Iteration 4248, loss = 108.97286008\n",
      "Iteration 4249, loss = 108.96938303\n",
      "Iteration 4250, loss = 108.96567294\n",
      "Iteration 4251, loss = 108.96654758\n",
      "Iteration 4252, loss = 108.96480022\n",
      "Iteration 4253, loss = 108.96469469\n",
      "Iteration 4254, loss = 108.96282257\n",
      "Iteration 4255, loss = 108.96693834\n",
      "Iteration 4256, loss = 108.96541839\n",
      "Iteration 4257, loss = 108.96255571\n",
      "Iteration 4258, loss = 108.96355943\n",
      "Iteration 4259, loss = 108.96268163\n",
      "Iteration 4260, loss = 108.96009003\n",
      "Iteration 4261, loss = 108.96409438\n",
      "Iteration 4262, loss = 108.96365191\n",
      "Iteration 4263, loss = 108.95787264\n",
      "Iteration 4264, loss = 108.95839814\n",
      "Iteration 4265, loss = 108.95707218\n",
      "Iteration 4266, loss = 108.95976067\n",
      "Iteration 4267, loss = 108.95759746\n",
      "Iteration 4268, loss = 108.95799576\n",
      "Iteration 4269, loss = 108.95927750\n",
      "Iteration 4270, loss = 108.95853239\n",
      "Iteration 4271, loss = 108.95590856\n",
      "Iteration 4272, loss = 108.95523838\n",
      "Iteration 4273, loss = 108.95495254\n",
      "Iteration 4274, loss = 108.95347830\n",
      "Iteration 4275, loss = 108.95386743\n",
      "Iteration 4276, loss = 108.95233951\n",
      "Iteration 4277, loss = 108.95223974\n",
      "Iteration 4278, loss = 108.95049288\n",
      "Iteration 4279, loss = 108.95273592\n",
      "Iteration 4280, loss = 108.95378547\n",
      "Iteration 4281, loss = 108.95286698\n",
      "Iteration 4282, loss = 108.95012946\n",
      "Iteration 4283, loss = 108.94963140\n",
      "Iteration 4284, loss = 108.94947285\n",
      "Iteration 4285, loss = 108.94737060\n",
      "Iteration 4286, loss = 108.94773609\n",
      "Iteration 4287, loss = 108.94621388\n",
      "Iteration 4288, loss = 108.94692786\n",
      "Iteration 4289, loss = 108.94508668\n",
      "Iteration 4290, loss = 108.94670531\n",
      "Iteration 4291, loss = 108.94781742\n",
      "Iteration 4292, loss = 108.94696024\n",
      "Iteration 4293, loss = 108.94433632\n",
      "Iteration 4294, loss = 108.94353066\n",
      "Iteration 4295, loss = 108.94319586\n",
      "Iteration 4296, loss = 108.94193476\n",
      "Iteration 4297, loss = 108.94240206\n",
      "Iteration 4298, loss = 108.94099712\n",
      "Iteration 4299, loss = 108.93996135\n",
      "Iteration 4300, loss = 108.93843234\n",
      "Iteration 4301, loss = 108.93944132\n",
      "Iteration 4302, loss = 108.93914601\n",
      "Iteration 4303, loss = 108.93878075\n",
      "Iteration 4304, loss = 108.93646447\n",
      "Iteration 4305, loss = 108.94042902\n",
      "Iteration 4306, loss = 108.93916406\n",
      "Iteration 4307, loss = 108.93606802\n",
      "Iteration 4308, loss = 108.93704747\n",
      "Iteration 4309, loss = 108.93591904\n",
      "Iteration 4310, loss = 108.93315536\n",
      "Iteration 4311, loss = 108.93869060\n",
      "Iteration 4312, loss = 108.93856300\n",
      "Iteration 4313, loss = 108.93242101\n",
      "Iteration 4314, loss = 108.93661102\n",
      "Iteration 4315, loss = 108.93968625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4316, loss = 108.94042378\n",
      "Iteration 4317, loss = 108.93900273\n",
      "Iteration 4318, loss = 108.93562869\n",
      "Iteration 4319, loss = 108.93066755\n",
      "Iteration 4320, loss = 108.93381152\n",
      "Iteration 4321, loss = 108.93661243\n",
      "Iteration 4322, loss = 108.93310671\n",
      "Iteration 4323, loss = 108.92735230\n",
      "Iteration 4324, loss = 108.92934010\n",
      "Iteration 4325, loss = 108.92925706\n",
      "Iteration 4326, loss = 108.92728099\n",
      "Iteration 4327, loss = 108.92482404\n",
      "Iteration 4328, loss = 108.92380066\n",
      "Iteration 4329, loss = 108.92568600\n",
      "Iteration 4330, loss = 108.92395346\n",
      "Iteration 4331, loss = 108.92343774\n",
      "Iteration 4332, loss = 108.92292465\n",
      "Iteration 4333, loss = 108.92221060\n",
      "Iteration 4334, loss = 108.92219303\n",
      "Iteration 4335, loss = 108.92263649\n",
      "Iteration 4336, loss = 108.92016349\n",
      "Iteration 4337, loss = 108.91932260\n",
      "Iteration 4338, loss = 108.92242468\n",
      "Iteration 4339, loss = 108.91934180\n",
      "Iteration 4340, loss = 108.92162228\n",
      "Iteration 4341, loss = 108.92312807\n",
      "Iteration 4342, loss = 108.92279269\n",
      "Iteration 4343, loss = 108.92062995\n",
      "Iteration 4344, loss = 108.91669849\n",
      "Iteration 4345, loss = 108.92169849\n",
      "Iteration 4346, loss = 108.92314592\n",
      "Iteration 4347, loss = 108.91863067\n",
      "Iteration 4348, loss = 108.91660788\n",
      "Iteration 4349, loss = 108.91898078\n",
      "Iteration 4350, loss = 108.91907362\n",
      "Iteration 4351, loss = 108.91740073\n",
      "Iteration 4352, loss = 108.91419096\n",
      "Iteration 4353, loss = 108.91416397\n",
      "Iteration 4354, loss = 108.91469707\n",
      "Iteration 4355, loss = 108.91020154\n",
      "Iteration 4356, loss = 108.91037565\n",
      "Iteration 4357, loss = 108.90927959\n",
      "Iteration 4358, loss = 108.91047721\n",
      "Iteration 4359, loss = 108.91013398\n",
      "Iteration 4360, loss = 108.90797432\n",
      "Iteration 4361, loss = 108.91144376\n",
      "Iteration 4362, loss = 108.91022906\n",
      "Iteration 4363, loss = 108.90729512\n",
      "Iteration 4364, loss = 108.90829207\n",
      "Iteration 4365, loss = 108.90734360\n",
      "Iteration 4366, loss = 108.90472940\n",
      "Iteration 4367, loss = 108.90972955\n",
      "Iteration 4368, loss = 108.90934905\n",
      "Iteration 4369, loss = 108.90304672\n",
      "Iteration 4370, loss = 108.90854611\n",
      "Iteration 4371, loss = 108.91181403\n",
      "Iteration 4372, loss = 108.91263493\n",
      "Iteration 4373, loss = 108.91125674\n",
      "Iteration 4374, loss = 108.90799270\n",
      "Iteration 4375, loss = 108.90308874\n",
      "Iteration 4376, loss = 108.90331769\n",
      "Iteration 4377, loss = 108.90593756\n",
      "Iteration 4378, loss = 108.90233177\n",
      "Iteration 4379, loss = 108.90005135\n",
      "Iteration 4380, loss = 108.90202153\n",
      "Iteration 4381, loss = 108.90191991\n",
      "Iteration 4382, loss = 108.90002822\n",
      "Iteration 4383, loss = 108.89642575\n",
      "Iteration 4384, loss = 108.90058308\n",
      "Iteration 4385, loss = 108.90134694\n",
      "Iteration 4386, loss = 108.89554850\n",
      "Iteration 4387, loss = 108.89787204\n",
      "Iteration 4388, loss = 108.90095058\n",
      "Iteration 4389, loss = 108.90146653\n",
      "Iteration 4390, loss = 108.89954248\n",
      "Iteration 4391, loss = 108.89675702\n",
      "Iteration 4392, loss = 108.89251042\n",
      "Iteration 4393, loss = 108.89783428\n",
      "Iteration 4394, loss = 108.89976950\n",
      "Iteration 4395, loss = 108.89548034\n",
      "Iteration 4396, loss = 108.88994752\n",
      "Iteration 4397, loss = 108.89208099\n",
      "Iteration 4398, loss = 108.89182463\n",
      "Iteration 4399, loss = 108.89055687\n",
      "Iteration 4400, loss = 108.88768608\n",
      "Iteration 4401, loss = 108.89156595\n",
      "Iteration 4402, loss = 108.89127784\n",
      "Iteration 4403, loss = 108.88600150\n",
      "Iteration 4404, loss = 108.88981805\n",
      "Iteration 4405, loss = 108.89281466\n",
      "Iteration 4406, loss = 108.89293159\n",
      "Iteration 4407, loss = 108.89089000\n",
      "Iteration 4408, loss = 108.88795983\n",
      "Iteration 4409, loss = 108.88315466\n",
      "Iteration 4410, loss = 108.88700121\n",
      "Iteration 4411, loss = 108.88960952\n",
      "Iteration 4412, loss = 108.88517827\n",
      "Iteration 4413, loss = 108.88026799\n",
      "Iteration 4414, loss = 108.88241100\n",
      "Iteration 4415, loss = 108.88237671\n",
      "Iteration 4416, loss = 108.88089979\n",
      "Iteration 4417, loss = 108.87736333\n",
      "Iteration 4418, loss = 108.88239710\n",
      "Iteration 4419, loss = 108.88301058\n",
      "Iteration 4420, loss = 108.87690321\n",
      "Iteration 4421, loss = 108.87945496\n",
      "Iteration 4422, loss = 108.88270559\n",
      "Iteration 4423, loss = 108.88303520\n",
      "Iteration 4424, loss = 108.88151791\n",
      "Iteration 4425, loss = 108.87846884\n",
      "Iteration 4426, loss = 108.87348582\n",
      "Iteration 4427, loss = 108.87839656\n",
      "Iteration 4428, loss = 108.88111245\n",
      "Iteration 4429, loss = 108.87647934\n",
      "Iteration 4430, loss = 108.87138108\n",
      "Iteration 4431, loss = 108.87387326\n",
      "Iteration 4432, loss = 108.87373213\n",
      "Iteration 4433, loss = 108.87210605\n",
      "Iteration 4434, loss = 108.86911641\n",
      "Iteration 4435, loss = 108.87209118\n",
      "Iteration 4436, loss = 108.87288117\n",
      "Iteration 4437, loss = 108.86743479\n",
      "Iteration 4438, loss = 108.87109687\n",
      "Iteration 4439, loss = 108.87365904\n",
      "Iteration 4440, loss = 108.87452149\n",
      "Iteration 4441, loss = 108.87259435\n",
      "Iteration 4442, loss = 108.86951089\n",
      "Iteration 4443, loss = 108.86497850\n",
      "Iteration 4444, loss = 108.86764916\n",
      "Iteration 4445, loss = 108.87052683\n",
      "Iteration 4446, loss = 108.86608635\n",
      "Iteration 4447, loss = 108.86310103\n",
      "Iteration 4448, loss = 108.86538128\n",
      "Iteration 4449, loss = 108.86507032\n",
      "Iteration 4450, loss = 108.86406032\n",
      "Iteration 4451, loss = 108.86119098\n",
      "Iteration 4452, loss = 108.86277428\n",
      "Iteration 4453, loss = 108.86220607\n",
      "Iteration 4454, loss = 108.85668743\n",
      "Iteration 4455, loss = 108.85688004\n",
      "Iteration 4456, loss = 108.85546878\n",
      "Iteration 4457, loss = 108.85909434\n",
      "Iteration 4458, loss = 108.85646532\n",
      "Iteration 4459, loss = 108.85686788\n",
      "Iteration 4460, loss = 108.85930481\n",
      "Iteration 4461, loss = 108.85906743\n",
      "Iteration 4462, loss = 108.85632515\n",
      "Iteration 4463, loss = 108.85175416\n",
      "Iteration 4464, loss = 108.85946684\n",
      "Iteration 4465, loss = 108.86109783\n",
      "Iteration 4466, loss = 108.85550023\n",
      "Iteration 4467, loss = 108.85277884\n",
      "Iteration 4468, loss = 108.85499081\n",
      "Iteration 4469, loss = 108.85620620\n",
      "Iteration 4470, loss = 108.85411811\n",
      "Iteration 4471, loss = 108.85092885\n",
      "Iteration 4472, loss = 108.84671242\n",
      "Iteration 4473, loss = 108.85543897\n",
      "Iteration 4474, loss = 108.85709784\n",
      "Iteration 4475, loss = 108.85341745\n",
      "Iteration 4476, loss = 108.84442305\n",
      "Iteration 4477, loss = 108.84594608\n",
      "Iteration 4478, loss = 108.84800116\n",
      "Iteration 4479, loss = 108.84697652\n",
      "Iteration 4480, loss = 108.84306784\n",
      "Iteration 4481, loss = 108.84712344\n",
      "Iteration 4482, loss = 108.84834890\n",
      "Iteration 4483, loss = 108.84281548\n",
      "Iteration 4484, loss = 108.84544463\n",
      "Iteration 4485, loss = 108.84796480\n",
      "Iteration 4486, loss = 108.84789929\n",
      "Iteration 4487, loss = 108.84698390\n",
      "Iteration 4488, loss = 108.84355142\n",
      "Iteration 4489, loss = 108.83818142\n",
      "Iteration 4490, loss = 108.84141371\n",
      "Iteration 4491, loss = 108.84394290\n",
      "Iteration 4492, loss = 108.83927981\n",
      "Iteration 4493, loss = 108.83712725\n",
      "Iteration 4494, loss = 108.83955715\n",
      "Iteration 4495, loss = 108.83862195\n",
      "Iteration 4496, loss = 108.83730883\n",
      "Iteration 4497, loss = 108.83475164\n",
      "Iteration 4498, loss = 108.83567417\n",
      "Iteration 4499, loss = 108.83578345\n",
      "Iteration 4500, loss = 108.83215996\n",
      "Iteration 4501, loss = 108.83287636\n",
      "Iteration 4502, loss = 108.83068935\n",
      "Iteration 4503, loss = 108.83246848\n",
      "Iteration 4504, loss = 108.82836272\n",
      "Iteration 4505, loss = 108.83059190\n",
      "Iteration 4506, loss = 108.83214727\n",
      "Iteration 4507, loss = 108.83162317\n",
      "Iteration 4508, loss = 108.83042254\n",
      "Iteration 4509, loss = 108.82642435\n",
      "Iteration 4510, loss = 108.83175070\n",
      "Iteration 4511, loss = 108.83291138\n",
      "Iteration 4512, loss = 108.82740345\n",
      "Iteration 4513, loss = 108.82637861\n",
      "Iteration 4514, loss = 108.83066410\n",
      "Iteration 4515, loss = 108.83232790\n",
      "Iteration 4516, loss = 108.83048124\n",
      "Iteration 4517, loss = 108.82703523\n",
      "Iteration 4518, loss = 108.82148964\n",
      "Iteration 4519, loss = 108.82857298\n",
      "Iteration 4520, loss = 108.83181279\n",
      "Iteration 4521, loss = 108.82783997\n",
      "Iteration 4522, loss = 108.82127896\n",
      "Iteration 4523, loss = 108.82264472\n",
      "Iteration 4524, loss = 108.82298234\n",
      "Iteration 4525, loss = 108.82196853\n",
      "Iteration 4526, loss = 108.81926629\n",
      "Iteration 4527, loss = 108.82013282\n",
      "Iteration 4528, loss = 108.81841799\n",
      "Iteration 4529, loss = 108.81623517\n",
      "Iteration 4530, loss = 108.81789443\n",
      "Iteration 4531, loss = 108.81622867\n",
      "Iteration 4532, loss = 108.81681965\n",
      "Iteration 4533, loss = 108.81319514\n",
      "Iteration 4534, loss = 108.81640816\n",
      "Iteration 4535, loss = 108.81833417\n",
      "Iteration 4536, loss = 108.81837282\n",
      "Iteration 4537, loss = 108.81533095\n",
      "Iteration 4538, loss = 108.81056003\n",
      "Iteration 4539, loss = 108.81503149\n",
      "Iteration 4540, loss = 108.81674611\n",
      "Iteration 4541, loss = 108.81224662\n",
      "Iteration 4542, loss = 108.81278871\n",
      "Iteration 4543, loss = 108.81347466\n",
      "Iteration 4544, loss = 108.81638111\n",
      "Iteration 4545, loss = 108.81649861\n",
      "Iteration 4546, loss = 108.81336057\n",
      "Iteration 4547, loss = 108.80750379\n",
      "Iteration 4548, loss = 108.81330118\n",
      "Iteration 4549, loss = 108.81523530\n",
      "Iteration 4550, loss = 108.81160093\n",
      "Iteration 4551, loss = 108.80618763\n",
      "Iteration 4552, loss = 108.80671000\n",
      "Iteration 4553, loss = 108.80772290\n",
      "Iteration 4554, loss = 108.80668739\n",
      "Iteration 4555, loss = 108.80290082\n",
      "Iteration 4556, loss = 108.80183820\n",
      "Iteration 4557, loss = 108.80344732\n",
      "Iteration 4558, loss = 108.80170928\n",
      "Iteration 4559, loss = 108.80199346\n",
      "Iteration 4560, loss = 108.79992486\n",
      "Iteration 4561, loss = 108.79791991\n",
      "Iteration 4562, loss = 108.79811327\n",
      "Iteration 4563, loss = 108.79684795\n",
      "Iteration 4564, loss = 108.79862007\n",
      "Iteration 4565, loss = 108.79709741\n",
      "Iteration 4566, loss = 108.80003453\n",
      "Iteration 4567, loss = 108.80092999\n",
      "Iteration 4568, loss = 108.80096652\n",
      "Iteration 4569, loss = 108.79957971\n",
      "Iteration 4570, loss = 108.79649056\n",
      "Iteration 4571, loss = 108.79741063\n",
      "Iteration 4572, loss = 108.79833757\n",
      "Iteration 4573, loss = 108.79328533\n",
      "Iteration 4574, loss = 108.79800730\n",
      "Iteration 4575, loss = 108.80019679\n",
      "Iteration 4576, loss = 108.79885013\n",
      "Iteration 4577, loss = 108.79931073\n",
      "Iteration 4578, loss = 108.79674709\n",
      "Iteration 4579, loss = 108.79224878\n",
      "Iteration 4580, loss = 108.79445966\n",
      "Iteration 4581, loss = 108.79530254\n",
      "Iteration 4582, loss = 108.79144626\n",
      "Iteration 4583, loss = 108.79031903\n",
      "Iteration 4584, loss = 108.79372000\n",
      "Iteration 4585, loss = 108.79373976\n",
      "Iteration 4586, loss = 108.79053870\n",
      "Iteration 4587, loss = 108.78832071\n",
      "Iteration 4588, loss = 108.78600936\n",
      "Iteration 4589, loss = 108.78546279\n",
      "Iteration 4590, loss = 108.78612521\n",
      "Iteration 4591, loss = 108.78572804\n",
      "Iteration 4592, loss = 108.78469942\n",
      "Iteration 4593, loss = 108.78083035\n",
      "Iteration 4594, loss = 108.78590674\n",
      "Iteration 4595, loss = 108.78521743\n",
      "Iteration 4596, loss = 108.77912557\n",
      "Iteration 4597, loss = 108.78162390\n",
      "Iteration 4598, loss = 108.78106713\n",
      "Iteration 4599, loss = 108.77910738\n",
      "Iteration 4600, loss = 108.77908051\n",
      "Iteration 4601, loss = 108.77850713\n",
      "Iteration 4602, loss = 108.77879699\n",
      "Iteration 4603, loss = 108.77786873\n",
      "Iteration 4604, loss = 108.77612113\n",
      "Iteration 4605, loss = 108.77712495\n",
      "Iteration 4606, loss = 108.77613574\n",
      "Iteration 4607, loss = 108.77578513\n",
      "Iteration 4608, loss = 108.77553205\n",
      "Iteration 4609, loss = 108.77338285\n",
      "Iteration 4610, loss = 108.77340510\n",
      "Iteration 4611, loss = 108.77371184\n",
      "Iteration 4612, loss = 108.77118297\n",
      "Iteration 4613, loss = 108.77190388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4614, loss = 108.77301005\n",
      "Iteration 4615, loss = 108.77084659\n",
      "Iteration 4616, loss = 108.77094053\n",
      "Iteration 4617, loss = 108.77251894\n",
      "Iteration 4618, loss = 108.76882287\n",
      "Iteration 4619, loss = 108.76811029\n",
      "Iteration 4620, loss = 108.76973450\n",
      "Iteration 4621, loss = 108.76724126\n",
      "Iteration 4622, loss = 108.76749186\n",
      "Iteration 4623, loss = 108.76893618\n",
      "Iteration 4624, loss = 108.76594772\n",
      "Iteration 4625, loss = 108.76633294\n",
      "Iteration 4626, loss = 108.76874201\n",
      "Iteration 4627, loss = 108.76423827\n",
      "Iteration 4628, loss = 108.76401156\n",
      "Iteration 4629, loss = 108.76700586\n",
      "Iteration 4630, loss = 108.76411865\n",
      "Iteration 4631, loss = 108.76247649\n",
      "Iteration 4632, loss = 108.76418366\n",
      "Iteration 4633, loss = 108.76187763\n",
      "Iteration 4634, loss = 108.76129835\n",
      "Iteration 4635, loss = 108.76249103\n",
      "Iteration 4636, loss = 108.76005747\n",
      "Iteration 4637, loss = 108.75995163\n",
      "Iteration 4638, loss = 108.76180793\n",
      "Iteration 4639, loss = 108.75825434\n",
      "Iteration 4640, loss = 108.75734196\n",
      "Iteration 4641, loss = 108.75974387\n",
      "Iteration 4642, loss = 108.75777323\n",
      "Iteration 4643, loss = 108.75688664\n",
      "Iteration 4644, loss = 108.75702961\n",
      "Iteration 4645, loss = 108.75533651\n",
      "Iteration 4646, loss = 108.75455913\n",
      "Iteration 4647, loss = 108.75625601\n",
      "Iteration 4648, loss = 108.75510434\n",
      "Iteration 4649, loss = 108.75369011\n",
      "Iteration 4650, loss = 108.75489878\n",
      "Iteration 4651, loss = 108.75398100\n",
      "Iteration 4652, loss = 108.75244223\n",
      "Iteration 4653, loss = 108.75311497\n",
      "Iteration 4654, loss = 108.75268133\n",
      "Iteration 4655, loss = 108.75245829\n",
      "Iteration 4656, loss = 108.75129271\n",
      "Iteration 4657, loss = 108.75051380\n",
      "Iteration 4658, loss = 108.75059281\n",
      "Iteration 4659, loss = 108.74992775\n",
      "Iteration 4660, loss = 108.74873470\n",
      "Iteration 4661, loss = 108.74783796\n",
      "Iteration 4662, loss = 108.74707678\n",
      "Iteration 4663, loss = 108.74727039\n",
      "Iteration 4664, loss = 108.74743671\n",
      "Iteration 4665, loss = 108.74704709\n",
      "Iteration 4666, loss = 108.74729375\n",
      "Iteration 4667, loss = 108.74612624\n",
      "Iteration 4668, loss = 108.74468891\n",
      "Iteration 4669, loss = 108.74542466\n",
      "Iteration 4670, loss = 108.74510815\n",
      "Iteration 4671, loss = 108.74422864\n",
      "Iteration 4672, loss = 108.74968723\n",
      "Iteration 4673, loss = 108.74685161\n",
      "Iteration 4674, loss = 108.74339520\n",
      "Iteration 4675, loss = 108.74540731\n",
      "Iteration 4676, loss = 108.74499862\n",
      "Iteration 4677, loss = 108.74184864\n",
      "Iteration 4678, loss = 108.74240055\n",
      "Iteration 4679, loss = 108.74167003\n",
      "Iteration 4680, loss = 108.74176073\n",
      "Iteration 4681, loss = 108.74224571\n",
      "Iteration 4682, loss = 108.74152376\n",
      "Iteration 4683, loss = 108.73968475\n",
      "Iteration 4684, loss = 108.73873451\n",
      "Iteration 4685, loss = 108.73665049\n",
      "Iteration 4686, loss = 108.73889644\n",
      "Iteration 4687, loss = 108.73974215\n",
      "Iteration 4688, loss = 108.73939742\n",
      "Iteration 4689, loss = 108.73726020\n",
      "Iteration 4690, loss = 108.73604114\n",
      "Iteration 4691, loss = 108.73460971\n",
      "Iteration 4692, loss = 108.73683945\n",
      "Iteration 4693, loss = 108.73804567\n",
      "Iteration 4694, loss = 108.73611829\n",
      "Iteration 4695, loss = 108.73456639\n",
      "Iteration 4696, loss = 108.73579733\n",
      "Iteration 4697, loss = 108.73514045\n",
      "Iteration 4698, loss = 108.73471255\n",
      "Iteration 4699, loss = 108.73442311\n",
      "Iteration 4700, loss = 108.73470309\n",
      "Iteration 4701, loss = 108.73337992\n",
      "Iteration 4702, loss = 108.73143075\n",
      "Iteration 4703, loss = 108.72896043\n",
      "Iteration 4704, loss = 108.73092127\n",
      "Iteration 4705, loss = 108.73214085\n",
      "Iteration 4706, loss = 108.73108915\n",
      "Iteration 4707, loss = 108.72941530\n",
      "Iteration 4708, loss = 108.72684265\n",
      "Iteration 4709, loss = 108.72525540\n",
      "Iteration 4710, loss = 108.72710863\n",
      "Iteration 4711, loss = 108.72554343\n",
      "Iteration 4712, loss = 108.72545923\n",
      "Iteration 4713, loss = 108.72477261\n",
      "Iteration 4714, loss = 108.72844214\n",
      "Iteration 4715, loss = 108.72540505\n",
      "Iteration 4716, loss = 108.72457078\n",
      "Iteration 4717, loss = 108.72623887\n",
      "Iteration 4718, loss = 108.72655960\n",
      "Iteration 4719, loss = 108.72467913\n",
      "Iteration 4720, loss = 108.72238230\n",
      "Iteration 4721, loss = 108.72764828\n",
      "Iteration 4722, loss = 108.72752948\n",
      "Iteration 4723, loss = 108.71967688\n",
      "Iteration 4724, loss = 108.72346199\n",
      "Iteration 4725, loss = 108.72669677\n",
      "Iteration 4726, loss = 108.72826883\n",
      "Iteration 4727, loss = 108.72703441\n",
      "Iteration 4728, loss = 108.72358953\n",
      "Iteration 4729, loss = 108.71985534\n",
      "Iteration 4730, loss = 108.71855803\n",
      "Iteration 4731, loss = 108.72081708\n",
      "Iteration 4732, loss = 108.71494770\n",
      "Iteration 4733, loss = 108.71515765\n",
      "Iteration 4734, loss = 108.71536195\n",
      "Iteration 4735, loss = 108.71615454\n",
      "Iteration 4736, loss = 108.71529358\n",
      "Iteration 4737, loss = 108.71488133\n",
      "Iteration 4738, loss = 108.71718927\n",
      "Iteration 4739, loss = 108.71541704\n",
      "Iteration 4740, loss = 108.71468748\n",
      "Iteration 4741, loss = 108.71292668\n",
      "Iteration 4742, loss = 108.71418694\n",
      "Iteration 4743, loss = 108.71348619\n",
      "Iteration 4744, loss = 108.71161433\n",
      "Iteration 4745, loss = 108.71698284\n",
      "Iteration 4746, loss = 108.71477417\n",
      "Iteration 4747, loss = 108.70979967\n",
      "Iteration 4748, loss = 108.71111557\n",
      "Iteration 4749, loss = 108.70946077\n",
      "Iteration 4750, loss = 108.70796383\n",
      "Iteration 4751, loss = 108.71405760\n",
      "Iteration 4752, loss = 108.71264299\n",
      "Iteration 4753, loss = 108.70698873\n",
      "Iteration 4754, loss = 108.70876162\n",
      "Iteration 4755, loss = 108.70959008\n",
      "Iteration 4756, loss = 108.70734744\n",
      "Iteration 4757, loss = 108.70865590\n",
      "Iteration 4758, loss = 108.70624891\n",
      "Iteration 4759, loss = 108.70634574\n",
      "Iteration 4760, loss = 108.70850280\n",
      "Iteration 4761, loss = 108.70840517\n",
      "Iteration 4762, loss = 108.70512708\n",
      "Iteration 4763, loss = 108.70196018\n",
      "Iteration 4764, loss = 108.70269323\n",
      "Iteration 4765, loss = 108.70512991\n",
      "Iteration 4766, loss = 108.70295078\n",
      "Iteration 4767, loss = 108.70157672\n",
      "Iteration 4768, loss = 108.70144013\n",
      "Iteration 4769, loss = 108.70512562\n",
      "Iteration 4770, loss = 108.70303441\n",
      "Iteration 4771, loss = 108.70249415\n",
      "Iteration 4772, loss = 108.70308259\n",
      "Iteration 4773, loss = 108.70312176\n",
      "Iteration 4774, loss = 108.70194268\n",
      "Iteration 4775, loss = 108.69824038\n",
      "Iteration 4776, loss = 108.70245967\n",
      "Iteration 4777, loss = 108.70272041\n",
      "Iteration 4778, loss = 108.69705167\n",
      "Iteration 4779, loss = 108.70029258\n",
      "Iteration 4780, loss = 108.70294309\n",
      "Iteration 4781, loss = 108.70407804\n",
      "Iteration 4782, loss = 108.70466152\n",
      "Iteration 4783, loss = 108.70157662\n",
      "Iteration 4784, loss = 108.69651511\n",
      "Iteration 4785, loss = 108.69322180\n",
      "Iteration 4786, loss = 108.69538717\n",
      "Iteration 4787, loss = 108.69259452\n",
      "Iteration 4788, loss = 108.69170643\n",
      "Iteration 4789, loss = 108.69157163\n",
      "Iteration 4790, loss = 108.69411829\n",
      "Iteration 4791, loss = 108.69173485\n",
      "Iteration 4792, loss = 108.69305138\n",
      "Iteration 4793, loss = 108.69506243\n",
      "Iteration 4794, loss = 108.69622656\n",
      "Iteration 4795, loss = 108.69448924\n",
      "Iteration 4796, loss = 108.69211057\n",
      "Iteration 4797, loss = 108.68926158\n",
      "Iteration 4798, loss = 108.68958410\n",
      "Iteration 4799, loss = 108.68975934\n",
      "Iteration 4800, loss = 108.69109366\n",
      "Iteration 4801, loss = 108.68928614\n",
      "Iteration 4802, loss = 108.68539872\n",
      "Iteration 4803, loss = 108.68834451\n",
      "Iteration 4804, loss = 108.68749662\n",
      "Iteration 4805, loss = 108.68552573\n",
      "Iteration 4806, loss = 108.68685899\n",
      "Iteration 4807, loss = 108.68833783\n",
      "Iteration 4808, loss = 108.68624409\n",
      "Iteration 4809, loss = 108.68268605\n",
      "Iteration 4810, loss = 108.68155066\n",
      "Iteration 4811, loss = 108.68190247\n",
      "Iteration 4812, loss = 108.68243646\n",
      "Iteration 4813, loss = 108.68336040\n",
      "Iteration 4814, loss = 108.68142676\n",
      "Iteration 4815, loss = 108.67943277\n",
      "Iteration 4816, loss = 108.67894814\n",
      "Iteration 4817, loss = 108.67836484\n",
      "Iteration 4818, loss = 108.68138887\n",
      "Iteration 4819, loss = 108.67854431\n",
      "Iteration 4820, loss = 108.68137221\n",
      "Iteration 4821, loss = 108.68357914\n",
      "Iteration 4822, loss = 108.68454179\n",
      "Iteration 4823, loss = 108.68251054\n",
      "Iteration 4824, loss = 108.67926434\n",
      "Iteration 4825, loss = 108.67715334\n",
      "Iteration 4826, loss = 108.67764395\n",
      "Iteration 4827, loss = 108.67754726\n",
      "Iteration 4828, loss = 108.67810561\n",
      "Iteration 4829, loss = 108.67828431\n",
      "Iteration 4830, loss = 108.67609509\n",
      "Iteration 4831, loss = 108.67673174\n",
      "Iteration 4832, loss = 108.67531821\n",
      "Iteration 4833, loss = 108.67316688\n",
      "Iteration 4834, loss = 108.67497853\n",
      "Iteration 4835, loss = 108.67516500\n",
      "Iteration 4836, loss = 108.67256841\n",
      "Iteration 4837, loss = 108.67201056\n",
      "Iteration 4838, loss = 108.67222635\n",
      "Iteration 4839, loss = 108.67536085\n",
      "Iteration 4840, loss = 108.67592348\n",
      "Iteration 4841, loss = 108.67479503\n",
      "Iteration 4842, loss = 108.67273982\n",
      "Iteration 4843, loss = 108.66951662\n",
      "Iteration 4844, loss = 108.67234685\n",
      "Iteration 4845, loss = 108.67391696\n",
      "Iteration 4846, loss = 108.66756245\n",
      "Iteration 4847, loss = 108.67200054\n",
      "Iteration 4848, loss = 108.67464003\n",
      "Iteration 4849, loss = 108.67766810\n",
      "Iteration 4850, loss = 108.67756116\n",
      "Iteration 4851, loss = 108.67438578\n",
      "Iteration 4852, loss = 108.66896979\n",
      "Iteration 4853, loss = 108.66535989\n",
      "Iteration 4854, loss = 108.67442479\n",
      "Iteration 4855, loss = 108.67863243\n",
      "Iteration 4856, loss = 108.67362450\n",
      "Iteration 4857, loss = 108.66335575\n",
      "Iteration 4858, loss = 108.66651947\n",
      "Iteration 4859, loss = 108.66841210\n",
      "Iteration 4860, loss = 108.66757141\n",
      "Iteration 4861, loss = 108.66397752\n",
      "Iteration 4862, loss = 108.66042594\n",
      "Iteration 4863, loss = 108.66197986\n",
      "Iteration 4864, loss = 108.66395364\n",
      "Iteration 4865, loss = 108.66190793\n",
      "Iteration 4866, loss = 108.66050262\n",
      "Iteration 4867, loss = 108.65992815\n",
      "Iteration 4868, loss = 108.65925928\n",
      "Iteration 4869, loss = 108.65845493\n",
      "Iteration 4870, loss = 108.65862105\n",
      "Iteration 4871, loss = 108.65977245\n",
      "Iteration 4872, loss = 108.65966126\n",
      "Iteration 4873, loss = 108.65913298\n",
      "Iteration 4874, loss = 108.65698649\n",
      "Iteration 4875, loss = 108.65967002\n",
      "Iteration 4876, loss = 108.65778184\n",
      "Iteration 4877, loss = 108.65572351\n",
      "Iteration 4878, loss = 108.65665615\n",
      "Iteration 4879, loss = 108.65645763\n",
      "Iteration 4880, loss = 108.65527983\n",
      "Iteration 4881, loss = 108.65421281\n",
      "Iteration 4882, loss = 108.65230510\n",
      "Iteration 4883, loss = 108.65517353\n",
      "Iteration 4884, loss = 108.65577091\n",
      "Iteration 4885, loss = 108.65721020\n",
      "Iteration 4886, loss = 108.65582644\n",
      "Iteration 4887, loss = 108.65152889\n",
      "Iteration 4888, loss = 108.65643081\n",
      "Iteration 4889, loss = 108.65749533\n",
      "Iteration 4890, loss = 108.65055470\n",
      "Iteration 4891, loss = 108.65364014\n",
      "Iteration 4892, loss = 108.65825399\n",
      "Iteration 4893, loss = 108.65987985\n",
      "Iteration 4894, loss = 108.65903276\n",
      "Iteration 4895, loss = 108.65609561\n",
      "Iteration 4896, loss = 108.65245259\n",
      "Iteration 4897, loss = 108.64734139\n",
      "Iteration 4898, loss = 108.65566304\n",
      "Iteration 4899, loss = 108.65825854\n",
      "Iteration 4900, loss = 108.65351063\n",
      "Iteration 4901, loss = 108.64616219\n",
      "Iteration 4902, loss = 108.64842384\n",
      "Iteration 4903, loss = 108.64944504\n",
      "Iteration 4904, loss = 108.64899785\n",
      "Iteration 4905, loss = 108.64628465\n",
      "Iteration 4906, loss = 108.64328529\n",
      "Iteration 4907, loss = 108.65136476\n",
      "Iteration 4908, loss = 108.65110991\n",
      "Iteration 4909, loss = 108.64500726\n",
      "Iteration 4910, loss = 108.64730421\n",
      "Iteration 4911, loss = 108.65097408\n",
      "Iteration 4912, loss = 108.65131509\n",
      "Iteration 4913, loss = 108.65058581\n",
      "Iteration 4914, loss = 108.64921851\n",
      "Iteration 4915, loss = 108.64467333\n",
      "Iteration 4916, loss = 108.64064801\n",
      "Iteration 4917, loss = 108.64875849\n",
      "Iteration 4918, loss = 108.65171728\n",
      "Iteration 4919, loss = 108.64593953\n",
      "Iteration 4920, loss = 108.63961889\n",
      "Iteration 4921, loss = 108.64325314\n",
      "Iteration 4922, loss = 108.64379666\n",
      "Iteration 4923, loss = 108.64139986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4924, loss = 108.63944601\n",
      "Iteration 4925, loss = 108.63646192\n",
      "Iteration 4926, loss = 108.64284305\n",
      "Iteration 4927, loss = 108.64366242\n",
      "Iteration 4928, loss = 108.63620842\n",
      "Iteration 4929, loss = 108.63882006\n",
      "Iteration 4930, loss = 108.64274345\n",
      "Iteration 4931, loss = 108.64327408\n",
      "Iteration 4932, loss = 108.64410216\n",
      "Iteration 4933, loss = 108.64237043\n",
      "Iteration 4934, loss = 108.63755388\n",
      "Iteration 4935, loss = 108.63325101\n",
      "Iteration 4936, loss = 108.64427787\n",
      "Iteration 4937, loss = 108.64758794\n",
      "Iteration 4938, loss = 108.64153553\n",
      "Iteration 4939, loss = 108.63234572\n",
      "Iteration 4940, loss = 108.63593641\n",
      "Iteration 4941, loss = 108.63724800\n",
      "Iteration 4942, loss = 108.63752600\n",
      "Iteration 4943, loss = 108.63549263\n",
      "Iteration 4944, loss = 108.63114343\n",
      "Iteration 4945, loss = 108.63518000\n",
      "Iteration 4946, loss = 108.63729170\n",
      "Iteration 4947, loss = 108.63093805\n",
      "Iteration 4948, loss = 108.63280201\n",
      "Iteration 4949, loss = 108.63537972\n",
      "Iteration 4950, loss = 108.63685970\n",
      "Iteration 4951, loss = 108.63719533\n",
      "Iteration 4952, loss = 108.63520058\n",
      "Iteration 4953, loss = 108.63048607\n",
      "Iteration 4954, loss = 108.62747255\n",
      "Iteration 4955, loss = 108.63528702\n",
      "Iteration 4956, loss = 108.63725430\n",
      "Iteration 4957, loss = 108.63314422\n",
      "Iteration 4958, loss = 108.62675621\n",
      "Iteration 4959, loss = 108.63090661\n",
      "Iteration 4960, loss = 108.63222681\n",
      "Iteration 4961, loss = 108.63166080\n",
      "Iteration 4962, loss = 108.62812100\n",
      "Iteration 4963, loss = 108.62336855\n",
      "Iteration 4964, loss = 108.62917607\n",
      "Iteration 4965, loss = 108.63029162\n",
      "Iteration 4966, loss = 108.62216618\n",
      "Iteration 4967, loss = 108.62589012\n",
      "Iteration 4968, loss = 108.63015980\n",
      "Iteration 4969, loss = 108.63101620\n",
      "Iteration 4970, loss = 108.62977951\n",
      "Iteration 4971, loss = 108.62948381\n",
      "Iteration 4972, loss = 108.62589404\n",
      "Iteration 4973, loss = 108.62106926\n",
      "Iteration 4974, loss = 108.62592021\n",
      "Iteration 4975, loss = 108.62881035\n",
      "Iteration 4976, loss = 108.62346812\n",
      "Iteration 4977, loss = 108.61943192\n",
      "Iteration 4978, loss = 108.62168672\n",
      "Iteration 4979, loss = 108.62301984\n",
      "Iteration 4980, loss = 108.62280918\n",
      "Iteration 4981, loss = 108.61984019\n",
      "Iteration 4982, loss = 108.61818079\n",
      "Iteration 4983, loss = 108.62054084\n",
      "Iteration 4984, loss = 108.61995188\n",
      "Iteration 4985, loss = 108.61520404\n",
      "Iteration 4986, loss = 108.61844541\n",
      "Iteration 4987, loss = 108.61865988\n",
      "Iteration 4988, loss = 108.61598434\n",
      "Iteration 4989, loss = 108.61797095\n",
      "Iteration 4990, loss = 108.61653134\n",
      "Iteration 4991, loss = 108.61555025\n",
      "Iteration 4992, loss = 108.61742655\n",
      "Iteration 4993, loss = 108.61783897\n",
      "Iteration 4994, loss = 108.61549705\n",
      "Iteration 4995, loss = 108.61158345\n",
      "Iteration 4996, loss = 108.61596559\n",
      "Iteration 4997, loss = 108.61546286\n",
      "Iteration 4998, loss = 108.61145786\n",
      "Iteration 4999, loss = 108.61237650\n",
      "Iteration 5000, loss = 108.61225818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\master4\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.1, batch_size='auto', beta_1=0.9,\n",
       "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "             hidden_layer_sizes=(100, 10), learning_rate='constant',\n",
       "             learning_rate_init=0.001, max_fun=15000, max_iter=5000,\n",
       "             momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "             power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "             tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelNN.fit( car_df[['speed']], car_df['dist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-1.48824984e-095, -3.98061124e-002, -4.36043589e-002,\n",
       "         -4.31388607e-002,  1.94100825e-001,  2.43646363e-001,\n",
       "          2.95218158e-001,  2.80656453e-001,  3.28137825e-001,\n",
       "          2.25644363e-001,  1.87812213e-001,  2.15932224e-001,\n",
       "          3.12937790e-001,  2.43148816e-001,  4.12145580e-001,\n",
       "          1.50645304e-110, -4.38676163e-002,  1.71205397e-002,\n",
       "         -2.06161296e-001,  1.85923540e-001,  1.36294534e-096,\n",
       "         -9.42806080e-117,  1.82827523e-001, -5.26070533e-117,\n",
       "          1.01162334e-001,  2.17132427e-001, -1.94016272e-001,\n",
       "          3.53396580e-001,  2.87789621e-001, -3.86381498e-002,\n",
       "          2.84242911e-001, -4.05835838e-002, -2.10344433e-096,\n",
       "         -8.14356915e-003, -4.26938695e-002,  5.30803736e-100,\n",
       "         -9.93346421e-093,  1.23415721e-109,  1.58410956e-112,\n",
       "          2.11873930e-001, -6.67656443e-002,  2.10394890e-001,\n",
       "         -2.75856532e-003,  1.85974858e-001,  1.60244593e-001,\n",
       "         -1.10012583e-115, -7.12251569e-095,  1.54207370e-001,\n",
       "         -1.01436649e-002, -8.16257360e-097,  2.25426655e-102,\n",
       "          4.01502736e-106,  7.42606500e-004,  7.56722471e-002,\n",
       "          1.87100970e-001, -4.00212971e-002,  1.88600108e-001,\n",
       "          1.58392474e-001, -3.96861641e-002, -1.13590610e-095,\n",
       "          3.19842778e-001, -4.51669134e-002, -1.86272110e-001,\n",
       "         -8.25468092e-092, -1.76525304e-094,  1.48673873e-113,\n",
       "         -2.34560531e-116,  3.23120192e-001, -7.80557866e-117,\n",
       "          9.77097908e-098, -4.14290361e-002, -6.33324034e-117,\n",
       "          1.82599024e-001,  2.36867985e-001,  4.62030201e-116,\n",
       "          1.78309682e-001, -1.96472703e-001, -1.65085795e-103,\n",
       "         -3.03689615e-094,  2.39554007e-001,  2.44939079e-001,\n",
       "          3.17737343e-001,  3.02726344e-001, -6.36763562e-002,\n",
       "          3.33412399e-095, -1.12857858e-101,  4.72484131e-115,\n",
       "          3.78323610e-001,  2.93887061e-001,  1.31066748e-001,\n",
       "          2.14410243e-001, -3.95083304e-002,  1.38007839e-001,\n",
       "          1.23667275e-001,  1.94896300e-001,  1.91694897e-001,\n",
       "          2.40292880e-001,  3.56946385e-001, -1.52500005e-001,\n",
       "          1.33578499e-001]]),\n",
       " array([[-6.56683184e-115,  7.86850791e-116,  1.01542539e-112,\n",
       "         -9.50923396e-103,  2.10188893e-110,  9.21479685e-097,\n",
       "         -2.51207507e-093,  1.42220174e-109,  1.03810248e-115,\n",
       "          2.02574233e-116],\n",
       "        [-1.12865993e+000,  1.09412968e-112,  2.57663096e-114,\n",
       "         -1.01105266e+000, -1.03419900e+000,  3.33951630e-097,\n",
       "          1.48435604e-001, -1.02846310e-111,  1.67111817e-108,\n",
       "         -8.74265530e-114],\n",
       "        [-1.02921657e+000,  1.47689134e-102, -8.36891837e-101,\n",
       "         -9.84592018e-001, -9.96481051e-001, -1.26175456e-112,\n",
       "          1.62693025e-001,  6.79026331e-111,  3.20774797e-116,\n",
       "         -4.86090283e-099],\n",
       "        [-1.00778144e+000, -5.65389927e-115, -3.13709078e-113,\n",
       "         -9.71039746e-001, -1.01394142e+000, -9.91555629e-113,\n",
       "          1.60956036e-001, -3.84769544e-102, -2.25163953e-116,\n",
       "         -7.05396629e-116],\n",
       "        [ 3.80350187e-001,  1.18308679e-002, -2.61804459e-001,\n",
       "          2.70050519e-001,  1.93365044e-001, -9.58342943e-110,\n",
       "          4.67493105e-003,  3.67029948e-116,  1.45919130e-096,\n",
       "         -1.64620805e-001],\n",
       "        [ 1.51802043e-001,  3.04505151e-002,  1.44798264e-003,\n",
       "          1.35654976e-001,  1.85074423e-001,  9.89064755e-074,\n",
       "         -4.38115831e-002, -1.50361955e-095,  1.06938721e-100,\n",
       "          1.04441112e-001],\n",
       "        [ 3.00698547e-001,  1.87829158e-001,  4.78298651e-002,\n",
       "          3.61084196e-001,  1.92614233e-001,  2.13289240e-103,\n",
       "         -2.08233551e-002, -3.82296162e-117, -2.78619107e-116,\n",
       "          1.95127608e-001],\n",
       "        [ 3.25104922e-001, -3.83397462e-002,  1.69469250e-001,\n",
       "          2.18912588e-001, -2.89458526e-002, -1.37987497e-098,\n",
       "         -3.40546040e-002,  1.08400924e-104, -9.70159847e-110,\n",
       "          7.63741343e-002],\n",
       "        [ 1.44342007e-001, -2.31727716e-001,  1.80442706e-003,\n",
       "          1.09123389e-001,  1.15582186e-001,  3.85422418e-101,\n",
       "         -6.29325022e-002,  1.16374636e-116,  1.45687784e-113,\n",
       "         -5.75267600e-002],\n",
       "        [ 3.68393391e-001, -2.24116437e-001,  1.79388666e-001,\n",
       "          4.49349855e-002, -1.43726572e-002, -1.38723538e-107,\n",
       "         -3.15462562e-002,  1.72712859e-116, -9.09596011e-117,\n",
       "          3.91506190e-002],\n",
       "        [ 2.57472159e-001,  1.10783183e-001, -3.59247024e-002,\n",
       "          7.99589633e-002, -7.18235716e-002,  7.02861145e-116,\n",
       "         -2.73621472e-002, -3.12126695e-100, -8.30142367e-099,\n",
       "         -1.92114619e-001],\n",
       "        [ 2.57084828e-002,  1.02147415e-001,  7.65308076e-003,\n",
       "          3.04998525e-001, -6.37323335e-002, -2.62488629e-112,\n",
       "         -5.66560588e-002, -9.79244567e-107,  1.67863375e-117,\n",
       "          1.03901015e-001],\n",
       "        [ 1.68291066e-001, -8.05673306e-002, -2.44432755e-001,\n",
       "          2.65198976e-001,  3.12258085e-001, -5.99328919e-117,\n",
       "         -5.52821829e-002,  8.54592000e-117,  1.22790330e-033,\n",
       "          7.33088953e-002],\n",
       "        [ 1.76325018e-001, -1.83881058e-001, -1.63149928e-001,\n",
       "          1.22514958e-001,  1.40904674e-001, -9.43748464e-117,\n",
       "         -5.18984697e-002,  1.18007955e-116, -8.17512866e-094,\n",
       "          1.95182535e-001],\n",
       "        [ 2.66032016e-002, -1.07067052e-001,  8.45352823e-003,\n",
       "          2.04876976e-001,  2.71121599e-001, -1.49154892e-115,\n",
       "         -3.87328059e-002,  7.91398686e-110,  6.65883267e-116,\n",
       "          1.00952318e-001],\n",
       "        [ 5.34464702e-113, -3.56388958e-102, -5.17712323e-098,\n",
       "         -2.41200793e-110,  3.51008792e-102,  1.06938877e-100,\n",
       "          1.53033045e-099,  9.02093662e-117,  2.86440637e-094,\n",
       "         -7.42032812e-115],\n",
       "        [-1.02819067e+000, -3.68819963e-118,  1.02850402e-115,\n",
       "         -9.79194676e-001, -9.84147374e-001, -1.63168797e-112,\n",
       "          1.65090320e-001, -1.19255421e-104, -5.00994027e-107,\n",
       "         -2.45741740e-103],\n",
       "        [-7.45705819e-002,  1.55119491e-001,  1.75567665e-001,\n",
       "          2.73348294e-001,  6.96927618e-002, -1.32769644e-014,\n",
       "         -8.72497746e-113,  3.85782803e-103, -2.83150230e-114,\n",
       "         -8.51888944e-002],\n",
       "        [ 1.36099615e+000,  4.13199442e-106, -7.97657209e-116,\n",
       "          1.09772952e+000,  1.23268965e+000, -2.81412475e-102,\n",
       "          1.65218598e-001,  8.12226397e-101, -1.04858202e-109,\n",
       "         -3.20784086e-102],\n",
       "        [ 2.64241197e-001,  4.85493279e-094,  2.24663552e-109,\n",
       "          2.57621854e-001,  6.91114127e-002,  2.02087294e-099,\n",
       "         -4.74504955e-002,  5.93502668e-115,  2.18302817e-094,\n",
       "          4.68203123e-096],\n",
       "        [ 6.39115586e-116, -2.88330914e-106, -3.69986796e-110,\n",
       "         -5.19319953e-114, -2.34949007e-098,  1.80658576e-106,\n",
       "         -4.86100298e-108, -4.18718421e-113,  3.51215012e-116,\n",
       "          3.71077046e-006],\n",
       "        [ 1.11632918e-110,  2.53724629e-113, -5.84601315e-109,\n",
       "         -4.59552165e-102, -8.10310770e-104,  3.32953240e-053,\n",
       "          1.49694391e-110, -2.12915392e-103, -2.63377285e-100,\n",
       "          3.28466058e-103],\n",
       "        [ 8.99542091e-002,  1.38354815e-001,  9.70080713e-002,\n",
       "          9.52341911e-002,  3.53383069e-001, -3.90715872e-097,\n",
       "         -5.88535383e-002,  2.69828751e-108, -9.45941429e-112,\n",
       "          9.43857228e-002],\n",
       "        [ 1.79070746e-107, -7.88515657e-109,  1.29267748e-116,\n",
       "         -7.46396211e-101, -3.61312282e-094, -2.55187853e-106,\n",
       "          2.60650392e-098, -9.83350593e-115,  6.88979173e-105,\n",
       "          1.66623869e-114],\n",
       "        [ 1.30111526e-002,  1.99396858e-001, -1.12964365e-001,\n",
       "         -4.18693361e-003,  4.72539997e-002, -1.83025729e-113,\n",
       "         -3.93063675e-003, -2.38576941e-102,  1.46507432e-114,\n",
       "          1.44594013e-001],\n",
       "        [ 3.06661596e-001,  1.95383505e-001,  1.13156622e-001,\n",
       "          1.42180947e-001,  1.55554796e-001, -1.99339400e-114,\n",
       "         -5.17642528e-002,  3.74380857e-109,  6.88760447e-103,\n",
       "          6.55928661e-002],\n",
       "        [ 1.35193557e+000, -6.64007889e-112,  2.30766782e-115,\n",
       "          1.03358393e+000,  1.24106171e+000, -7.00445774e-107,\n",
       "          1.48549322e-001, -3.26046769e-099,  4.57294369e-100,\n",
       "          5.62919473e-117],\n",
       "        [ 2.66429733e-001, -3.53356114e-002, -1.08857058e-002,\n",
       "          1.66296906e-001,  3.52468492e-001,  2.00437076e-116,\n",
       "         -2.02900212e-002, -4.31322880e-114, -6.63482317e-117,\n",
       "          4.58238739e-002],\n",
       "        [-4.21179118e-002,  1.19386148e-001, -2.91602959e-002,\n",
       "          5.32537390e-002,  2.84957496e-001,  3.97714791e-106,\n",
       "         -3.81945288e-002,  6.79659739e-099, -1.22825417e-102,\n",
       "         -2.19761692e-001],\n",
       "        [-1.06286883e+000,  3.78514041e-094,  4.42606617e-100,\n",
       "         -1.03575377e+000, -1.09431327e+000, -7.53783710e-111,\n",
       "          1.45131503e-001, -8.24683567e-116,  1.42011811e-106,\n",
       "         -1.99838041e-116],\n",
       "        [ 2.94354845e-001, -1.33775353e-001,  4.22913367e-002,\n",
       "          2.04057574e-001,  2.68831576e-001, -2.59149586e-116,\n",
       "         -3.03058154e-002, -6.01094881e-114,  3.87836723e-115,\n",
       "         -1.78825403e-001],\n",
       "        [-1.03575898e+000, -5.65502862e-116, -3.29807050e-096,\n",
       "         -9.75813244e-001, -1.01407096e+000,  7.99315877e-114,\n",
       "          1.54117085e-001, -3.48217068e-109,  3.43221855e-098,\n",
       "          8.16899668e-103],\n",
       "        [ 1.00236468e-115,  1.18947662e-116, -9.39469381e-101,\n",
       "         -3.30526756e-112, -5.30888278e-110,  3.43155993e-117,\n",
       "         -2.22448525e-108, -3.04053745e-094, -1.37811841e-108,\n",
       "         -5.73790357e-116],\n",
       "        [-5.74989556e-020,  1.62811191e-113,  2.80632181e-116,\n",
       "         -3.60205073e-038,  3.44043132e-027,  1.86308697e-097,\n",
       "         -1.90985264e-112, -7.17852190e-110, -8.61880036e-095,\n",
       "          2.95457661e-097],\n",
       "        [-1.01780863e+000, -6.59962443e-106,  1.93162302e-104,\n",
       "         -9.76181826e-001, -9.98393899e-001,  8.84239839e-013,\n",
       "          1.58904108e-001, -2.90324194e-109,  2.65473709e-116,\n",
       "         -4.78821906e-111],\n",
       "        [ 8.80813669e-111,  7.17042382e-111,  8.77113012e-102,\n",
       "         -2.23668952e-116, -7.32399671e-116, -2.08016076e-099,\n",
       "          1.17941004e-116,  6.94336914e-099,  2.91781546e-114,\n",
       "          6.90608235e-102],\n",
       "        [ 4.53989610e-113, -2.57610303e-115,  1.47117176e-115,\n",
       "          3.19940822e-101,  6.75538063e-111, -2.76399568e-107,\n",
       "          1.62988184e-111,  9.26440957e-113,  3.84577326e-113,\n",
       "          1.00497447e-099],\n",
       "        [ 4.47815931e-112, -4.98673101e-114,  1.11767986e-116,\n",
       "         -9.94857945e-115, -9.24820079e-117, -3.75877974e-112,\n",
       "          4.81008688e-114, -4.51493610e-113, -3.51437528e-114,\n",
       "          3.52118239e-115],\n",
       "        [-3.99122805e-115,  4.71564639e-112, -3.62556910e-116,\n",
       "         -5.53647240e-101,  8.54528274e-111,  6.14000681e-098,\n",
       "          1.65642739e-109,  1.33431828e-116,  2.64460729e-113,\n",
       "         -1.44938866e-116],\n",
       "        [ 3.48387102e-001, -2.33646849e-001, -2.07422299e-001,\n",
       "          1.95754848e-001,  1.66617500e-001,  2.39291659e-096,\n",
       "         -2.34479488e-002,  7.68424491e-104, -7.74392406e-094,\n",
       "          1.07626831e-001],\n",
       "        [-1.41112868e+000,  4.71547352e-036, -1.17521187e-060,\n",
       "         -1.35384132e+000, -1.38725803e+000, -7.61041640e-117,\n",
       "         -8.79284430e-004, -9.44214183e-110,  5.87111769e-101,\n",
       "          1.69542745e-052],\n",
       "        [ 7.41264188e-002, -1.39528768e-001,  2.74808294e-003,\n",
       "          2.87528051e-001,  2.10037106e-001,  1.34810507e-107,\n",
       "         -1.29260488e-002,  6.83306203e-096,  1.04201035e-102,\n",
       "          8.44164121e-003],\n",
       "        [-3.51808448e-002, -1.33354014e-001,  5.67156767e-002,\n",
       "          9.85800684e-002, -1.03454660e-001,  8.41003404e-103,\n",
       "         -4.54962039e-110, -3.78365573e-115,  6.69525126e-096,\n",
       "         -2.10892255e-001],\n",
       "        [ 2.44618289e-001, -1.96503959e-001, -2.08153058e-001,\n",
       "          2.30651735e-002,  1.68789718e-001,  6.31030027e-116,\n",
       "         -3.60575404e-002, -4.17246325e-114,  4.15373583e-112,\n",
       "         -1.65143376e-001],\n",
       "        [-1.02868680e-001, -1.91488330e-001, -8.02846991e-002,\n",
       "          2.75696067e-001,  1.16530880e-001,  2.37271481e-096,\n",
       "         -1.37878064e-002, -2.98968717e-114, -4.12661989e-113,\n",
       "          5.67745591e-003],\n",
       "        [-8.93716826e-107,  6.63700163e-100, -4.20076737e-115,\n",
       "         -2.94950955e-099,  1.29455622e-115,  7.09277779e-115,\n",
       "         -4.20731320e-116, -2.69312887e-095,  1.09416315e-116,\n",
       "          5.52465269e-103],\n",
       "        [-7.02097911e-117, -1.72110229e-096, -1.51168975e-116,\n",
       "         -6.25836106e-116, -1.21197182e-116, -2.66568990e-098,\n",
       "         -1.01624154e-097, -3.25907600e-095,  2.13438539e-116,\n",
       "          6.77607833e-109],\n",
       "        [ 2.57248691e-001,  2.88757261e-102,  1.81849432e-098,\n",
       "          3.27046560e-001,  4.53111626e-002,  2.65643128e-112,\n",
       "          5.17378544e-003,  1.96110854e-104,  5.97588686e-112,\n",
       "          1.10198717e-098],\n",
       "        [-2.54385325e-001,  1.02364699e-001, -2.14039341e-001,\n",
       "         -4.35691520e-002, -1.80862912e-002,  5.67061504e-103,\n",
       "          8.38416609e-003, -4.70599257e-103,  3.75545776e-115,\n",
       "          4.05773607e-002],\n",
       "        [ 5.58273504e-096,  1.15078789e-116,  4.37466509e-114,\n",
       "         -1.84919165e-108, -2.74567997e-106, -1.19353919e-117,\n",
       "          8.24024350e-116,  5.04529711e-115, -5.60770257e-113,\n",
       "         -1.16202959e-099],\n",
       "        [ 3.34474217e-114,  1.49568512e-116,  1.68606412e-097,\n",
       "          4.88189380e-101, -2.44374827e-116,  3.30142505e-108,\n",
       "          1.47880983e-113,  3.48836514e-100, -2.84190655e-096,\n",
       "          5.77681273e-096],\n",
       "        [ 7.03732150e-110, -6.34668584e-108, -1.30302778e-116,\n",
       "          8.25943001e-102,  8.42856781e-114,  3.38350088e-109,\n",
       "          1.37382412e-106,  2.60473797e-116, -5.70493711e-113,\n",
       "          3.51227446e-094],\n",
       "        [ 1.05441573e-001,  1.61352855e-001, -2.32692539e-002,\n",
       "         -8.01299996e-002,  8.56719425e-002, -1.70237681e-114,\n",
       "          3.45926199e-115,  1.37176793e-108, -5.02627804e-101,\n",
       "          1.28843082e-001],\n",
       "        [-9.78006826e-003,  1.62983561e-001,  6.14218038e-002,\n",
       "         -8.42093711e-003, -6.20425685e-002,  4.68143309e-114,\n",
       "         -6.57285618e-003, -1.97225209e-116, -2.88744843e-108,\n",
       "          1.14205710e-001],\n",
       "        [ 3.03097594e-001, -6.43501590e-003,  1.60136469e-001,\n",
       "          3.99535595e-001,  1.73201786e-002, -5.29593870e-116,\n",
       "         -7.60969058e-003,  2.37731609e-116, -5.78763806e-116,\n",
       "         -2.32244969e-001],\n",
       "        [-1.08334114e+000,  9.50690548e-112, -2.70148523e-095,\n",
       "         -9.91655260e-001, -1.04940707e+000,  2.48968343e-116,\n",
       "          1.68289029e-001,  1.96066314e-097, -2.22284142e-116,\n",
       "          9.54618745e-112],\n",
       "        [ 2.97492008e-001, -6.10409324e-002, -1.14689240e-001,\n",
       "          3.37767512e-001,  6.58796955e-002,  3.03010092e-101,\n",
       "         -8.08207391e-003,  1.36514422e-102,  2.01542131e-116,\n",
       "         -2.41685232e-001],\n",
       "        [ 4.16608256e-001, -2.45850595e-007, -2.04514051e-042,\n",
       "          3.18466627e-001,  4.26893575e-002, -1.56844946e-113,\n",
       "         -3.38665249e-004, -2.12068697e-104,  5.58919946e-103,\n",
       "         -3.56171030e-045],\n",
       "        [-1.04465387e+000, -2.43967188e-100, -3.58871592e-112,\n",
       "         -9.78881575e-001, -1.00877343e+000,  1.79815272e-115,\n",
       "          1.52986280e-001, -2.15106150e-103,  5.71700139e-116,\n",
       "          5.79581244e-116],\n",
       "        [-7.75975920e-115,  1.63147096e-113,  4.12617905e-106,\n",
       "         -5.24369250e-102, -3.15998702e-109,  4.93721476e-117,\n",
       "         -2.39769213e-108,  6.16443744e-117, -2.64354262e-114,\n",
       "         -1.00353185e-106],\n",
       "        [ 2.55453550e-001,  5.84750377e-002, -1.70836788e-001,\n",
       "          4.63803119e-002,  1.03290012e-001,  5.12765247e-117,\n",
       "         -5.78152043e-002, -7.35800915e-115,  1.54958622e-100,\n",
       "         -9.94492644e-002],\n",
       "        [-2.08734323e+000, -7.36880739e-115, -4.49943995e-104,\n",
       "         -1.59335552e+000, -1.85118941e+000,  1.58151323e-096,\n",
       "          2.60604668e-001,  1.02887898e-094,  2.51187834e-107,\n",
       "          6.15934031e-117],\n",
       "        [ 1.34284015e+000, -8.73801867e-110,  7.48106591e-117,\n",
       "          9.67925371e-001,  1.15189507e+000, -4.28215063e-116,\n",
       "          1.35487280e-001, -9.35123389e-097,  1.12243094e-098,\n",
       "          4.70562480e-100],\n",
       "        [-2.13820091e-101, -1.16605451e-114, -1.03434714e-108,\n",
       "          3.43628712e-116, -9.05161807e-100,  2.42307299e-110,\n",
       "         -5.23128059e-104,  1.44797116e-116, -1.42757062e-099,\n",
       "          1.35631086e-116],\n",
       "        [-5.00562337e-111,  9.03690258e-117,  4.46430857e-110,\n",
       "          1.22923865e-112,  9.83133850e-098, -2.65507414e-114,\n",
       "         -7.68586301e-109, -5.52807013e-110,  1.81761521e-116,\n",
       "          1.09348404e-105],\n",
       "        [-4.04687578e-097,  1.89961518e-111,  7.71441398e-116,\n",
       "         -5.13938970e-117,  6.33406075e-117,  8.41988303e-113,\n",
       "         -4.23231653e-116,  1.79759017e-093, -7.62189503e-100,\n",
       "          2.82247539e-110],\n",
       "        [ 2.89353320e-116, -6.94340837e-099,  3.66905700e-113,\n",
       "          1.99084526e-101,  3.35167341e-104,  4.23472648e-116,\n",
       "         -1.66698563e-110, -1.06507103e-116,  4.09971231e-116,\n",
       "         -1.44266982e-078],\n",
       "        [ 2.98566069e-002, -9.63007903e-003,  6.50567695e-002,\n",
       "          3.01059319e-001,  2.46898501e-001,  5.20650587e-104,\n",
       "         -6.90447884e-002, -4.31572117e-099, -1.57605811e-102,\n",
       "          3.26399089e-002],\n",
       "        [-1.64503254e-111,  6.28987228e-116,  1.28866061e-116,\n",
       "          2.94045121e-101, -2.01766782e-096,  5.68543824e-100,\n",
       "          1.99944338e-095, -1.12275464e-099, -3.88326984e-020,\n",
       "         -4.64330538e-112],\n",
       "        [-4.25684125e-115,  7.73695952e-115,  6.77947694e-117,\n",
       "         -1.37445500e-093, -1.44381609e-095, -1.96600644e-115,\n",
       "         -9.67413431e-105,  2.85717188e-100,  9.96438464e-106,\n",
       "         -1.34944451e-110],\n",
       "        [-1.04908073e+000, -2.87370905e-111,  6.79044266e-099,\n",
       "         -1.06168337e+000, -1.05581228e+000,  1.61844446e-116,\n",
       "          1.53264222e-001,  4.99833539e-116, -6.46480576e-097,\n",
       "         -8.10328222e-115],\n",
       "        [ 1.69041496e-110, -4.95091767e-112,  2.22784458e-112,\n",
       "         -7.33001974e-095,  2.65545862e-115,  1.48292410e-102,\n",
       "          1.64745840e-107, -1.84972820e-100, -7.54559389e-114,\n",
       "         -5.63482384e-117],\n",
       "        [ 3.86032314e-001,  5.31856313e-002, -2.32776415e-001,\n",
       "          3.71834758e-001,  4.05861988e-002,  1.12992650e-097,\n",
       "          5.43093912e-003, -7.65333666e-096,  2.25330876e-099,\n",
       "         -3.48824944e-002],\n",
       "        [ 7.25434139e-002, -9.47160693e-002, -1.72946235e-001,\n",
       "         -5.74166808e-002,  2.44250427e-001,  1.29042222e-099,\n",
       "         -5.96846166e-002, -1.39719336e-108,  8.43721716e-116,\n",
       "         -1.93015117e-001],\n",
       "        [-6.21890961e-104, -8.12249295e-114,  7.20190894e-098,\n",
       "         -1.79304073e-097,  4.12691770e-107, -3.86046814e-102,\n",
       "         -6.70068091e-117,  3.13287333e-101,  2.48437521e-107,\n",
       "         -4.34193254e-112],\n",
       "        [ 2.08238504e-002,  2.13611031e-002, -7.62352510e-004,\n",
       "          4.15733375e-002,  8.02084645e-002, -6.54230454e-116,\n",
       "         -1.78562171e-002, -7.37061345e-111,  3.97013500e-113,\n",
       "          1.78814092e-001],\n",
       "        [ 1.37302805e+000, -1.61739920e-115,  5.84358980e-094,\n",
       "          1.04569885e+000,  1.19201767e+000, -2.49407371e-010,\n",
       "          1.50267363e-001, -4.85648945e-107, -9.95970521e-114,\n",
       "         -4.40318239e-115],\n",
       "        [ 8.21350107e-117, -6.53082860e-099,  4.80147514e-117,\n",
       "          3.21669993e-105,  1.99330121e-104,  2.53372742e-111,\n",
       "         -6.32421181e-105, -1.17485076e-111,  1.75132385e-116,\n",
       "         -5.83865493e-116],\n",
       "        [-1.18003944e-111,  1.67539631e-115, -1.34157401e-108,\n",
       "          5.31678349e-094, -9.38526373e-104,  3.66355748e-112,\n",
       "         -2.21593467e-105, -7.65553903e-117,  3.45323621e-107,\n",
       "          3.56648937e-098],\n",
       "        [ 2.00440326e-001, -2.52342077e-002,  1.77855099e-001,\n",
       "          5.91103612e-002,  3.55143518e-001,  3.92041630e-116,\n",
       "         -4.90230809e-002, -1.51653932e-101, -2.36116711e-108,\n",
       "         -1.83550013e-002],\n",
       "        [-1.67801220e-002, -2.09699596e-001,  1.28542121e-001,\n",
       "          2.81217971e-001,  1.01523055e-001,  7.44737615e-117,\n",
       "         -4.88881806e-002, -3.22168661e-113,  5.05003481e-100,\n",
       "          1.07271625e-001],\n",
       "        [ 1.68349160e-001,  8.70517925e-002,  1.10938097e-001,\n",
       "          1.28556531e-001,  3.14488999e-001,  9.58213887e-113,\n",
       "         -3.07447944e-002, -1.36020715e-093, -5.53873829e-118,\n",
       "         -1.37649410e-002],\n",
       "        [ 2.56743638e-001,  3.77116676e-002, -1.34029831e-001,\n",
       "          2.19328857e-001,  3.01560676e-001,  2.73588475e-115,\n",
       "         -4.32434114e-002,  1.39606589e-104, -4.24049724e-096,\n",
       "         -1.19016344e-001],\n",
       "        [-1.52683455e+000,  9.28270587e-013,  4.22592307e-023,\n",
       "         -1.46144970e+000, -1.50002163e+000,  1.59720234e-109,\n",
       "         -1.88168626e-003,  1.69413872e-018, -7.46603089e-116,\n",
       "         -2.86681553e-022],\n",
       "        [ 1.26159534e-113, -2.70782653e-112,  8.57992540e-102,\n",
       "          2.14583671e-095, -7.45832902e-113, -1.86047698e-113,\n",
       "         -1.46540994e-112,  2.68122469e-114,  5.14954030e-099,\n",
       "          4.40048717e-116],\n",
       "        [ 5.54173295e-103, -1.03183803e-093, -8.58606321e-103,\n",
       "          7.52515000e-112,  1.87109429e-093,  6.53700570e-005,\n",
       "         -1.28014516e-098, -7.97221354e-100, -8.61169496e-115,\n",
       "          4.06405243e-111],\n",
       "        [ 2.17782406e-102, -5.48700397e-107, -6.92286144e-116,\n",
       "         -1.28550270e-116,  4.69622608e-103, -7.24731384e-096,\n",
       "         -9.11863573e-104,  6.69726007e-113,  8.85395565e-103,\n",
       "          3.21165430e-103],\n",
       "        [ 2.78337721e-001, -4.85325994e-002,  1.61565811e-001,\n",
       "          1.23017293e-001,  4.93902516e-002, -3.76468551e-112,\n",
       "         -6.17391316e-002,  3.08842825e-117, -3.39603212e-114,\n",
       "         -1.52305625e-001],\n",
       "        [ 2.27531206e-001,  1.23711224e-001,  1.51536320e-001,\n",
       "         -2.12095468e-002,  1.21711959e-001,  7.81844349e-116,\n",
       "         -2.04314945e-002, -1.15945366e-112, -1.42228727e-115,\n",
       "         -1.10312085e-001],\n",
       "        [ 2.58358288e-001,  7.26111755e-118, -5.39518778e-103,\n",
       "          2.62007080e-001,  2.03447857e-001, -1.42541729e-110,\n",
       "          4.49876915e-003, -3.00367442e-116,  1.96787283e-097,\n",
       "          6.72824866e-116],\n",
       "        [ 1.03826722e-001,  1.09087486e-001, -5.24463498e-002,\n",
       "          3.40352378e-001, -5.45746130e-003,  7.84786943e-117,\n",
       "         -6.19193284e-002, -9.63323754e-117, -9.12294085e-116,\n",
       "          3.05815596e-002],\n",
       "        [-1.04495985e+000, -5.31208249e-116,  2.93812109e-111,\n",
       "         -9.98352435e-001, -1.02630341e+000,  8.47455516e-116,\n",
       "          1.50590604e-001, -2.67290413e-114, -1.29943245e-116,\n",
       "         -6.07732478e-101],\n",
       "        [ 4.16796804e-001, -1.38128648e-001, -1.45721309e-001,\n",
       "          3.92901910e-001,  4.07084984e-001, -1.71276829e-100,\n",
       "         -2.47471738e-003, -2.74651388e-116, -2.37483277e-100,\n",
       "          4.18536115e-002],\n",
       "        [-4.49232224e-002, -4.60421103e-002, -7.46971949e-002,\n",
       "          1.05995580e-001, -9.56587915e-002, -6.84524047e-110,\n",
       "         -2.57664510e-002, -3.28795178e-098,  1.61089463e-096,\n",
       "          5.38172786e-002],\n",
       "        [ 2.14273469e-001, -8.35700209e-002,  7.48024433e-002,\n",
       "          3.25997145e-001, -3.39433762e-002, -3.33167650e-095,\n",
       "         -1.03514326e-002,  1.87690031e-097,  8.66854754e-103,\n",
       "         -1.43203813e-002],\n",
       "        [ 1.92117998e-001,  1.19798126e-116,  2.55964086e-012,\n",
       "          2.29753734e-001,  2.39149212e-001, -1.56272965e-116,\n",
       "         -3.65263267e-002, -4.11549049e-113, -2.07704799e-099,\n",
       "         -6.26201332e-095],\n",
       "        [ 2.13470038e-002,  1.19237576e-001,  1.99675612e-001,\n",
       "          8.24560797e-002,  3.08864991e-001, -2.77618939e-115,\n",
       "         -5.46338261e-002,  2.39158586e-101, -2.85782655e-114,\n",
       "         -5.02322333e-002],\n",
       "        [ 2.85925719e-001, -4.56645310e-002, -3.17749359e-002,\n",
       "          1.62752878e-001,  1.65854615e-001, -1.10595419e-107,\n",
       "         -5.36337585e-002,  1.33381848e-109,  5.60140840e-115,\n",
       "         -1.13516975e-001],\n",
       "        [ 5.27696293e+000,  7.87089124e-072,  1.30957620e-091,\n",
       "          5.31805185e+000,  5.29978037e+000,  2.14349177e-113,\n",
       "          2.61902038e-001, -6.60108132e-116, -2.02887868e-116,\n",
       "         -1.12791621e-080],\n",
       "        [ 2.02790148e-001, -9.17041365e-002, -1.77963897e-001,\n",
       "          3.95572966e-001,  3.08403484e-001, -7.91038634e-115,\n",
       "          1.30336778e-002, -7.13428348e-114,  1.95661868e-116,\n",
       "         -8.57668794e-002]]),\n",
       " array([[ 7.92593325e-001],\n",
       "        [-5.57725834e-001],\n",
       "        [-2.96707153e-001],\n",
       "        [ 5.60695806e-001],\n",
       "        [ 6.75531103e-001],\n",
       "        [-4.97277779e-082],\n",
       "        [-8.69181149e-001],\n",
       "        [-2.27614188e-051],\n",
       "        [ 8.89399147e-116],\n",
       "        [-3.45473853e-001]])]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelNN.coefs_ # 100개씩 10개가 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e1dedc7848>]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter( car_df['speed'], car_df['dist'])\n",
    "plt.plot(car_df['speed'],  modelNN.predict( car_df[['speed']] ) , 'r:')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
